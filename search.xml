<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A Markov Chain Model in Manpower Systems</title>
    <url>/yantongtt.github.io/2020/08/04/A-Markov-Chain-Model-in-Manpower-Systems/</url>
    <content><![CDATA[<h1 id="如何根据互联网员工年龄"><a href="#如何根据互联网员工年龄" class="headerlink" title="如何根据互联网员工年龄"></a>如何根据互联网员工年龄</h1><h2 id="基于-Markov-过程预测未来公司人员规模"><a href="#基于-Markov-过程预测未来公司人员规模" class="headerlink" title="基于 Markov 过程预测未来公司人员规模"></a>基于 Markov 过程预测未来公司人员规模</h2><p>其实不管是互联网、金融还是其他行业，拟定一个合理员工年龄结构战略对任何一家企业的长期发展都有着极其重要的影响，同时也是人力规划的核心目标。在制定未来一段时期人员补充方案时，企业有必要对不同方案下的员工年龄结构变化趋势进行预测，从而判断不同方案下员工队伍的变化是否能支撑未来发展需要。<br><a id="more"></a></p>
<h3 id="从Markov-Chain到模型构建"><a href="#从Markov-Chain到模型构建" class="headerlink" title="从Markov Chain到模型构建"></a>从Markov Chain到模型构建</h3><p>这个模型我是基于Markov Chain 的思想来构建的。何为Markov Chain？简单来说就是假设某一时刻状态转移的概率只依赖于它的前一个状态。</p>
<p>在年龄预测模型中，基本的几个状态为不同的年龄段。Markov Chain 模型主要是分析一个人在某一阶段内由一个年龄段调到另一个年龄段的可能性。从一个年龄段转移到另一个年龄段的状态属于内部流动，意味着该员工仍在该企业就职。但还存在外部流动的状态，离职与退休。属于员工流失。通过运用Markov 过程原理，分析每个年龄段的员工流动到不同状态的流动趋势和概率，构建完整的预测周期运算过程，以便为人力资源在新增人员对企业总量规模、人员年龄结构的规划中提供依据。</p>
<p>这么说不太直观，举个例子。在项目中，我将内部流动分为7个阶段，分别为20-24，25-28，29-31，32-35，36-40，41-50，51-60。外部流动为离职率和退休率。</p>
<p>根据Markov Chain原理，转移矩阵中的概率就是某一状态到另一状态的可能性。下表就是项目中转移矩阵的结构。我们就用这个转移矩阵就表示这一年内发生的事情。其中，p_23表示今年处于[25,28]这一年龄阶段的员工在下一年流向[29,31]的概率。为了方便计算，我用频率表示，即今年处于[25,28]这一年龄阶段而在下一年流向[29,31]的员工总数与今年处于[25,28]这一年龄阶段的员工总数比值。为什么大部分概率为0其实很好理解，这些概率为0的状态转移情况是不可能发生的。今年[25,28]年龄段的员工明年可能是26,27,28,29，因此在内部流动中只能有两种状态。而先前说的p_23为今年28岁的员工明年没有离职流向[29,31]年龄的概率。</p>
<p><strong>Transition Matrix from Preview Year to Next Year</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>20-24</th>
<th>25-28</th>
<th>29-31</th>
<th>32-35</th>
<th>36-40</th>
<th>41-50</th>
<th>51-60</th>
<th>Dismission</th>
<th>Retirement</th>
<th>Recruitment</th>
</tr>
</thead>
<tbody>
<tr>
<td>20-24</td>
<td>p_11</td>
<td>p_12</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_18</td>
<td>0</td>
<td>p_1_10</td>
</tr>
<tr>
<td>25-28</td>
<td>0</td>
<td>p_22</td>
<td>p_23</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_28</td>
<td>0</td>
<td>p_2_10</td>
</tr>
<tr>
<td>29-31</td>
<td>0</td>
<td>0</td>
<td>p_33</td>
<td>p_34</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_38</td>
<td>0</td>
<td>p_3_10</td>
</tr>
<tr>
<td>32-35</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_44</td>
<td>p_45</td>
<td>0</td>
<td>0</td>
<td>p_48</td>
<td>0</td>
<td>p_4_10</td>
</tr>
<tr>
<td>36-40</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_55</td>
<td>p_56</td>
<td>0</td>
<td>p_58</td>
<td>0</td>
<td>p_5_10</td>
</tr>
<tr>
<td>41-50</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_66</td>
<td>p_67</td>
<td>p_68</td>
<td>0</td>
<td>p_6_10</td>
</tr>
<tr>
<td>51-60</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_77</td>
<td>p_78</td>
<td>p_79</td>
<td>p_7_10</td>
</tr>
</tbody>
</table>
</div>
<h3 id="模型假设与定义"><a href="#模型假设与定义" class="headerlink" title="模型假设与定义"></a>模型假设与定义</h3><p><img src="hypotheses.png" alt="images"></p>
<p>假设，</p>
<ul>
<li>$N(t)$ 为第t年年初各年龄段员工总数，$N(t) = [n_1(t), n_2(t), … , n_7(t)]$，其中$n_1(t),n_2(t),…,n_7(t)$ 分别为第t年20-24岁，25-28岁，29-31岁，32-35岁，36-40岁，41-50岁，51-60岁的员工人数。</li>
<li>$L(t)$ 为在第t整年各年龄段离职员工人数，$L(t) = [l_1(t), l_2(t), … , l_7(t)]$，而$P(t) = [p_1(t), p_2(t), … , p_7(t)]$分别为发生概率，即$L(t)=N(t)\times P(t)$</li>
<li>$R_t(t)$ 为在第t整年各年龄段退休员工人数，$R_t(t)=[0,0,0,0,0,0,r_7(t)]$ </li>
<li>$R_c(t)$ 为在第t整年各年龄段新增员工人数</li>
<li>$P_{ij}(t+1)$ 第t年从状态$i$到下一年转移到状态$j$的概率</li>
</ul>
<p><strong>模型假设</strong> : 下一年年初的员工总数=这一年年末剩余+下一年年初新增。而这一年年末剩余就等于这一年年初的总数乘上转移矩阵。这就是markov process在这个算法中的运用。<script type="math/tex">N(t+1)=N(t)*P_{ij}(t+1)+R_c(t+1)</script></p>
]]></content>
      <categories>
        <category>Stochastic Model</category>
      </categories>
      <tags>
        <tag>人力资源</tag>
        <tag>Markov Chain</tag>
      </tags>
  </entry>
  <entry>
    <title>An Overview of Time Series Forecasting Models</title>
    <url>/yantongtt.github.io/2020/08/06/An-overview-of-time-series-forecasting-models/</url>
    <content><![CDATA[<p><strong>This post describes 5 forecasting models and I apply them to predict the total summer spend for leisure in NYC </strong> </p>
<p>Last semester, in my Marketing Analytics class, I have been working with Time Series Data. This data records total summer spending in NYC from 1959 to 2019. There is no trend, and no seasonality in this data. Very simple but I still wanted to review what a Time series is as well as make my understanding more concert on that. This article only provides some simple time series models. What I am trying to say is that time series predictions are difficult in reality and always require a very specialized data scientist to implement it. The models in this post are so limited that we should learn more beyond these.</p>
<a id="more"></a>
<h2 id="How-the-time-series-looks-like"><a href="#How-the-time-series-looks-like" class="headerlink" title="How the time series looks like?"></a>How the time series looks like?</h2><p>The data contains only 2 columns, one column is Date and the other column relates to total summer spending. </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>year</th>
<th>summerspend</th>
</tr>
</thead>
<tbody>
<tr>
<td>1959</td>
<td>22627333.2</td>
</tr>
<tr>
<td>1960</td>
<td>30509799.9</td>
</tr>
<tr>
<td>1961</td>
<td>24408277.9</td>
</tr>
<tr>
<td>1962</td>
<td>23772213.4</td>
</tr>
<tr>
<td>1963</td>
<td>23572771.3</td>
</tr>
</tbody>
</table>
</div>
<p>It shows the summer spending on New England parks by greater NYC visitors to New England parks from 1959 till 2019. The goal is to predict the total spending for each of the next five years.</p>
<h2 id="Time-Series-Analysis"><a href="#Time-Series-Analysis" class="headerlink" title="Time Series Analysis"></a>Time Series Analysis</h2><h3 id="Step-1-Visualizing-the-time-series"><a href="#Step-1-Visualizing-the-time-series" class="headerlink" title="Step 1: Visualizing the time series"></a>Step 1: Visualizing the time series</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data[<span class="string">&#x27;summerspend&#x27;</span>].plot();</span><br></pre></td></tr></table></figure>
<p><img src="summerspend_plot.png" alt="images"></p>
<p>When we work on time series forecasting, a series need to be stationary. Stationary process has the property that the mean, variance and autocorrelation structure do not change over time. The mean is constant in this case. However, it is hard to say stable variance and autocorrelation. Before ARMA model, I will try to detect this. </p>
<p>We also visualize the data in our series through a distribution too.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.distplot(data[<span class="string">&#x27;summerspend&#x27;</span>], hist=<span class="literal">True</span>, kde=<span class="literal">True</span>, </span><br><span class="line">             bins=<span class="number">50</span>, color = <span class="string">&#x27;darkblue&#x27;</span>, </span><br><span class="line">             hist_kws=&#123;<span class="string">&#x27;edgecolor&#x27;</span>:<span class="string">&#x27;black&#x27;</span>&#125;,</span><br><span class="line">             kde_kws=&#123;<span class="string">&#x27;linewidth&#x27;</span>: <span class="number">4</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="summerspend_d.png" alt="images"></p>
<p>We can observe a near-normal distribution over consumption values.</p>
<h3 id="Step-2-Creating-Training-and-Test-Datasets-for-Modeling"><a href="#Step-2-Creating-Training-and-Test-Datasets-for-Modeling" class="headerlink" title="Step 2: Creating Training and Test Datasets for Modeling"></a>Step 2: Creating Training and Test Datasets for Modeling</h3><p>Because we need to capture the time factor in time series data, I devided total data as training data and test data by time. The first 70% older data is training data, and 30% newer data is test data.</p>
<p><img src="train_test.png" alt="images"></p>
<p>We divided data into training data and test data by time. As we can see in the this graph, blue old data is used to build models and orange new data is used for prediction.</p>
<h3 id="Step-3-Exploring-Time-Series-to-Select-Proper-Models"><a href="#Step-3-Exploring-Time-Series-to-Select-Proper-Models" class="headerlink" title="Step 3: Exploring Time Series to Select Proper Models"></a>Step 3: Exploring Time Series to Select Proper Models</h3><p>A given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.</p>
<p>In order to select a proper time series model, we should do ETS decomposition. That is extract trend and seasonality from our data. The resultant series will become stationary through this process.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.tsa.seasonal <span class="keyword">import</span> seasonal_decompose</span><br><span class="line"></span><br><span class="line">result = seasonal_decompose(train[<span class="string">&#x27;summerspend&#x27;</span>], model=<span class="string">&#x27;multiplicative&#x27;</span>)  <span class="comment"># model=&#x27;mul&#x27; also works</span></span><br><span class="line">result.plot();</span><br></pre></td></tr></table></figure>
<p><img src="ETS.png" alt="images"></p>
<p>According to the ETS decomposition, there is no significant seasonal and trend. Therefore, we will not consider the seasonal model, such as SARIMAX, Holt-Winters, Holt’s Linear Trend.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.graphics.tsaplots <span class="keyword">import</span> plot_acf</span><br><span class="line"><span class="keyword">from</span> statsmodels.graphics.tsaplots <span class="keyword">import</span> plot_pacf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">211</span>)</span><br><span class="line">fig = sm.graphics.tsa.plot_acf(train,lags=<span class="number">20</span>,ax=ax1)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">212</span>)</span><br><span class="line">fig = sm.graphics.tsa.plot_pacf(train,lags=<span class="number">20</span>,ax=ax2)</span><br></pre></td></tr></table></figure>
<p><img src="ACF.png" alt="images"></p>
<p>What’s more, this time series seems to be stationary. Both ACF and PACF fall into confidence interval abruptly, cutting off at q = 0 and p = 0,respectively. But for more precisely prediction, we will try AR(1), MA(1), ARMA(1,1) in the following modeling process</p>
<h3 id="Step-4-Modeling"><a href="#Step-4-Modeling" class="headerlink" title="Step 4: Modeling"></a>Step 4: Modeling</h3><p>We use training dataset for modeling, and test dataset to measure the performance of models. The performance indicator mainly is RMSE. But we also use AIC and BIC to measure the performances of AR(1), MA(1), ARMA(1,1) , to select the best model in ARMA.</p>
<h4 id="Method-1-Naive-Approach"><a href="#Method-1-Naive-Approach" class="headerlink" title="Method 1: Naive Approach"></a>Method 1: Naive Approach</h4><p>Consider the process is no trend and no seasonal factor, we apply the Naive Approach at first.</p>
<p>We can infer from the graph that the price of the coin is stable from the start. Many a times we are provided with a dataset, which is stable throughout it’s time period. If we want to forecast the price for the next day, we can simply take the last day value and estimate the same value for the next day. Such forecasting technique which assumes that the next expected point is equal to the last observed point is called Naive Method.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Now we will implement the Naive method to forecast the prices for test data.</span></span><br><span class="line"></span><br><span class="line">train_summerspend = np.asarray(train.summerspend)</span><br><span class="line">y_hat = test.copy()</span><br><span class="line">y_hat[<span class="string">&#x27;naive&#x27;</span>] = train_summerspend[len(train_summerspend)<span class="number">-1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train.index, train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test.index,test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat.index,y_hat[<span class="string">&#x27;naive&#x27;</span>], label=<span class="string">&#x27;Naive Forecast&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Naive Forecast&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Naive Method.png" alt="images"></p>
<p>We will now calculate RMSE to check to accuracy of our model on test data set.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RMSE = [] <span class="comment"># collect rmse of all the models</span></span><br><span class="line"></span><br><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat.naive))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment"># RMSE = 4292286.145806624</span></span><br></pre></td></tr></table></figure>
<p>We can infer from the RMSE value and the graph above, that Naive method isn’t suited for datasets with high variability. It is best suited for stable datasets. We can still improve our score by adopting different techniques.</p>
<h4 id="Method-2-Simple-Average"><a href="#Method-2-Simple-Average" class="headerlink" title="Method 2: Simple Average"></a>Method 2: Simple Average</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_hat_avg = test.copy()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>] = train[<span class="string">&#x27;summerspend&#x27;</span>].mean()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>], label=<span class="string">&#x27;Average Forecast&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="simple_average.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.avg_forecast))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment"># RMSE = 2404469.0779477805</span></span><br></pre></td></tr></table></figure>
<p>We can see the simple average can improve the score. The reason might be the time series is no trend.</p>
<h4 id="Method-3-Moving-Average"><a href="#Method-3-Moving-Average" class="headerlink" title="Method 3 Moving Average"></a>Method 3 Moving Average</h4><p>The algorithm I propose here is an attempt to find the best moving average according to the time window period we choose. We’ll try different moving averages length and find the one that minimizes RMSE. I will perform a for loop that spans among 2-period moving average to 10-period moving average. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p_number = []</span><br><span class="line">rmse_value = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">11</span>):</span><br><span class="line">        y_hat_avg = test.copy()</span><br><span class="line">        y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>] = train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(i).mean().iloc[<span class="number">-1</span>]</span><br><span class="line">        plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">        plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">        plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">        plt.plot(y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>], label=<span class="string">&#x27;Moving Average Forecast&#x27;</span>)</span><br><span class="line">        plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(i).mean(), label=<span class="string">&#x27;Train Moving Average&#x27;</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.moving_avg_forecast))</span><br><span class="line">        p_number.append(i)</span><br><span class="line">        rmse_value.append(rms)</span><br><span class="line">        print(<span class="string">&quot;RMSe of Moving Average Model when p =&quot;</span>,i,<span class="string">&quot;is&quot;</span>,rms)</span><br><span class="line">RMSE_MA = pd.DataFrame(&#123;<span class="string">&quot;p_number&quot;</span> : p_number,<span class="string">&quot;rmse_value&quot;</span> : rmse_value&#125;)</span><br><span class="line">RMSE_MA</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>p_number</th>
<th>rmse_value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>3931156</td>
</tr>
<tr>
<td>3</td>
<td>2608152</td>
</tr>
<tr>
<td>4</td>
<td>3016208</td>
</tr>
<tr>
<td>5</td>
<td>2808586</td>
</tr>
<tr>
<td>6</td>
<td>2842545</td>
</tr>
<tr>
<td>7</td>
<td>2541544</td>
</tr>
<tr>
<td>8</td>
<td>2466775</td>
</tr>
<tr>
<td>9</td>
<td>2408699</td>
</tr>
<tr>
<td>10</td>
<td>2451784</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RMSE_MA[RMSE_MA.rmse_value == RMSE_MA.rmse_value.min()]</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>p_number</th>
<th>rmse_value</th>
</tr>
</thead>
<tbody>
<tr>
<td>9</td>
<td>2408699</td>
</tr>
</tbody>
</table>
</div>
<p>In Moving Average model, we use p = 9.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_hat_avg = test.copy()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>] = train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(<span class="number">9</span>).mean().iloc[<span class="number">-1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>], label=<span class="string">&#x27;Moving Average Forecast&#x27;</span>)</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(i).mean(), label=<span class="string">&#x27;Train Moving Average&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="MA_model.png" alt="images"></p>
<h4 id="Method-4-Simple-Exponential-Smoothing"><a href="#Method-4-Simple-Exponential-Smoothing" class="headerlink" title="Method 4  Simple Exponential Smoothing"></a>Method 4  Simple Exponential Smoothing</h4><p>Exponential smoothing forecasting methods are similar in that a prediction is a weighted sum of past observations, but the model explicitly uses an exponentially decreasing weight for past observations. Specifically, past observations are weighted with a geometrically decreasing ratio. The formula below tells about its principle</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.tsa.api <span class="keyword">import</span> ExponentialSmoothing, SimpleExpSmoothing</span><br><span class="line">y_hat_avg = test.copy()</span><br><span class="line">fit2 = SimpleExpSmoothing(np.asarray(train[<span class="string">&#x27;summerspend&#x27;</span>])).fit(smoothing_level=<span class="number">0.6</span>,optimized=<span class="literal">False</span>)</span><br><span class="line">y_hat_avg[<span class="string">&#x27;SES&#x27;</span>] = fit2.forecast(len(test))</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;SES&#x27;</span>], label=<span class="string">&#x27;SES&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Simple_EXP.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.SES))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment">#RMSE = 3577811.749127613</span></span><br></pre></td></tr></table></figure>
<h4 id="Method-5-ARMA"><a href="#Method-5-ARMA" class="headerlink" title="Method 5  ARMA"></a>Method 5  ARMA</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ARMA_name = [<span class="string">&quot;AR(1)&quot;</span>,<span class="string">&quot;MA(1)&quot;</span>,<span class="string">&quot;ARMA(1,1)&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#AR(1)</span></span><br><span class="line">y_hat_avg = test.copy()</span><br><span class="line">fit1 = sm.tsa.ARMA(train.summerspend, order=(<span class="number">1</span>,<span class="number">0</span>)).fit()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;ARMA&#x27;</span>] = fit1.predict(start=<span class="string">&quot;2001-1-1&quot;</span>, end=<span class="string">&quot;2019-1-1&quot;</span>, dynamic=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="AR_1.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ARMA(1,1)</span></span><br><span class="line">y_hat_avg = test.copy()</span><br><span class="line">fit3 = sm.tsa.ARMA(train.summerspend, order=(<span class="number">1</span>,<span class="number">1</span>)).fit()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;ARMA&#x27;</span>] = fit3.predict(start=<span class="string">&quot;2001-1-1&quot;</span>, end=<span class="string">&quot;2019-1-1&quot;</span>, dynamic=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#MA(1)</span></span><br><span class="line"><span class="comment"># That is method 3 moving average method</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>ARMA_name</th>
<th>AIC</th>
<th>BIC</th>
</tr>
</thead>
<tbody>
<tr>
<td>AR(1)</td>
<td>1359.187</td>
<td>1364.4</td>
</tr>
<tr>
<td>MA(1)</td>
<td>1358.679</td>
<td>1363.892</td>
</tr>
<tr>
<td>ARMA(1,1)</td>
<td>1361.447</td>
<td>1368.398</td>
</tr>
</tbody>
</table>
</div>
<p>We select ARMA(1,1) with the largest AIC value and BIC value</p>
<p><img src="ARMA11.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.ARMA))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment">#RMSE = 2415096.1564545105</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-5-Select-the-Best-Model"><a href="#Step-5-Select-the-Best-Model" class="headerlink" title="Step 5: Select the Best Model"></a>Step 5: Select the Best Model</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Naive Model</td>
<td>4292286</td>
</tr>
<tr>
<td style="text-align:left">Simple Average</td>
<td>2404469</td>
</tr>
<tr>
<td style="text-align:left">Moving Average</td>
<td>2408699</td>
</tr>
<tr>
<td style="text-align:left">Simple Exponential Smoothing</td>
<td>3577812</td>
</tr>
<tr>
<td style="text-align:left">ARMA(1,1)</td>
<td>2415096</td>
</tr>
</tbody>
</table>
</div>
<p>To minimize RMSE, I select Simple Average model. For more precisely prediction, I use all the dataset to build the model.</p>
<h3 id="Step-6-Model-for-Future-Prediction"><a href="#Step-6-Model-for-Future-Prediction" class="headerlink" title="Step 6: Model for Future Prediction"></a>Step 6: Model for Future Prediction</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Build Simple Average Model with all data</span></span><br><span class="line">pred_dates = pd.date_range(<span class="string">&#x27;2020-01-01&#x27;</span>, periods=<span class="number">5</span>, freq=<span class="string">&#x27;AS&#x27;</span>)</span><br><span class="line">pred = pd.Series(data[<span class="string">&#x27;summerspend&#x27;</span>].mean(),index=pred_dates)</span><br><span class="line">pred = pd.DataFrame(pred)</span><br><span class="line">pred.columns = [<span class="string">&quot;Forecast&quot;</span>]</span><br><span class="line">pred</span><br><span class="line"></span><br><span class="line">y_hat_avg = data.copy()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>] = data[<span class="string">&#x27;summerspend&#x27;</span>].mean()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>], label=<span class="string">&#x27;Average Forecast&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="step_5.png" alt="images"></p>
<p>According to Moving Average model, we predict the total spending in summer by greater NYC visitors to New England parks for each of the next five years are 24185805.88 dollars.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Comparing all the models, the simple average model minimized RMSE. For more precise prediction, we used all the dataset to build a new simple average model. This model forecasts the expected value equal to the average of all previously observed points. The predictions for five years are the same value, which is 24185805.88 dollars. The constant level is due to its algorithm. Unlike the ARMA model, it won’t capture the autoregressive factor. It takes the average of all the values previously known as the next value. Of course it won’t be exact, but somewhat close. </p>
<p>The reason why it is the best one would be its stationary property. This data is no upward or downward trend, no cycle fluctuation and no seasonality. Just like white noise, it is hard to predict the exact value. Sometimes, the simple one is the best one.</p>
<p>The goal of this project wasn’t to fit the best possible forecasting model for this NYC summer spending case, but to give an overview of forecasting models. In a real world, data will not as simple as this one, and may shows complex seasonality or trend. We need make more effort on preprocessing, feature engineering and feature selection. In a real world application a lot of time should be spent on preprocessing<strong>, </strong>feature engineering and feature selection.</p>
<p><strong>Thanks for reading!!! Please let me know if there is something I should add. And if you enjoy it, share it with your friends and colleagues : )</strong></p>
<p><strong>另外博客的中文版本也会及时更新的！谢谢！</strong></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Time Series</tag>
        <tag>Forecasting Models</tag>
      </tags>
  </entry>
  <entry>
    <title>Lending Club</title>
    <url>/yantongtt.github.io/2020/08/04/Lending-Club/</url>
    <content><![CDATA[<h1 id=""><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>风控</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Exploring for Airbnb Listings in Germany - Price Prediction</title>
    <url>/yantongtt.github.io/2020/08/06/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany-Price-Prediction/</url>
    <content><![CDATA[<p><strong>A look into the AirBnB public dataset for price prediction and understanding of the predictive variables</strong></p>
<p>In <em>Data Cleaning Challenge</em> part, I shared my understanding of data cleaning. Now, let’s dive deeper. </p>
<p>Think about an interesting question, “if you are a host, how to maximize your profits?” This time I used a linear regression model to explore deeper into the possible factors that contribute to Airbnb rental prices. It’s okay for you to try other models like GBDT or XGBoost. In this post, I will highlight the approach I used to answer this question as well as how I explain the results of regression analysis. </p>
<a id="more"></a>
<p>Same as last post, data is sourced from the <a href="http://insideairbnb.com/get-the-data.html"><strong>Inside Airbnb</strong></a> website. And data have been prepared in <em>Data Cleaning Challenge</em> part. On that part, I handled with some missing values and also create some features. </p>
<h3 id="Step-1-Think-about-the-problem-and-dataset"><a href="#Step-1-Think-about-the-problem-and-dataset" class="headerlink" title="Step 1: Think about the problem and dataset"></a>Step 1: Think about the problem and dataset</h3><p>Before diving head first into the data and producing large correlation matrices, I always try to think of the question and get a sense of the features. Why am I doing this analysis? What’s the goal? What relationships between features and the target variable make sense? Hope this tip can help you.</p>
<p>In my preview post, missing values have been handled, and also I removed some featrues like url or host_name. As shown in the below table, almost of data is in a very neat and ordered format. Date-typed date has been transformed as number of days or only extract the year. Some of null value has become new level of its variable, and some has been binned into cagegories. I will not go through the cleaning work in detail. Hope the preview blog would be helpful for you.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>host_since</th>
<th>host_location</th>
<th>host_response_time</th>
<th>host_response_rate</th>
<th>host_is_superhost</th>
<th>host_neighbourhood</th>
<th>host_listings_count</th>
<th>host_total_listings_count</th>
<th>host_has_profile_pic</th>
<th>host_identity_verified</th>
<th>street</th>
<th>neighbourhood</th>
<th>neighbourhood_cleansed</th>
<th>neighbourhood_group_cleansed</th>
<th>city</th>
<th>state</th>
<th>zipcode</th>
<th>market</th>
<th>smart_location</th>
<th>country_code</th>
<th>country</th>
<th>is_location_exact</th>
<th>property_type</th>
<th>room_type</th>
<th>accommodates</th>
<th>bathrooms</th>
<th>bedrooms</th>
<th>beds</th>
<th>bed_type</th>
<th>square_feet</th>
<th>price</th>
<th>weekly_price</th>
<th>monthly_price</th>
<th>security_deposit</th>
<th>cleaning_fee</th>
<th>guests_included</th>
<th>extra_people</th>
<th>minimum_nights</th>
<th>maximum_nights</th>
<th>calendar_updated</th>
<th>has_availability</th>
<th>availability_30</th>
<th>availability_60</th>
<th>availability_90</th>
<th>availability_365</th>
<th>calendar_last_scraped</th>
<th>number_of_reviews</th>
<th>first_review</th>
<th>last_review</th>
<th>review_scores_rating</th>
<th>review_scores_accuracy</th>
<th>review_scores_cleanliness</th>
<th>review_scores_checkin</th>
<th>review_scores_communication</th>
<th>review_scores_location</th>
<th>review_scores_value</th>
<th>requires_license</th>
<th>license</th>
<th>instant_bookable</th>
<th>is_business_travel_ready</th>
<th>cancellation_policy</th>
<th>require_guest_profile_picture</th>
<th>require_guest_phone_verification</th>
<th>calculated_host_listings_count</th>
<th>reviews_per_month</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>2008</td>
<td>Coledale, New South Wales, Australia</td>
<td>within a day</td>
<td>high</td>
<td>f</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg S√ºdwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10405</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Apartment</td>
<td>Entire home/apt</td>
<td>4</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>Real Bed</td>
<td>720</td>
<td>90</td>
<td>offer discount</td>
<td>offer discount</td>
<td>200</td>
<td>50</td>
<td>2</td>
<td>20</td>
<td>62</td>
<td>1125</td>
<td>7</td>
<td>t</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>220</td>
<td>2018/11/7</td>
<td>143</td>
<td>2009</td>
<td>2017</td>
<td>92</td>
<td>9</td>
<td>9</td>
<td>9</td>
<td>9</td>
<td>10</td>
<td>9</td>
<td>t</td>
<td>No</td>
<td>t</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>1.25</td>
</tr>
<tr>
<td>5</td>
<td>2009</td>
<td>Berlin, Berlin, Germany</td>
<td>within an hour</td>
<td>high</td>
<td>t</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Helmholtzplatz</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10437</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Apartment</td>
<td>Private room</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>Real Bed</td>
<td>N/A</td>
<td>42</td>
<td>no discount</td>
<td>no discount</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>24</td>
<td>2</td>
<td>10</td>
<td>3</td>
<td>t</td>
<td>15</td>
<td>26</td>
<td>26</td>
<td>26</td>
<td>2018/11/7</td>
<td>197</td>
<td>2009</td>
<td>2018</td>
<td>96</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>9</td>
<td>t</td>
<td>No</td>
<td>f</td>
<td>f</td>
<td>moderate</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>1.75</td>
</tr>
<tr>
<td>6</td>
<td>2009</td>
<td>Berlin, Berlin, Germany</td>
<td>within a few hours</td>
<td>high</td>
<td>f</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg S√ºdwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10405</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>f</td>
<td>Apartment</td>
<td>Entire home/apt</td>
<td>7</td>
<td>2.5</td>
<td>4</td>
<td>7</td>
<td>Real Bed</td>
<td>N/A</td>
<td>180</td>
<td>offer discount</td>
<td>no discount</td>
<td>400</td>
<td>80</td>
<td>5</td>
<td>10</td>
<td>6</td>
<td>14</td>
<td>14</td>
<td>t</td>
<td>0</td>
<td>7</td>
<td>7</td>
<td>137</td>
<td>2018/11/7</td>
<td>6</td>
<td>2015</td>
<td>2018</td>
<td>100</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>t</td>
<td>Yes</td>
<td>f</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>0.15</td>
</tr>
<tr>
<td>7</td>
<td>2009</td>
<td>Berlin, Berlin, Germany</td>
<td>within a day</td>
<td>high</td>
<td>f</td>
<td>Prenzlauer Berg</td>
<td>3</td>
<td>3</td>
<td>t</td>
<td>f</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg Nordwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10437</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Apartment</td>
<td>Entire home/apt</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>Real Bed</td>
<td>N/A</td>
<td>70</td>
<td>offer discount</td>
<td>offer discount</td>
<td>500</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>90</td>
<td>1125</td>
<td>1</td>
<td>t</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>129</td>
<td>2018/11/7</td>
<td>23</td>
<td>2010</td>
<td>2018</td>
<td>93</td>
<td>10</td>
<td>10</td>
<td>9</td>
<td>10</td>
<td>9</td>
<td>9</td>
<td>t</td>
<td>No</td>
<td>f</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>3</td>
<td>0.23</td>
</tr>
<tr>
<td>10</td>
<td>2010</td>
<td>Berlin, Berlin, Germany</td>
<td>within an hour</td>
<td>high</td>
<td>t</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg S√ºdwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10405</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Other</td>
<td>Private room</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>Real Bed</td>
<td>N/A</td>
<td>45</td>
<td>offer discount</td>
<td>offer discount</td>
<td>0</td>
<td>18</td>
<td>1</td>
<td>26</td>
<td>3</td>
<td>30</td>
<td>7</td>
<td>t</td>
<td>8</td>
<td>18</td>
<td>42</td>
<td>42</td>
<td>2018/11/7</td>
<td>279</td>
<td>2010</td>
<td>2018</td>
<td>96</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>t</td>
<td>No</td>
<td>f</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>2.83</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Step-2-Feature-Selection"><a href="#Step-2-Feature-Selection" class="headerlink" title="Step 2: Feature Selection"></a>Step 2: Feature Selection</h3><p>As mentioned in last blog, features has been engineered, including transform the format and create more effecient and useful features. Now is the step of feature selection. Not all the variables will be used in the linear regression model. There are three reasons. One is that some data is unique for each datapoint like id, host_id. These features are very easy  to identify, and I have been removed in data cleaning. The second thing is another extreme situation, where all the value for every datapoint is same or constant. These features are named as “Zero Variance Feature”, which are useless and redundant. I will detect them by primary value analysis. When one feature’s primary value ratio is over a critical level, the feature has less predictable ability. The third one is that there are strong correlations between variables. I use correlation coefficient and VIF to detect</p>
<h4 id="1-Primary-Value-Ratio"><a href="#1-Primary-Value-Ratio" class="headerlink" title="1. Primary Value Ratio"></a>1. Primary Value Ratio</h4><p>Delect the feature whose primary value ratio over 80%</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">RowNumber = nrow(PB_lm)</span><br><span class="line">cols = colnames(PB_lm)</span><br><span class="line">Primary_Value_Ratio = c()</span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> cols)&#123;</span><br><span class="line">        Primary_Value_Ratio = c(Primary_Value_Ratio,max(table(PB_lm[i]))/RowNumber) <span class="comment"># calculate the ratio</span></span><br><span class="line">&#125;</span><br><span class="line">Primary_Value = data.frame(cols,Primary_Value_Ratio)</span><br><span class="line">Primary_Value[Primary_Value$Primary_Value_Ratio&gt;<span class="number">0.8</span>,]</span><br><span class="line"></span><br><span class="line">PB_lm = select(PB_lm,-Primary_Value[Primary_Value$Primary_Value_Ratio&gt;<span class="number">0.8</span>,]$cols)</span><br></pre></td></tr></table></figure>
<h4 id="2-Correlation-Analysis"><a href="#2-Correlation-Analysis" class="headerlink" title="2. Correlation Analysis"></a>2. Correlation Analysis</h4><p>Correlation analysis is a statistical method used to evaluate the strength of relationship between two quantitative variables. A high correlation means that two or more variables have a strong relationship with each other, while a weak correlation means that the variables are hardly related. In linear regression model, one model assumption is that variables should be independent of one another. Otherwises, one variable will be explained by other. </p>
<p><img src="correlation.png" alt="images"></p>
<h4 id="3-Skewness-of-Dependent-Variable"><a href="#3-Skewness-of-Dependent-Variable" class="headerlink" title="3. Skewness of Dependent Variable"></a>3. Skewness of Dependent Variable</h4><p>Before we detect multicollinearity problem, I transformed price into normal distribution. Price follows right-skewed Distribution. It is a common phenomenon for most real-life variables. However, for linear regression model, it is essential for residual of the model to follow normal distribution. And then the response variable will also follow. Here I used log transformation to make response variable into normal distribution. The new repsonce variable should be log(price). For log transforming, the data point which price is 0 should be deleted. </p>
<h3 id="Step-3-Run-OLS-and-check-for-linear-regression-assumptions"><a href="#Step-3-Run-OLS-and-check-for-linear-regression-assumptions" class="headerlink" title="Step 3: Run OLS and check for linear regression assumptions"></a>Step 3: Run OLS and check for linear regression assumptions</h3><p>The OLS model is the most common estimation method for linear models, and will provide us with the simplest linear regression model to base our future models off of. It’s always good to start simple then add complexity. In addition, regression model is a powerful analysis that can analyze multiple variables simultaneously to answer complex research questions. However, if you don’t satisfy the OLS assumptions, you might not be able to trust the results. OLS model is a great place to check for linear regression assumptions.</p>
<h4 id="1-Get-Variance-Inflation-Factors-VIFs-to-Detect-Multicollinearity"><a href="#1-Get-Variance-Inflation-Factors-VIFs-to-Detect-Multicollinearity" class="headerlink" title="1. Get Variance Inflation Factors (VIFs) to Detect Multicollinearity"></a>1. Get Variance Inflation Factors (VIFs) to Detect Multicollinearity</h4><figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Build the initial model</span></span><br><span class="line">lm_1 &lt;- lm(log_price ~., data = train_data3) <span class="comment"># data should been splitted into training dataset and test dataset.</span></span><br><span class="line"></span><br><span class="line">lendfitback &lt;- step(lm_1,direction = <span class="string">&quot;backward&quot;</span>) </span><br><span class="line">summary(lendfitback)</span><br><span class="line">vif(lendfitback,digits = <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>VIFs are not produced by the OLS table so you should manually extract them. They are a great way to check for multicollinearity in your model. Multicollinearity is when there is high correlation between your features. It is an assumption of linear regression that your data does not have multicollinearity, so make sure to check this. You want your VIFs under 7.</p>
<p>Here, VIFs values of neighbourhood_cleansed, zipcode, first_review are large, which indicates that there is significant Multicollinearity problem with 3 features. Therefore, we delete them and rebuild a new model. </p>
<h4 id="2-Check-Residuals-of-Model"><a href="#2-Check-Residuals-of-Model" class="headerlink" title="2. Check Residuals of Model"></a>2. Check Residuals of Model</h4><p>Also, another important asssumption is about residuals.</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Build the second model without 3 multicollinearity features</span></span><br><span class="line">lm_2 &lt;- lm(log_price ~. - neighbourhood_cleansed - zipcode-first_review, data = train_data3)</span><br><span class="line">lendfitback_2 &lt;- step(lm_2,direction = <span class="string">&quot;backward&quot;</span>)</span><br><span class="line">plot(lendfitback_2)</span><br></pre></td></tr></table></figure>
<h5 id="Resuduals-vs-Fitted"><a href="#Resuduals-vs-Fitted" class="headerlink" title="Resuduals vs. Fitted"></a>Resuduals vs. Fitted</h5><p>Even though residuals of some outliers over the range of [-2,2], most of data points randomly distributes around 0. This model fits the data well</p>
<p><img src="Resuduals_vs._Fitted.png" alt="images"></p>
<h5 id="Normal-Q-Q-Plot"><a href="#Normal-Q-Q-Plot" class="headerlink" title="Normal Q_Q Plot"></a>Normal Q_Q Plot</h5><p>Some data points deviate from the diagonal, so residuals do not follow normal distribution strictly. </p>
<p><img src="Normal_QQ_Plot.png" alt="images"></p>
<h5 id="Scale-Location"><a href="#Scale-Location" class="headerlink" title="Scale-Location"></a>Scale-Location</h5><p>Seem to be constant with no trend. There is no serious problem in Heteroskedasticity.</p>
<p><img src="Scale-Location.png" alt="images"></p>
<h5 id="Residual-Independence"><a href="#Residual-Independence" class="headerlink" title="Residual Independence"></a>Residual Independence</h5><figure class="highlight r"><table><tr><td class="code"><pre><span class="line">durbinWatsonTest(lendfitback_2)</span><br></pre></td></tr></table></figure>
<p>In Durbin Watson Test, the p-value is larger than 0.05, we can assume errors are independent. In conclusion, the residuals can be regarded as stochastic error.</p>
<h3 id="Step-4-Prediction-in-valid-dataset-and-Summary"><a href="#Step-4-Prediction-in-valid-dataset-and-Summary" class="headerlink" title="Step 4: Prediction in valid dataset and Summary"></a>Step 4: Prediction in valid dataset and Summary</h3><p>After we check the residuals, we know the model is satisfied OLS assumptions. And we use the model to predict on validation dataset.</p>
<p><img src="summary.png" alt="images"></p>
<p>The adjusted r-squared is only 0.5277. This means only 52.77% variation has been explained by the multiple regression model. In general, the higher the R-squared, the better the model fits your data. Even though the R-squared is not closing to 1, we also can infer the sample data are well correspond to the fitted (assumed) model. In fact, R-squared doesn’t tell us the entire story. After we check the residual plots, we know the residuals independent and identically distributed in normal distribution. That is, this model fits the data well.</p>
<p>In valid dataset, RMSE is 0.4206808. The standard deviation of the predictions from the actual values in valid dataset would be 0.4206808. What’s more, RMSE in training dataset is 0.3775776. The difference between these two number is small. This tells us the model is good-fit and has generalization ability. The model has the ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.</p>
<p><strong>Thanks for reading!</strong></p>
<p><strong>Please let me know if there is something I should add. And if you enjoy it, share it with your friends and colleagues : )</strong></p>
<p><strong>另外博客的中文版本也会及时更新的！谢谢！</strong></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Airbnb Listings</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Exploring for Airbnb Dataset in Germany - Data Cleaning Challenge</title>
    <url>/yantongtt.github.io/2020/08/04/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany/</url>
    <content><![CDATA[<p>Even though these short term rentals may no longer be booked for vacation or leisure purposes because of coronavirus crackdown recently, we can not deny that Airbnb has seen a meteoric growth since its inception in 2008 with the number of rentals listed on its website growting exponentially every year. I wish this bussiness may flourish again one day, because it definitely makes a cozy place for living or travelling. In the past </p>
<p>I will be working with Prenzlauer Berg data, one neighborhood of Berlin. Located in the district of Pankow, Prenzlauer Berg is one of the most charismatic neighborhoods there, with countless of bars and cafes. For many reasons, Prenzlauer Berg is a common choice as a base for the visitors of Berlin. There are different varieties of Airbnb properties in this area, so let us driven into it!</p>
<a id="more"></a>
<p>Acturally, this project is done in R three months ago, including data preparation &amp; exploration, rental price prediction, clustering analysis to place rentals. I will write three blogs to illustrate my project here. </p>
<p>In this blog, I will perform an exploration anlysis at first. The Airbnb dataset is sourced from <a href="http://insideairbnb.com/get-the-data.html"><strong>Inside Airbnb</strong></a> website, including detailed listings data. </p>
<p><strong>A quick glance at the data shows that there are:</strong></p>
<ul>
<li>3899 unique listing in Prenzlauer Berg in total. And the first rental was up in August, 2008 by a host from Florida. </li>
<li>Rental price is ranging from 0 dollar to 5000 dollars for one night. Listing with $ 5000 price tag, a hostel with a little infomation about host and hostel, may be one outlier in dataset. </li>
</ul>
<p>In my oppinion, Data Cleaning is the most fundamental but essential work in data analysis. Not only because incorrect data can reduce the modeling effectiveness, but also because we can explore more deeper into this dataset. They exist side by side and play a part together. Without that understanding, we have no basis from which to make decisions about what data is relevant as we clean and prepare our data.</p>
<p>Data cleaning is the process of cleaning or standardising the data to make it ready for analysis. Most of times, we deal with discrepancies such as incorrect data formats, missing data, errors. It is irrational to delete data with missing values because they may reveal some information. Besides the type errors, there are still many reasons to make it. The Airbnb dataset is . It’s an interesting challenge. In this blog, I will illustrate how I cope with them.</p>
<h2 id="Handling-missing-values"><a href="#Handling-missing-values" class="headerlink" title="Handling missing values"></a>Handling missing values</h2><p>See how many missing data points we have</p>
<p>In the raw dataset, there are too many missing values in the form of entire rows with all NA value. We just delete them because there is no meaning. After we delete 1131 NA rows, let’s see how many we have in each column.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Feature</th>
<th>Number of NA</th>
<th>How it looks like</th>
</tr>
</thead>
<tbody>
<tr>
<td>square_feet</td>
<td>2695</td>
<td>NA 720 NA NA NA NA NA NA NA 1012</td>
</tr>
<tr>
<td>license</td>
<td>2564</td>
<td><NA> 03/Z/RA/003410-18 <NA></td>
</tr>
<tr>
<td>monthly_price</td>
<td>2392</td>
<td>“$1,000.00”, “​$1,001.00”, NA</td>
</tr>
<tr>
<td>weekly_price</td>
<td>2253</td>
<td>“$1,000.00”,”$1,050.00”,NA</td>
</tr>
<tr>
<td>notes</td>
<td>1894</td>
<td>“ I have a cat, please be aware if you have an allergy.”, NA</td>
</tr>
<tr>
<td>interaction</td>
<td>1530</td>
<td>“ feel free to contact me or my person of trust any time”,NA</td>
</tr>
<tr>
<td>host_response_time</td>
<td>1502</td>
<td>“a few days or more”, “within a week”, NA</td>
</tr>
<tr>
<td>host_response_rate</td>
<td>1502</td>
<td>“0%,”10%”,”100%”, NA</td>
</tr>
<tr>
<td>access</td>
<td>1462</td>
<td>“ Available inside the room there will be a kettle …”, NA</td>
</tr>
<tr>
<td>neighborhood_overview</td>
<td>1335</td>
<td>“ A mix of Celebrities the Young Creative Scene…”, NA</td>
</tr>
<tr>
<td>house_rules</td>
<td>1313</td>
<td>“<em> keine laute Musik </em> bitte nicht mit…” ,NA</td>
</tr>
<tr>
<td>host_about</td>
<td>1271</td>
<td>“We love to travel ourselves a lot and prefer to stay in  apartments…” NA</td>
</tr>
<tr>
<td>transit</td>
<td>1152</td>
<td>“The apartment is situated 3min on foot from the Station Hermannstr.  “,NA</td>
</tr>
<tr>
<td>security_deposit</td>
<td>1064</td>
<td>“$0.00”,”$1,000.00”, NA</td>
</tr>
<tr>
<td>cleaning_fee</td>
<td>768</td>
<td>“$0.00”,”$10.00”,NA</td>
</tr>
<tr>
<td>host_neighbourhood</td>
<td>606</td>
<td>“Adlershof”,”Alt-Hohenschönhausen”,NA</td>
</tr>
<tr>
<td>review_scores_value</td>
<td>505</td>
<td>NA 9 9 10 9 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_checkin</td>
<td>505</td>
<td>NA 9 10 10 9 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_location</td>
<td>504</td>
<td>NA 10 10 10 9 10 10 10 NA 10</td>
</tr>
<tr>
<td>review_scores_communication</td>
<td>502</td>
<td>NA 9 10 10 10 10 8 9 NA 9</td>
</tr>
<tr>
<td>review_scores_accuracy</td>
<td>502</td>
<td>NA 9 10 10 10 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_cleanliness</td>
<td>501</td>
<td>NA 9 10 10 10 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_rating</td>
<td>500</td>
<td>NA 92 96 100 93 96 87 94 NA 91</td>
</tr>
<tr>
<td>reviews_per_month</td>
<td>445</td>
<td>NA 1.25 1.75 0.15 0.23 2.83 0.75 0.18</td>
</tr>
<tr>
<td>first_review</td>
<td>445</td>
<td>2009-06-20,”2009-08-18”,NA</td>
</tr>
<tr>
<td>last_review</td>
<td>444</td>
<td>2010-09-16,”2011-01-26”,NA</td>
</tr>
<tr>
<td>summary</td>
<td>130</td>
<td>“Ich bin sehr stolz darauf meine Wohnung freundlichen  Berlinbesuchern”, NA</td>
</tr>
<tr>
<td>zipcode</td>
<td>83</td>
<td>10115,”10115\n10115”,NA</td>
</tr>
<tr>
<td>state</td>
<td>19</td>
<td>“berlin”,”Berlin”,NA</td>
</tr>
<tr>
<td>description</td>
<td>16</td>
<td>“ 24m2 room in Neukoelln with large double bed, …” NA</td>
</tr>
<tr>
<td>host_location</td>
<td>11</td>
<td>“Berlin, Berlin, Germany “,”Coledale, New South Wales,  Australia”, NA</td>
</tr>
<tr>
<td>market</td>
<td>9</td>
<td>Berlin,”Juarez”,NA</td>
</tr>
<tr>
<td>name</td>
<td>8</td>
<td>“\tThe right place for your  stay”, NA</td>
</tr>
<tr>
<td>bathrooms</td>
<td>7</td>
<td>NA 1.0 1.0 2.5 1.0 1.0</td>
</tr>
<tr>
<td>beds</td>
<td>2</td>
<td>NA 2 2   7 1 1</td>
</tr>
<tr>
<td>host_total_listings_count</td>
<td>1</td>
<td>NA 1 1   1 3 1</td>
</tr>
<tr>
<td>host_since</td>
<td>1</td>
<td>“2008-10-19”, “2009-05-16”,  “2009-08-25”,”2009-11-18”, NA</td>
</tr>
<tr>
<td>host_name</td>
<td>1</td>
<td>“Britta”,”Bright”,”Philipp”, “ Chris +  Oliver”, “Wolfram”,NA</td>
</tr>
<tr>
<td>host_listings_count</td>
<td>1</td>
<td>NA 1 1 1 3 1 1 2 NA 2 …</td>
</tr>
<tr>
<td>host_is_superhost</td>
<td>1</td>
<td>“f”,”t”, NA</td>
</tr>
<tr>
<td>host_identity_verified</td>
<td>1</td>
<td>“f”,”t”, NA</td>
</tr>
<tr>
<td>host_has_profile_pic</td>
<td>1</td>
<td>“f”,”t”, NA</td>
</tr>
<tr>
<td>city</td>
<td>1</td>
<td>Berlin Berlin Berlin Berlin Berlin  NA</td>
</tr>
<tr>
<td>bedrooms</td>
<td>1</td>
<td>1 1 4 0 1 2 2 2 0 2</td>
</tr>
<tr>
<td>street</td>
<td>0</td>
<td>“\nKreuzberg, Berlin, Germany”…</td>
</tr>
<tr>
<td>smart_location</td>
<td>0</td>
<td>“\nKreuzberg, Germany”…</td>
</tr>
<tr>
<td>room_type</td>
<td>0</td>
<td>Entire home/apt  Private  room   Shared room</td>
</tr>
<tr>
<td>requires_license</td>
<td>0</td>
<td>f  t</td>
</tr>
<tr>
<td>require_guest_profile_picture</td>
<td>0</td>
<td>f  t</td>
</tr>
<tr>
<td>require_guest_phone_verification</td>
<td>0</td>
<td>f  t</td>
</tr>
<tr>
<td>property_type</td>
<td>0</td>
<td>Apartment  Apartment  Apartment   Apartment  Condominium …</td>
</tr>
<tr>
<td>price</td>
<td>0</td>
<td>$0.00,”$1,000.00”</td>
</tr>
<tr>
<td>number_of_reviews</td>
<td>0</td>
<td>143 197 6 23 279 56 18 163 28 69</td>
</tr>
<tr>
<td>neighbourhood_group_cleansed</td>
<td>0</td>
<td>Charlottenburg-Wilm.,…</td>
</tr>
<tr>
<td>neighbourhood_cleansed</td>
<td>0</td>
<td>Adlershof,”Albrechtstr.”…</td>
</tr>
<tr>
<td>neighbourhood</td>
<td>0</td>
<td>Adlershof,”Alt-Hohenschönhausen” …</td>
</tr>
<tr>
<td>minimum_nights</td>
<td>0</td>
<td>62 2 6 90 3 3 3 3 90 60</td>
</tr>
<tr>
<td>maximum_nights</td>
<td>0</td>
<td>1125 10 14 1125 30 30 21 1125 1125 365</td>
</tr>
<tr>
<td>longitude</td>
<td>0</td>
<td>13.4 13.4 13.4 13.4 13.4</td>
</tr>
<tr>
<td>latitude</td>
<td>0</td>
<td>52.5 52.5 52.5 52.5 52.5</td>
</tr>
<tr>
<td>last_scraped</td>
<td>0</td>
<td>2018-11-07 2018-11-09</td>
</tr>
<tr>
<td>is_location_exact</td>
<td>0</td>
<td>“f”,”t”</td>
</tr>
<tr>
<td>is_business_travel_ready</td>
<td>0</td>
<td>“f”,”t”</td>
</tr>
<tr>
<td>instant_bookable</td>
<td>0</td>
<td>“f”,”t”</td>
</tr>
<tr>
<td>id</td>
<td>0</td>
<td>3176 7071 9991 14325 17409 20858 24569</td>
</tr>
<tr>
<td>host_verifications</td>
<td>0</td>
<td>“[‘email’, ‘facebook’, ‘jumio’, ‘offline_government_id’,  ‘government_id’]”…</td>
</tr>
<tr>
<td>host_id</td>
<td>0</td>
<td>3718 17391 33852 55531 67590</td>
</tr>
<tr>
<td>has_availability</td>
<td>0</td>
<td>“t”</td>
</tr>
<tr>
<td>guests_included</td>
<td>0</td>
<td>2 1 5 1 1 2 2 4 1 2</td>
</tr>
<tr>
<td>extra_people</td>
<td>0</td>
<td>$0.00”,”$10.00”,…</td>
</tr>
<tr>
<td>country_code</td>
<td>0</td>
<td>“DE”</td>
</tr>
<tr>
<td>country</td>
<td>0</td>
<td>“Germany”</td>
</tr>
<tr>
<td>cancellation_policy</td>
<td>0</td>
<td>“flexible”,”moderate”,..</td>
</tr>
<tr>
<td>calendar_updated</td>
<td>0</td>
<td>“ 1 week ago”, “10 months ago”, “11 months  ago”, “12 months ago”…</td>
</tr>
<tr>
<td>calendar_last_scraped</td>
<td>0</td>
<td>“2018-11-07”,”2018-11-09”</td>
</tr>
<tr>
<td>calculated_host_listings_count</td>
<td>0</td>
<td>1  2  3    4  5  6    7  8  9   10  11  12   13  15  16   19  45</td>
</tr>
<tr>
<td>bed_type</td>
<td>0</td>
<td>Airbed     Couch     Futon Pull-out Sofa   Real Bed</td>
</tr>
<tr>
<td>availability_90</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>availability_60</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>availability_365</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>availability_30</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>amenities</td>
<td>0</td>
<td>{Internet,Wifi,Kitchen,”Buzzer/wireless intercom”,Crib}…</td>
</tr>
<tr>
<td>accommodates</td>
<td>0</td>
<td>1  2    3  4  5    6  7  8    9  10  11   12  16</td>
</tr>
</tbody>
</table>
</div>
<p> That seems like a lot! For almost cells in this dataset are empty. In the next step, we are going to take a closer look at some of the columns with missing values and try to fighure out what might be going on with them,</p>
<h3 id="Figure-out-why-the-data-is-missing"><a href="#Figure-out-why-the-data-is-missing" class="headerlink" title="Figure out why the data is missing"></a>Figure out why the data is missing</h3><p>This is the point at which we get into the part of data science that I like to call “data intution”, by which I mean “really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis”. It can be a frustrating part of data science, especially if you’re newer to the field and don’t have a lot of experience. For dealing with missing values, you’ll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:</p>
<p>Before we impute the missing value, it is a critical point to get into this part. This helps us to get “data intuition” by really looking at your dataset and trying to figure out why it is this way and how that will affect your analysis. Keep the feature or not is a really important question for analysis, and you can ask yourself <strong>“Is this value missing becuase it wasn’t recorded or becuase it dosen’t exist?”</strong> </p>
<p>For the first thing, like square_feet of house is existed but some hosts do not input this kind of data for many possible reasons, we should consider to impute them in a proper way or delete them directly. These value you probaly do not want to keep them as NA. Also, you can try to guess what it might have been based on the other values in that column and row. However, if the value is missing because it doesn’t exist, for example monthly_price and weekly_price for some listings, we should consider feature engineering to transform them into more meaningful features. Some listings do not have monthly prices or weekly price because they do not offer the discounts for long-term renting. It tells us a lot! Something like “probably the host want to make profit by offering discount because his/her house is more suitable for  long-term accommodation. Maybe no visitor who traveling here wants to live in this place. Or the host think only rent for one day can not get profit, and he/she have to clean or manage this accommodation everyday. In this way, I made new features about how many discounts that the house provide. (I will talk it in the latter content.)</p>
<h3 id="Ways-to-handle-missing-values"><a href="#Ways-to-handle-missing-values" class="headerlink" title="Ways to handle missing values"></a>Ways to handle missing values</h3><p>Let me talk about the way to handle missing values in many situations. </p>
<ul>
<li><strong>Drop Columns with Missing Values</strong></li>
</ul>
<p>Even though square_feet is an significant variable toward rent price, there are too many missing value here. inproper imputation may bring more bias. Let take mean imputation for an example. If we impute missing value as mean value, and regard it is missing randomly, then the mean estimator is unbiased. This is the logic to achieve the mean i putation. However, if we do in this way, we do not keep the relationships with other variables. Statistically, the estimator is unbiased, but the standard deviation is biased. And most of our interest is to find the relationships, so mean imputation is not a good choice.  In this case, we can not use simple option to fill square_feet with the null value. Also, we can not find any straight relationships with this variable.  Take many factors into consideration, we just drop this column directly. </p>
<ul>
<li><p><strong>Feature Engineering</strong></p>
<ul>
<li>Whether or not</li>
</ul>
<p>license is a feature discribe the host’s license. If host has that, they should upload the exact license code here. Acturally, we don’t care about the exact text information on that. Whether has license or not is more meaningful for us. Therefore, I transform the license code as “T”, and missing value as “F”. “T” is the abbreviation of “true”, representing the host has license code for the accomendation. While “F” is “false”, which means they do have license code. </p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">PB_3$license[which(is.na(PB_3$license)==<span class="literal">F</span>)] = <span class="string">&quot;Yes&quot;</span></span><br><span class="line">PB_3$license[which(is.na(PB_3$license)==<span class="literal">T</span>)] = <span class="string">&quot;No&quot;</span></span><br></pre></td></tr></table></figure>
<p>The dataset still has some variables which can handle in the same method.</p>
<ul>
<li>Date to Time </li>
</ul>
<p>First review date is an important feature for analysis. It tells us how long does the host operating this accomendation. However, as a category variable, first_review will be re-encoded as many 0/1 variables. We can transform the date as days, a numeric variable. How to transform? Create a new feature.</p>
<p>Here, I created a new feature Business_interval, which measures number of day after this airbnb has first review. Also, same method works on other date variables, like last_review, host_since. I created a feature named No_Business_interval. This measures number of day after the last review. Longer interval means this airbnb is no business recently. And the third feature is operation_interval, which is number of day after host_since. This measure how long that the airbnb is. </p>
<p>There are missing value in first_review and last_review the two variables. The reason would be there is no review. Therefore, the time interval would be 0.</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">first_review = as.Date(PB_2$first_review,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">last_review = as.Date(PB_2$last_review,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">host_since = as.Date(PB_2$host_since,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">Business_interval = today() - first_review</span><br><span class="line">No_Business_interval = today() - last_review</span><br><span class="line">operation_interval = today() - host_since</span><br><span class="line"></span><br><span class="line">Business_interval[is.na(Business_interval) == <span class="literal">T</span>] = <span class="number">0</span></span><br><span class="line">No_Business_interval[is.na(No_Business_interval) == <span class="literal">T</span>] = <span class="number">0</span> </span><br><span class="line">operation_interval[is.na(operation_interval) == <span class="literal">T</span>] = <span class="number">0</span> </span><br></pre></td></tr></table></figure>
<ul>
<li>Binning into category</li>
</ul>
<p>In my oppinion, missing is also information on that feature. I dont like delete or impute them directly. Sometimes, I’d like to keep them, making a new value for them. That is, I will create a new level named, “N/A” for the missing value. However, problems come out. If we use “N/A” to present the missing value, then the variable will be categorical feature. It is okay for the features with a small amount of values like gender. After created new level, gender can be “female”, “male”, “don’t know”. But for numeric variables like host_response_rate, value is numeric ranging from 0 to 100%. In this case, I created new level as “N/A” (not available), and binned the rate into two group, high rate(host_response_rate &gt;= 80%) and low rate(host_response_rate &lt; 80%). The information will be reducted but it is more effeciently for analysis and do not influence the analysis a lot. </p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># transform missing value to &quot;N/A&quot;(not available). create a level for null value</span></span><br><span class="line">PB_3$host_response_time[is.na(PB_3$host_response_time)] &lt;- <span class="string">&quot;N/A&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># host_response_rate &#123;high: host_response_rate &gt;= 80%&#125;, &#123;low: host_response_rate &lt; 80%&#125;, &#123;N/A: not available&#125;</span></span><br><span class="line">PB_3$host_response_rate &lt;- as.numeric(sub(<span class="string">&quot;%&quot;</span>,<span class="string">&quot;&quot;</span>,PB_3$host_response_rate))/<span class="number">100</span></span><br><span class="line">summary(as.factor(PB_3$host_response_rate))</span><br><span class="line">PB_3$host_response_rate[PB_3$host_response_rate&gt;= <span class="number">0.8</span>] &lt;- <span class="string">&quot;high&quot;</span></span><br><span class="line">PB_3$host_response_rate[PB_3$host_response_rate&lt; <span class="number">0.8</span>] &lt;- <span class="string">&quot;low&quot;</span></span><br><span class="line">PB_3$host_response_rate[is.na(PB_3$host_response_rate)] &lt;- <span class="string">&quot;N/A&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Other feature engineering</li>
</ul>
<p>As I mention above, we can use monthly_price and weekly_price to create new features about the discount that host offered. In this step, we can not use the exact number from weekly_price and monthly_price. Because the two prices are calculated after we decide the daily price. That is, it will be higher when daily price is high. However, we can measure whether the airbnb provide the discount or not. If providing, we use the value as “offer discount”, and if not, the value will be “no discount”.</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(PB_lm$weekly_price)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (is.na(PB_lm$weekly_price[i]) == <span class="literal">T</span>)&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = PB_lm$price[i]*<span class="number">7</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">weekly_discount_ratio = (PB_lm$weekly_price)/(PB_lm$price*<span class="number">7</span>)</span><br><span class="line">weekly_discount_ratio[is.na(weekly_discount_ratio) == <span class="literal">T</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment">#summary(weekly_discount_ratio)</span></span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(weekly_discount_ratio))&#123;</span><br><span class="line">        <span class="keyword">if</span> (weekly_discount_ratio[i] &lt; <span class="number">1</span> )&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = <span class="string">&quot;offer discount&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = <span class="string">&quot;no discount&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">PB_lm$weekly_price = as.factor(PB_lm$weekly_price)</span><br><span class="line"><span class="comment">#summary(PB_lm$weekly_price)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><strong>Filling in missing values automatically</strong></li>
</ul>
<p>Sure, I mentioned that I do not like mean imputation in <em>Drop Columns with Missing Values</em>. But the situation is on condition that mean value is meaningless. Acturally, we imputate the missing value using the idea of maximum likelihood. Some features like host_has_profile_pic, presenting whether host has profile picture or not, have most value of “t”. Most of host upload their profile picture on the sites. Therefore, we can use mode value to impute the missing value. </p>
<h2 id="Inconsistent-Data-Entry"><a href="#Inconsistent-Data-Entry" class="headerlink" title="Inconsistent Data Entry"></a>Inconsistent Data Entry</h2><h2 id="Scale-and-Normalize-Data"><a href="#Scale-and-Normalize-Data" class="headerlink" title="Scale and Normalize Data"></a>Scale and Normalize Data</h2><h2 id="Parsing-Dates"><a href="#Parsing-Dates" class="headerlink" title="Parsing Dates"></a>Parsing Dates</h2>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Airbnb Listings</tag>
        <tag>Exploratory Analysis</tag>
        <tag>Data Cleaning</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
  <entry>
    <title>Do Your Customers Want to Renew Fast-Pass?</title>
    <url>/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/</url>
    <content><![CDATA[<p>Instead of simple upmarket pricing system, some amusement parks adopt the price steategy to attract more customer percepetions and optimize the total revenue with “fast-pass” tickets. Remember when we went to the Universal, some of us must be willing to pay extra fee on the fast-pass for accessing to theme park in advance. That saves a lot of time! </p>
<p>Seasonal fast-pass is another type which allows visitors renewing for next season. In fact, even though the visitors enjoyed theme park, only fewer part  of them are willing to renew the fast-pass. In this case, the analysis on building a model that classifies households as season pass renewers or non-season pass renewers may help managers better understand their target customers. That is, understanding what consumers want and need is an ongoing imperative for marketing strategy. Machine Learning can make this job much easier and efficient.</p>
<p>In this post, I will present some frequently-used machine learing algorithms, which are Logistic Regression, Random Forest and XGBoost,  to develop prediction models. </p>
<a id="more"></a>
<h2 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a>Exploratory Data Analysis</h2><p>This dataset records the historical spending and personal information of Greater NYC households that held theme park family season passes. We start with checking out how our data looks like and visualize how it interacts with our label (the final column, renew or not). Let’s start with importing our data and print the random ten rows:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>householdID</th>
<th>visits</th>
<th>avgrides_perperson</th>
<th>avgmerch_perperson</th>
<th>avggoldzone_perperson</th>
<th>avgfood_perperson</th>
<th>goldzone_playersclub</th>
<th>own_car</th>
<th>homestate</th>
<th>FB_Like</th>
<th>renew</th>
</tr>
</thead>
<tbody>
<tr>
<td>2913</td>
<td>2</td>
<td>6.3</td>
<td>26.6</td>
<td>109.6</td>
<td>39.8</td>
<td>0</td>
<td>1</td>
<td>NY</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1613</td>
<td>4</td>
<td>12.4</td>
<td>15.6</td>
<td>82.8</td>
<td>62.5</td>
<td>0</td>
<td>1</td>
<td>NY</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1216</td>
<td>3</td>
<td>11.2</td>
<td>36.3</td>
<td>62.5</td>
<td>6.8</td>
<td>0</td>
<td>1</td>
<td>CT</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1155</td>
<td>1</td>
<td>8.1</td>
<td>19.2</td>
<td>9.6</td>
<td>42.2</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1110</td>
<td>18</td>
<td>11</td>
<td>10.7</td>
<td>69.9</td>
<td>16.2</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>865</td>
<td>3</td>
<td>11.5</td>
<td>73.4</td>
<td>166.3</td>
<td>4.6</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>852</td>
<td>2</td>
<td>13.2</td>
<td>25.9</td>
<td>26.7</td>
<td>43.3</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>586</td>
<td>1</td>
<td>9.7</td>
<td>52.8</td>
<td>26.7</td>
<td>39.3</td>
<td>0</td>
<td>0</td>
<td>NJ</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>346</td>
<td>9</td>
<td>10.4</td>
<td>25.4</td>
<td>132.6</td>
<td>8.6</td>
<td>0</td>
<td>1</td>
<td>CT</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>172</td>
<td>2</td>
<td>2.2</td>
<td>26.1</td>
<td>40</td>
<td>12.4</td>
<td>0</td>
<td>1</td>
<td>CT</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>A better way to see all the columns and their data type is using <strong>.info()</strong> method:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nyc_historical.info()</span><br><span class="line"><span class="comment"># &lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span></span><br><span class="line"><span class="comment"># RangeIndex: 3200 entries, 0 to 3199</span></span><br><span class="line"><span class="comment"># Data columns (total 11 columns):</span></span><br><span class="line"><span class="comment">#  #   Column                 Non-Null Count  Dtype  </span></span><br><span class="line"><span class="comment"># ---  ------                 --------------  -----  </span></span><br><span class="line"><span class="comment">#  0   householdID            3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  1   visits                 3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  2   avgrides_perperson     3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  3   avgmerch_perperson     3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  4   avggoldzone_perperson  3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  5   avgfood_perperson      3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  6   goldzone_playersclub   3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  7   own_car                3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  8   homestate              3200 non-null   object </span></span><br><span class="line"><span class="comment">#  9   FB_Like                3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  10  renew                  3200 non-null   int64  </span></span><br><span class="line"><span class="comment"># dtypes: float64(4), int64(6), object(1)</span></span><br><span class="line"><span class="comment"># memory usage: 275.1+ KB</span></span><br></pre></td></tr></table></figure>
<p>There is no missing value. Also, our dataset falls under two categories: one is categorical features: own_car, homestate, FB_Like renew, and another is numerical features: visits, avgrides_perperson, avgmerch_perperson… </p>
<h3 id="Target-Variable"><a href="#Target-Variable" class="headerlink" title="Target Variable"></a>Target Variable</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nyc_historical.renew.value_counts()</span><br><span class="line"><span class="comment"># 1    2126</span></span><br><span class="line"><span class="comment"># 0    1074</span></span><br><span class="line"><span class="comment"># Name: renew, dtype: int64</span></span><br></pre></td></tr></table></figure>
<p>This dataset is imbalanced, nearly 66% householders renewing the pass card. But fortunately, it is an extreme case. Dealing with slight imbalanced data, we will measure model performance with recall, F1 score, ROC curve, and its AUC</p>
<h3 id="Home-State"><a href="#Home-State" class="headerlink" title="Home State"></a>Home State</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nyc_historical.homestate.value_counts()</span><br><span class="line"><span class="comment"># NJ    1076</span></span><br><span class="line"><span class="comment"># NY    1065</span></span><br><span class="line"><span class="comment"># CT    1059</span></span><br><span class="line"><span class="comment"># Name: homestate, dtype: int64</span></span><br></pre></td></tr></table></figure>
<p>That seems like our sample source are balanced in terms of regional distribution. </p>
<p>Now, let’s start our feature engineering!</p>
<h2 id="Feature-Engineering-and-Selection"><a href="#Feature-Engineering-and-Selection" class="headerlink" title="Feature Engineering and Selection"></a>Feature Engineering and Selection</h2><p>As a side note, in the dataset we have, homestate column is string with three values. We convert it to integer to make it easier to use in our analysis.</p>
<h3 id="One-Hot-Encoding"><a href="#One-Hot-Encoding" class="headerlink" title="One-Hot Encoding"></a>One-Hot Encoding</h3><p>A method to create new columns out of categorical ones by assigning 0 &amp; 1s</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">homestate_dummy = pd.get_dummies(nyc_historical.homestate)</span><br><span class="line">homestate_dummy = homestate_dummy.drop([<span class="string">&quot;CT&quot;</span>], axis=<span class="number">1</span>)</span><br><span class="line">homestate_dummy.rename(columns = &#123;<span class="string">&#x27;NJ&#x27;</span>:<span class="string">&#x27;homestate_NJ&#x27;</span>&#125;, inplace = <span class="literal">True</span>) </span><br><span class="line">homestate_dummy.rename(columns = &#123;<span class="string">&#x27;NY&#x27;</span>:<span class="string">&#x27;homestate_NY&#x27;</span>&#125;, inplace = <span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">renew_data = pd.concat([nyc_historical,homestate_dummy],axis = <span class="number">1</span>)</span><br><span class="line">renew_data = renew_data.drop([<span class="string">&#x27;homestate&#x27;</span>],axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><h4 id="1-Compute-the-primary-value-ratio-of-each-variable"><a href="#1-Compute-the-primary-value-ratio-of-each-variable" class="headerlink" title="1. Compute the primary value ratio of each variable."></a>1. Compute the primary value ratio of each variable.</h4><p>If the ratio is larger than 85%, just delete it. We only keep the variables which can identify the target variable.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">primaryvalue_ratio</span>(<span class="params">data</span>):</span></span><br><span class="line">    recordcount = data.shape[<span class="number">0</span>]  <span class="comment">#number of row</span></span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> data.columns:</span><br><span class="line">        primaryvalue = data[col].value_counts().index[<span class="number">0</span>]</span><br><span class="line">        ratio = float(data[col].value_counts().iloc[<span class="number">0</span>])/recordcount</span><br><span class="line">        x.append([primaryvalue,ratio])</span><br><span class="line">    feature_primaryvalue_ratio = pd.DataFrame(x,index = data.columns)</span><br><span class="line">    feature_primaryvalue_ratio.columns = [<span class="string">&quot;primaryvalue&quot;</span>,<span class="string">&quot;primaryvalue_ratio&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> feature_primaryvalue_ratio</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = primaryvalue_ratio(X_train)</span><br><span class="line">d.sort_values([<span class="string">&#x27;primaryvalue_ratio&#x27;</span>], ascending=[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>primaryvalue</th>
<th>primaryvalue_ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>goldzone_playersclub</td>
<td>0</td>
<td>0.821429</td>
</tr>
<tr>
<td>own_car</td>
<td>1</td>
<td>0.750446</td>
</tr>
<tr>
<td>homestate_NJ</td>
<td>0</td>
<td>0.663393</td>
</tr>
<tr>
<td>homestate_NY</td>
<td>0</td>
<td>0.662054</td>
</tr>
<tr>
<td>FB_Like</td>
<td>0</td>
<td>0.5375</td>
</tr>
<tr>
<td>visits</td>
<td>2</td>
<td>0.144643</td>
</tr>
<tr>
<td>avgrides_perperson</td>
<td>10</td>
<td>0.022321</td>
</tr>
<tr>
<td>avgfood_perperson</td>
<td>18.4</td>
<td>0.005804</td>
</tr>
<tr>
<td>avgmerch_perperson</td>
<td>23.1</td>
<td>0.004464</td>
</tr>
<tr>
<td>avggoldzone_perperson</td>
<td>85.8</td>
<td>0.003571</td>
</tr>
</tbody>
</table>
</div>
<p>As we can see, there is no variable being demonated by its primary value. Therefore, we keep all the variables. (we set the criticle point as 85%)</p>
<h4 id="2-Check-the-missing-value"><a href="#2-Check-the-missing-value" class="headerlink" title="2. Check the missing value"></a>2. Check the missing value</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">na_table = X_train.isnull().sum(axis = <span class="number">0</span>)/X_train.shape[<span class="number">0</span>]</span><br><span class="line">na_table <span class="comment"># there is no missing value</span></span><br><span class="line"><span class="comment"># visits                   0.0</span></span><br><span class="line"><span class="comment"># avgrides_perperson       0.0</span></span><br><span class="line"><span class="comment"># avgmerch_perperson       0.0</span></span><br><span class="line"><span class="comment"># avggoldzone_perperson    0.0</span></span><br><span class="line"><span class="comment"># avgfood_perperson        0.0</span></span><br><span class="line"><span class="comment"># goldzone_playersclub     0.0</span></span><br><span class="line"><span class="comment"># own_car                  0.0</span></span><br><span class="line"><span class="comment"># FB_Like                  0.0</span></span><br><span class="line"><span class="comment"># homestate_NJ             0.0</span></span><br><span class="line"><span class="comment"># homestate_NY             0.0</span></span><br><span class="line"><span class="comment"># dtype: float64</span></span><br></pre></td></tr></table></figure>
<p>There is no missing value.</p>
<h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><h3 id="Traning-Test-Split"><a href="#Traning-Test-Split" class="headerlink" title="Traning Test Split"></a>Traning Test Split</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X = renew_data[[<span class="string">&quot;visits&quot;</span>,<span class="string">&quot;avgrides_perperson&quot;</span>,<span class="string">&quot;avgmerch_perperson&quot;</span>,<span class="string">&quot;avggoldzone_perperson&quot;</span>,<span class="string">&quot;avgfood_perperson&quot;</span>,<span class="string">&quot;goldzone_playersclub&quot;</span>,</span><br><span class="line"><span class="string">&quot;own_car&quot;</span>,<span class="string">&quot;FB_Like&quot;</span>,<span class="string">&quot;homestate_NJ&quot;</span>,<span class="string">&quot;homestate_NY&quot;</span>]]</span><br><span class="line">y = renew_data[<span class="string">&quot;renew&quot;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">21</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Part-I-Logistic-Regression-Model"><a href="#Part-I-Logistic-Regression-Model" class="headerlink" title="Part I: Logistic Regression Model"></a>Part I: Logistic Regression Model</h3><p>Predicting renew ro not is a binary classification problem. Along with being a robust model, Logistic Regression provides interpretable outcomes. Let me sort out our steps to follow for building a Logistic Regression model:</p>
<ol>
<li>Prepare the data (inputs for the model)</li>
<li>Fit the model and see the model summary</li>
</ol>
<h4 id="Prepare-the-data-inputs-for-the-model"><a href="#Prepare-the-data-inputs-for-the-model" class="headerlink" title="Prepare the data (inputs for the model)"></a>Prepare the data (inputs for the model)</h4><p>Logistic regression is a linear model, therefore, we should do the correlation analysis to remove the multicorreltion</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cor_table = X_train.corr()</span><br><span class="line">cor_table</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>visits</th>
<th>avgrides_perperson</th>
<th>avgmerch_perperson</th>
<th>avggoldzone_perperson</th>
<th>avgfood_perperson</th>
<th>goldzone_playersclub</th>
<th>own_car</th>
<th>FB_Like</th>
<th>homestate_NJ</th>
<th>homestate_NY</th>
</tr>
</thead>
<tbody>
<tr>
<td>visits</td>
<td>1</td>
<td>0.025168</td>
<td>0.006725</td>
<td>0.013401</td>
<td>0.00542</td>
<td>-0.00215</td>
<td>0.005085</td>
<td>0.00136</td>
<td>0.00836</td>
<td>-0.02133</td>
</tr>
<tr>
<td>avgrides_perperson</td>
<td>0.025168</td>
<td>1</td>
<td>-0.02006</td>
<td>-0.01301</td>
<td>-0.01464</td>
<td>-0.02224</td>
<td>-0.01931</td>
<td>-0.02509</td>
<td>0.005241</td>
<td>0.0181</td>
</tr>
<tr>
<td>avgmerch_perperson</td>
<td>0.006725</td>
<td>-0.02006</td>
<td>1</td>
<td>-0.00335</td>
<td>-0.01869</td>
<td>0.011748</td>
<td>-0.00907</td>
<td>-0.01407</td>
<td>0.003229</td>
<td>0.01602</td>
</tr>
<tr>
<td>avggoldzone_perperson</td>
<td>0.013401</td>
<td>-0.01301</td>
<td>-0.00335</td>
<td>1</td>
<td>-0.03123</td>
<td>-0.02063</td>
<td>0.012132</td>
<td>-0.02225</td>
<td>0.017494</td>
<td>0.025145</td>
</tr>
<tr>
<td>avgfood_perperson</td>
<td>0.00542</td>
<td>-0.01464</td>
<td>-0.01869</td>
<td>-0.03123</td>
<td>1</td>
<td>0.005289</td>
<td>-0.02512</td>
<td>0.037274</td>
<td>0.028503</td>
<td>-0.024</td>
</tr>
<tr>
<td>goldzone_playersclub</td>
<td>-0.00215</td>
<td>-0.02224</td>
<td>0.011748</td>
<td>-0.02063</td>
<td>0.005289</td>
<td>1</td>
<td>-0.03011</td>
<td>0.007014</td>
<td>0.018148</td>
<td>-0.0029</td>
</tr>
<tr>
<td>own_car</td>
<td>0.005085</td>
<td>-0.01931</td>
<td>-0.00907</td>
<td>0.012132</td>
<td>-0.02512</td>
<td>-0.03011</td>
<td>1</td>
<td>0.028009</td>
<td>-0.01711</td>
<td>-0.00674</td>
</tr>
<tr>
<td>FB_Like</td>
<td>0.00136</td>
<td>-0.02509</td>
<td>-0.01407</td>
<td>-0.02225</td>
<td>0.037274</td>
<td>0.007014</td>
<td>0.028009</td>
<td>1</td>
<td>0.009995</td>
<td>-0.03239</td>
</tr>
<tr>
<td>homestate_NJ</td>
<td>0.00836</td>
<td>0.005241</td>
<td>0.003229</td>
<td>0.017494</td>
<td>0.028503</td>
<td>0.018148</td>
<td>-0.01711</td>
<td>0.009995</td>
<td>1</td>
<td>-0.50893</td>
</tr>
<tr>
<td>homestate_NY</td>
<td>-0.02133</td>
<td>0.0181</td>
<td>0.01602</td>
<td>0.025145</td>
<td>-0.024</td>
<td>-0.0029</td>
<td>-0.00674</td>
<td>-0.03239</td>
<td>-0.50893</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">13</span>))</span><br><span class="line">sns.heatmap(cor_table.corr())</span><br></pre></td></tr></table></figure>
<p><img src="corr.png" alt="images"></p>
<p>As we can see, there is no high correlations in our dataset. Now, let’s build a logistic regression model.</p>
<h4 id="Fit-the-model-and-see-the-model-summary"><a href="#Fit-the-model-and-see-the-model-summary" class="headerlink" title="Fit the model and see the model summary"></a>Fit the model and see the model summary</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logmodel = LogisticRegression()</span><br><span class="line">logmodel.fit(X_train,y_train)</span><br><span class="line">logmodel.coef_</span><br><span class="line"><span class="comment"># array([[ 0.11772918,  0.03811318,  0.00485627,  0.00292365,  0.00243818,  0.52549573,  0.86123004, -0.10407854, -0.41440561, -0.27656558]])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prediction</span></span><br><span class="line">predictions = logmodel.predict(X_test)</span><br><span class="line"><span class="comment"># confusion matrix</span></span><br><span class="line">mat = confusion_matrix(predictions, y_test)</span><br><span class="line">sns.heatmap(mat, square=<span class="literal">True</span>, annot=<span class="literal">True</span>, cbar=<span class="literal">False</span>,fmt=<span class="string">&#x27;.20g&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Actual Result&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Predicted Result&quot;</span>)</span><br><span class="line">a, b = plt.ylim() </span><br><span class="line">a += <span class="number">0.5</span> </span><br><span class="line">b -= <span class="number">0.5</span></span><br><span class="line">plt.ylim(a, b)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="lr_result.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred_train = logmodel.predict(X_train)</span><br><span class="line">print(classification_report(y_train, pred_train,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.6476    0.3025    0.4124       747</span></span><br><span class="line"><span class="comment">#            1     0.7245    0.9176    0.8097      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7125      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.6860    0.6101    0.6111      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.6988    0.7125    0.6772      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(classification_report(y_test, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.6214    0.2661    0.3726       327</span></span><br><span class="line"><span class="comment">#            1     0.7073    0.9163    0.7983       633</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.6948       960</span></span><br><span class="line"><span class="comment">#    macro avg     0.6644    0.5912    0.5855       960</span></span><br><span class="line"><span class="comment"># weighted avg     0.6781    0.6948    0.6533       960</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predslog_lr = logmodel.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">metrics.roc_auc_score(y_test,predslog_lr, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 0.6828171273147141</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc</span>(<span class="params">labels, predict_prob</span>):</span></span><br><span class="line">    false_positive_rate,true_positive_rate,thresholds=roc_curve(labels, predict_prob)</span><br><span class="line">    roc_auc=auc(false_positive_rate, true_positive_rate)</span><br><span class="line">    plt.title(<span class="string">&#x27;ROC&#x27;</span>)</span><br><span class="line">    plt.plot(false_positive_rate, true_positive_rate,<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;AUC = %0.4f&#x27;</span>% roc_auc)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">    plt.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],<span class="string">&#x27;r--&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;TPR&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;FPR&#x27;</span>)</span><br><span class="line"> plot_roc(y_test,predslog_lr)</span><br></pre></td></tr></table></figure>
<p><img src="lr_ROC.png" alt="images"></p>
<p>We have two important outcomes from this report. </p>
<p>One is the coefficients of each feature, telling us which characteristics make customers renew the fast-pass? Logistic regression tells us <strong>avgrides_perperson</strong> is more important to the lable variable. The variable,  <strong>avgrides_perperson</strong>, means <em>what was the average number of rides per day, per person, among all family members who used the pass?</em> Primary value is <em>10</em> and coefficient of the variable is 0.86123004. An increase of one average ride in <strong>avgrides_perperson</strong>, the odds of renew season fast-pass will increase by 0.86123004. </p>
<p>The another one is the how the model performs. The performance matric is an essential criterion for model selection.</p>
<h3 id="Part-II-Random-Forest-Model"><a href="#Part-II-Random-Forest-Model" class="headerlink" title="Part II: Random Forest Model"></a>Part II: Random Forest Model</h3><h4 id="Build-a-Initial-Model"><a href="#Build-a-Initial-Model" class="headerlink" title="Build a Initial Model"></a>Build a Initial Model</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_rf=RandomForestClassifier(random_state = <span class="number">654</span>)</span><br><span class="line">clf_rf.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<h4 id="Hyperparameter-Tuning-the-Random-Forest"><a href="#Hyperparameter-Tuning-the-Random-Forest" class="headerlink" title="Hyperparameter Tuning the Random Forest"></a>Hyperparameter Tuning the Random Forest</h4><p>Maybe some of you will ask what is the diffenrence between hyperparameter and parameter? When I started my journey of data analytics, I knew the word parameter to be defined as a value which model itself generates. It would be quite confusing to name what we input into models. This is why the term <em>hyper-parameter</em> came out. Hyper-parameters are input into model in the hope of making the model more accurate. The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance.</p>
<p>This is the concept of hyperparameter. Actually, it has been used in all machine learning models. But in this post, I only present it in Random Forest and XGBoost. Hyperparameter Tuning is more important to these alsogorithm than Logistic Regression. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">200</span>],   <span class="comment"># large n_estimators can predict more precise. Therefore, I only consider a large number, 200 trees.</span></span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], </span><br><span class="line">    <span class="string">&#x27;max_features&#x27;</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],  <span class="comment"># In general, max_features should be set as sqrt of n_feature. sqrt(10) = 3 or 4</span></span><br><span class="line">    <span class="string">&#x27;min_samples_leaf&#x27;</span>: [<span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>],  <span class="comment"># smaller number of leaf would tend to capture the noise of dataset. Therefore, I set []</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CV_rfc = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv= <span class="number">5</span>)</span><br><span class="line">CV_rfc.fit(X_train, y_train)</span><br><span class="line">print(CV_rfc.best_params_)</span><br><span class="line"><span class="comment"># &#123;&#x27;max_depth&#x27;: 8, &#x27;max_features&#x27;: 4, &#x27;min_samples_leaf&#x27;: 8, &#x27;n_estimators&#x27;: 200&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_rf=RandomForestClassifier(n_estimators=<span class="number">200</span>, max_depth=<span class="number">8</span>, max_features=<span class="number">4</span>, min_samples_leaf=<span class="number">8</span>, random_state=<span class="number">654</span>)</span><br><span class="line">clf_rf.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_imp_df = pd.DataFrame(list(zip(clf_rf.feature_importances_, X_train)))</span><br><span class="line">feature_imp_df.columns = [<span class="string">&#x27;feature importance&#x27;</span>, <span class="string">&#x27;feature&#x27;</span>]</span><br><span class="line">feature_imp_df = feature_imp_df.sort_values(by=<span class="string">&#x27;feature importance&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">feature_imp_df</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>feature  importance</th>
<th>featurere</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.27666</td>
<td>visits</td>
</tr>
<tr>
<td>1</td>
<td>0.115035</td>
<td>avgrides_perperson</td>
</tr>
<tr>
<td>3</td>
<td>0.114043</td>
<td>avggoldzone_perperson</td>
</tr>
<tr>
<td>8</td>
<td>0.111199</td>
<td>homestate_NJ</td>
</tr>
<tr>
<td>2</td>
<td>0.109533</td>
<td>avgmerch_perperson</td>
</tr>
<tr>
<td>6</td>
<td>0.106009</td>
<td>own_car</td>
</tr>
<tr>
<td>4</td>
<td>0.090761</td>
<td>avgfood_perperson</td>
</tr>
<tr>
<td>9</td>
<td>0.034174</td>
<td>homestate_NY</td>
</tr>
<tr>
<td>5</td>
<td>0.029154</td>
<td>goldzone_playersclub</td>
</tr>
<tr>
<td>7</td>
<td>0.013431</td>
<td>FB_Like</td>
</tr>
</tbody>
</table>
</div>
<p>According to feature importance value, we can figure out the top 5 important features, which are avggoldzone_perperson, avgmerch_perperson, visits, avgfood_perperson and avgrides_perperson. These features have important value larger than 0.16, which means they are strongly predictable.</p>
<p>In general, when we have large size of features, we keep those have an importance of more than 0.15. However, our dataset is only have 10 variables. It doesn’t matter to keep all the variables in random forest and the following XGBoost because they are not linear model. And more variables will keep more information. Therefore, we keep all the feature.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prediction</span></span><br><span class="line">predictions = clf_rf.predict(X_test)</span><br><span class="line"><span class="comment"># confusion matrix</span></span><br><span class="line">mat = confusion_matrix(predictions, y_test)</span><br><span class="line">sns.heatmap(mat, square=<span class="literal">True</span>, annot=<span class="literal">True</span>, cbar=<span class="literal">False</span>,fmt=<span class="string">&#x27;.20g&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Actual Result&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Predicted Result&quot;</span>)</span><br><span class="line">a, b = plt.ylim() </span><br><span class="line">a += <span class="number">0.5</span> </span><br><span class="line">b -= <span class="number">0.5</span></span><br><span class="line">plt.ylim(a, b)</span><br><span class="line">plt.show()rf_</span><br></pre></td></tr></table></figure>
<p><img src="rf_result.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = clf_rf.predict(X_train)</span><br><span class="line">print(classification_report(y_train, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment">#  </span></span><br><span class="line"><span class="comment">#            0     0.9233    0.4029    0.5610       747</span></span><br><span class="line"><span class="comment">#            1     0.7670    0.9833    0.8618      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7897      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.8451    0.6931    0.7114      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.8191    0.7897    0.7615      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = clf_rf.predict(X_test)</span><br><span class="line">print(classification_report(y_test, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.8000    0.3058    0.4425       327</span></span><br><span class="line"><span class="comment">#            1     0.7281    0.9605    0.8283       633</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7375       960</span></span><br><span class="line"><span class="comment">#    macro avg     0.7641    0.6332    0.6354       960</span></span><br><span class="line"><span class="comment"># weighted avg     0.7526    0.7375    0.6969       960</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predslog_rf = clf_rf.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">metrics.roc_auc_score(y_test,predslog_rf, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># rf0.6974892628181901</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_roc(y_test,predslog_rf)</span><br></pre></td></tr></table></figure>
<p><img src="rf_ROC.png" alt="images"></p>
<h3 id="Part-III-XGBoost-Model"><a href="#Part-III-XGBoost-Model" class="headerlink" title="Part III : XGBoost Model"></a>Part III : XGBoost Model</h3><h4 id="Build-initial-model"><a href="#Build-initial-model" class="headerlink" title="Build initial model"></a>Build initial model</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_xgb=xgb.XGBClassifier(random_state=<span class="number">654</span>)</span><br><span class="line">clf_xgb.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<h4 id="Hyoeroarameters-tuning"><a href="#Hyoeroarameters-tuning" class="headerlink" title="Hyoeroarameters tuning"></a>Hyoeroarameters tuning</h4><p>In XGBoost, I tuned hyperparameters in 6 steps.</p>
<h5 id="step-1-n-estimators"><a href="#step-1-n-estimators" class="headerlink" title="step 1: n_estimators"></a>step 1: n_estimators</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">500</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>,<span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary:logistic&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;n_estimators&#x27;: 200&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step-2-max-depth-amp-min-child-weight"><a href="#Step-2-max-depth-amp-min-child-weight" class="headerlink" title="Step 2: max_depth &amp; min_child_weight"></a>Step 2: max_depth &amp; min_child_weight</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;max_depth&#x27;</span>: list(range(<span class="number">1</span>,<span class="number">5</span>,<span class="number">1</span>)), <span class="string">&#x27;min_child_weight&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;max_depth&#x27;: 3, &#x27;min_child_weight&#x27;: 5&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step-3-gamma"><a href="#Step-3-gamma" class="headerlink" title="Step 3: gamma"></a>Step 3: gamma</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;gamma&#x27;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;gamma&#x27;: 0.6&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="step-4-subsample-colsample-bytree"><a href="#step-4-subsample-colsample-bytree" class="headerlink" title="step 4: subsample, colsample_bytree"></a>step 4: subsample, colsample_bytree</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;subsample&#x27;</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>], <span class="string">&#x27;colsample_bytree&#x27;</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.6</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;colsample_bytree&#x27;: 0.9, &#x27;subsample&#x27;: 0.9&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="step-5-regalpha-amp-reglambda"><a href="#step-5-regalpha-amp-reglambda" class="headerlink" title="step 5: regalpha &amp; reglambda"></a>step 5: regalpha &amp; reglambda</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;reg_alpha&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;reg_lambda&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.6</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;reg_alpha&#x27;: 3, &#x27;reg_lambda&#x27;: 0.05&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="step-6-learning-rate"><a href="#step-6-learning-rate" class="headerlink" title="step 6: learning rate"></a>step 6: learning rate</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.07</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.6</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">0.05</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;learning_rate&#x27;: 0.1&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="Build-XGBoost-Model"><a href="#Build-XGBoost-Model" class="headerlink" title="Build XGBoost Model"></a>Build XGBoost Model</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf_xgb = xgb.XGBClassifier(n_estimators=<span class="number">200</span>, max_depth=<span class="number">3</span>, </span><br><span class="line">                            learning_rate=<span class="number">0.1</span>, subsample=<span class="number">0.9</span>, colsample_bytree=<span class="number">0.9</span>,scale_pos_weight=<span class="number">3.0</span>, </span><br><span class="line">                             silent=<span class="literal">True</span>, nthread=<span class="number">-1</span>, seed=<span class="number">0</span>, missing=<span class="literal">None</span>,objective=<span class="string">&#x27;binary:logistic&#x27;</span>, </span><br><span class="line">                             reg_alpha=<span class="number">3</span>, reg_lambda=<span class="number">0.05</span>, </span><br><span class="line">                             gamma=<span class="number">0.6</span>, min_child_weight=<span class="number">5</span>, </span><br><span class="line">                             max_delta_step=<span class="number">0</span>,base_score=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">clf_xgb.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h4 id="Feature-importance"><a href="#Feature-importance" class="headerlink" title="Feature importance"></a>Feature importance</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># feature importance </span></span><br><span class="line">feature_imp_df = pd.DataFrame(list(zip(clf_xgb.feature_importances_, X_train)))</span><br><span class="line">feature_imp_df.columns = [<span class="string">&#x27;feature importance&#x27;</span>, <span class="string">&#x27;feature&#x27;</span>]</span><br><span class="line">feature_imp_df = feature_imp_df.sort_values(by=<span class="string">&#x27;feature importance&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">feature_imp_df</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>feature importance</th>
<th>feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>0.321522</td>
<td>homestate_NJ</td>
</tr>
<tr>
<td>6</td>
<td>0.151555</td>
<td>own_car</td>
</tr>
<tr>
<td>0</td>
<td>0.13588</td>
<td>visits</td>
</tr>
<tr>
<td>9</td>
<td>0.095966</td>
<td>homestate_NY</td>
</tr>
<tr>
<td>5</td>
<td>0.075694</td>
<td>goldzone_playersclub</td>
</tr>
<tr>
<td>1</td>
<td>0.061954</td>
<td>avgrides_perperson</td>
</tr>
<tr>
<td>3</td>
<td>0.043305</td>
<td>avggoldzone_perperson</td>
</tr>
<tr>
<td>4</td>
<td>0.041291</td>
<td>avgfood_perperson</td>
</tr>
<tr>
<td>2</td>
<td>0.038462</td>
<td>avgmerch_perperson</td>
</tr>
<tr>
<td>7</td>
<td>0.034371</td>
<td>FB_Like</td>
</tr>
</tbody>
</table>
</div>
<p>According to feature importance value, it is noticable that homestate_NJ is the most important feature. Next is own_car, visits. These variables are strongly predictable.</p>
<h4 id="Measure-performance"><a href="#Measure-performance" class="headerlink" title="Measure performance"></a>Measure performance</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prediction</span></span><br><span class="line">predictions = clf_xgb.predict(X_test)</span><br><span class="line"><span class="comment"># confusion matrix</span></span><br><span class="line">mat = confusion_matrix(predictions, y_test)</span><br><span class="line">sns.heatmap(mat, square=<span class="literal">True</span>, annot=<span class="literal">True</span>, cbar=<span class="literal">False</span>,fmt=<span class="string">&#x27;.20g&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Actual Result&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Predicted Result&quot;</span>)</span><br><span class="line">a, b = plt.ylim() </span><br><span class="line">a += <span class="number">0.5</span> </span><br><span class="line">b -= <span class="number">0.5</span></span><br><span class="line">plt.ylim(a, b)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="xgb_result.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = clf_xgb.predict(X_train)</span><br><span class="line">print(classification_report(y_train, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.9598    0.2878    0.4428       747</span></span><br><span class="line"><span class="comment">#            1     0.7361    0.9940    0.8458      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7585      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.8480    0.6409    0.6443      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.8107    0.7585    0.7114      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = clf_xgb.predict(X_train)</span><br><span class="line">print(classification_report(y_train, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.9598    0.2878    0.4428       747</span></span><br><span class="line"><span class="comment">#            1     0.7361    0.9940    0.8458      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7585      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.8480    0.6409    0.6443      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.8107    0.7585    0.7114      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predslog_rf = clf_xgb.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">metrics.roc_auc_score(y_test,predslog_rf, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 0.6981704518553946</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_roc(y_test,predslog_rf)xgb</span><br></pre></td></tr></table></figure>
<p><img src="xbg_ROC.png" alt="images"></p>
<h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><p>As shown in the performance matrix, XGBoost is the best model with greatest accuracy, recall, f1 score, precision, AUC. Because this data is imbalanced, so we focus on recall, f1 score, precision and AUC.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Accuracy_test</th>
<th>Recall_test</th>
<th>F1_Score_test</th>
<th>Precision_test</th>
<th>AUC</th>
<th>Accuracy_train</th>
<th>Recall_train</th>
<th>F1_Score_train</th>
<th>Precision_train</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistic Regression</td>
<td>0.6948</td>
<td>0.9163</td>
<td>0.7983</td>
<td>0.7073</td>
<td>0.682817</td>
<td>0.7125</td>
<td>0.9176</td>
<td>0.8097</td>
<td>0.7245</td>
</tr>
<tr>
<td>Random Forest</td>
<td>0.7375</td>
<td>0.9605</td>
<td>0.8283</td>
<td>0.7281</td>
<td>0.697489</td>
<td>0.7897</td>
<td>0.9833</td>
<td>0.8618</td>
<td>0.767</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.7585</td>
<td>0.994</td>
<td>0.8458</td>
<td>0.7361</td>
<td>0.69817</td>
<td>0.7585</td>
<td>0.994</td>
<td>0.8458</td>
<td>0.7361</td>
</tr>
</tbody>
</table>
</div>
<p>To uncover the potential factors toward renewing a fast pass, we built three classification models using all the variables. These three models are Logistic Regression, Random Forest and XGBoost. We selected XGBoost as our final model for better performance. Based on the criteria, the classification models can divide individuals as two groups.</p>
<p>When we know an unknown customer information such as owning a car or not, the number of times that the pass was used during the season, we can predict whether the member will renew their card for next season or not.</p>
<h2 id="Thoughts-Beyond-Modeling"><a href="#Thoughts-Beyond-Modeling" class="headerlink" title="Thoughts Beyond Modeling"></a>Thoughts Beyond Modeling</h2><p>However, the model seems to be useless because we already know whether these customers renew or not. Even though these classification models might be useful for prediction in next season, these are more seasonal factors influencing on renewing. Situations are quite distinct in different seasons, unless we training the model with data from last year. For example, the ratio for spring customers to renew summer passes must be higher than that for the winter to renew spring passes. Winter and summer are two holiday for most students.</p>
<p>Therefore, there are more crucial things than predictions. We should be mining what important factors leading to customers renew their passes. And use these factors to promote the potential customers.</p>
<h2 id="Apply-Feature-Importance-to-Target-Potential-Customers"><a href="#Apply-Feature-Importance-to-Target-Potential-Customers" class="headerlink" title="Apply Feature Importance to Target Potential Customers"></a><strong>Apply Feature Importance to Target Potential Customers</strong></h2><p>Each model reveals the importances of features. That is, these relative scores can highlight which features may be most relevant to the target, and the converse, which features are the least relevant.</p>
<p>In Logistic Regression, these coefficients can provide the basis for a crude feature importance score. The higher the coefficient, the higher the “importance” of a feature. As the result shown, <strong>goldzone_playersclub</strong>, <strong>homestate_NJ</strong>(most negative), <strong>own_car</strong> these three features have higher coefficients. In Random Forest and XGBoost, the results are more obvious. These importance values are calculated by how to split the tree. We can obtain them by models’ outputs. In random forest, most top 5 valuable features are <strong>avggoldzone_perperson</strong>, <strong>avgmerch_perperson</strong>, <strong>visits</strong>, <strong>avgfood_perperson</strong>, <strong>avgrides_perperson</strong>. In XGBoost, <strong>homestate_NJ</strong> , <strong>own_car</strong> and <strong>visits</strong> are the most three important features. To summarize, <strong>homestate_NJ</strong>, <strong>own_car</strong>, <strong>visits</strong>, <strong>avggoldzone_perperson</strong>, <strong>avgmerch_perperson</strong> might be most relevant for customers to renew their season passes.</p>
<p>To sum up, three features should be considered.</p>
<ol>
<li><strong>homestate_NJ</strong>: here is a note that both homestate_NJ and homestate_NY have negative coefficients in Logistic Regression. In addition, the rest variable homestate_CT has been droped to avoid the multi correlation, so the coefficient is larger than homestate_NJ and homestate_NY. According to these statistics, we can infer that customers in New York and New Jersey would be busy with their works. Busy lives made them have fewer visits during last season. The second conjecture is that fewer family members in New Jersey and New York families. There are many different reasons to make them tend to believe it doesn’t worth spending money on season pass over a long period. Even though they have time on vacation, amusement parks are not usually their first leisure places. Therefore, the coefficients are negative. In order to attract more target people, we should focus on the customers living in Connecticut. These customers are more likely to renew their season passes than other two states.</li>
<li><strong>own_car</strong>: this variable is the essential to classify the customers. In addition, the coefficient is positive in Logistic Regression. Obviously, families owning cars are more willing to spend time in amusement parks. One reason is convenience. The second reason is that big families are willing to use this type of seasonal pass. They seem to be more beneficial if more family members use them. Owning a car is a common property of such a large family. According to this finding, we can make more promotions to attract families with cars. If they become our seasonal passes members for the first time, they would be more likely to renew the cards for the next seasons.</li>
<li><strong>visits</strong>: coefficient of visit is positive in Logistic Regression, and we can indicate that families with more visits are more likely to renew seasonal passes. It is reasonable that amusement park lovers are more likely to renew season passes. Lobster Land should promote the seasonal passes to those customers who have already spent a lot of time but are still not the seasonal pass members.</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The ability to identify our target customer group and get it right, is gure gold for Lobster Land. With the help of a well-trained classification model, marketers can rely on assumptions and guesswork and more on data-driven insights to find target customers precisely. According to the feature importance values, we can do more promotions for the players living in Connecticut, owning cars, or being willing to spend time in Lobsterland.</p>
<p><strong>Thanks for reading!!! Please let me know if there is something I should add. And if you enjoy it, share it with your friends and colleagues : )</strong></p>
<p><strong>另外博客的中文版本也会及时更新！谢谢！</strong></p>
]]></content>
      <tags>
        <tag>XGBoost</tag>
        <tag>Classification</tag>
        <tag>Logistic Regression</tag>
        <tag>Random Forest</tag>
        <tag>Marketing Strategy</tag>
      </tags>
  </entry>
</search>
