<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A Markov Chain Model in Manpower Systems</title>
    <url>/yantongtt.github.io/2020/08/04/A-Markov-Chain-Model-in-Manpower-Systems/</url>
    <content><![CDATA[<h1 id="如何根据互联网员工年龄"><a href="#如何根据互联网员工年龄" class="headerlink" title="如何根据互联网员工年龄"></a>如何根据互联网员工年龄</h1><h2 id="基于-Markov-过程预测未来公司人员规模"><a href="#基于-Markov-过程预测未来公司人员规模" class="headerlink" title="基于 Markov 过程预测未来公司人员规模"></a>基于 Markov 过程预测未来公司人员规模</h2><p>其实不管是互联网、金融还是其他行业，拟定一个合理员工年龄结构战略对任何一家企业的长期发展都有着极其重要的影响，同时也是人力规划的核心目标。在制定未来一段时期人员补充方案时，企业有必要对不同方案下的员工年龄结构变化趋势进行预测，从而判断不同方案下员工队伍的变化是否能支撑未来发展需要。<br><a id="more"></a></p>
<h3 id="从Markov-Chain到模型构建"><a href="#从Markov-Chain到模型构建" class="headerlink" title="从Markov Chain到模型构建"></a>从Markov Chain到模型构建</h3><p>这个模型我是基于Markov Chain 的思想来构建的。何为Markov Chain？简单来说就是假设某一时刻状态转移的概率只依赖于它的前一个状态。</p>
<p>在年龄预测模型中，基本的几个状态为不同的年龄段。Markov Chain 模型主要是分析一个人在某一阶段内由一个年龄段调到另一个年龄段的可能性。从一个年龄段转移到另一个年龄段的状态属于内部流动，意味着该员工仍在该企业就职。但还存在外部流动的状态，离职与退休。属于员工流失。通过运用Markov 过程原理，分析每个年龄段的员工流动到不同状态的流动趋势和概率，构建完整的预测周期运算过程，以便为人力资源在新增人员对企业总量规模、人员年龄结构的规划中提供依据。</p>
<p>这么说不太直观，举个例子。在项目中，我将内部流动分为7个阶段，分别为20-24，25-28，29-31，32-35，36-40，41-50，51-60。外部流动为离职率和退休率。</p>
<p>根据Markov Chain原理，转移矩阵中的概率就是某一状态到另一状态的可能性。下表就是项目中转移矩阵的结构。我们就用这个转移矩阵就表示这一年内发生的事情。其中，p_23表示今年处于[25,28]这一年龄阶段的员工在下一年流向[29,31]的概率。为了方便计算，我用频率表示，即今年处于[25,28]这一年龄阶段而在下一年流向[29,31]的员工总数与今年处于[25,28]这一年龄阶段的员工总数比值。为什么大部分概率为0其实很好理解，这些概率为0的状态转移情况是不可能发生的。今年[25,28]年龄段的员工明年可能是26,27,28,29，因此在内部流动中只能有两种状态。而先前说的p_23为今年28岁的员工明年没有离职流向[29,31]年龄的概率。</p>
<p><strong>Transition Matrix from Preview Year to Next Year</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>20-24</th>
<th>25-28</th>
<th>29-31</th>
<th>32-35</th>
<th>36-40</th>
<th>41-50</th>
<th>51-60</th>
<th>Dismission</th>
<th>Retirement</th>
<th>Recruitment</th>
</tr>
</thead>
<tbody>
<tr>
<td>20-24</td>
<td>p_11</td>
<td>p_12</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_18</td>
<td>0</td>
<td>p_1_10</td>
</tr>
<tr>
<td>25-28</td>
<td>0</td>
<td>p_22</td>
<td>p_23</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_28</td>
<td>0</td>
<td>p_2_10</td>
</tr>
<tr>
<td>29-31</td>
<td>0</td>
<td>0</td>
<td>p_33</td>
<td>p_34</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_38</td>
<td>0</td>
<td>p_3_10</td>
</tr>
<tr>
<td>32-35</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_44</td>
<td>p_45</td>
<td>0</td>
<td>0</td>
<td>p_48</td>
<td>0</td>
<td>p_4_10</td>
</tr>
<tr>
<td>36-40</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_55</td>
<td>p_56</td>
<td>0</td>
<td>p_58</td>
<td>0</td>
<td>p_5_10</td>
</tr>
<tr>
<td>41-50</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_66</td>
<td>p_67</td>
<td>p_68</td>
<td>0</td>
<td>p_6_10</td>
</tr>
<tr>
<td>51-60</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>p_77</td>
<td>p_78</td>
<td>p_79</td>
<td>p_7_10</td>
</tr>
</tbody>
</table>
</div>
<h3 id="模型假设与定义"><a href="#模型假设与定义" class="headerlink" title="模型假设与定义"></a>模型假设与定义</h3><p><img src="hypotheses.png" alt="images"></p>
<p>假设，</p>
<ul>
<li>$N(t)$ 为第t年年初各年龄段员工总数，$N(t) = [n_1(t), n_2(t), … , n_7(t)]$，其中$n_1(t),n_2(t),…,n_7(t)$ 分别为第t年20-24岁，25-28岁，29-31岁，32-35岁，36-40岁，41-50岁，51-60岁的员工人数。</li>
<li>$L(t)$ 为在第t整年各年龄段离职员工人数，$L(t) = [l_1(t), l_2(t), … , l_7(t)]$，而$P(t) = [p_1(t), p_2(t), … , p_7(t)]$分别为发生概率，即$L(t)=N(t)\times P(t)$</li>
<li>$R_t(t)$ 为在第t整年各年龄段退休员工人数，$R_t(t)=[0,0,0,0,0,0,r_7(t)]$ </li>
<li>$R_c(t)$ 为在第t整年各年龄段新增员工人数</li>
<li>$P_{ij}(t+1)$ 第t年从状态$i$到下一年转移到状态$j$的概率</li>
</ul>
<p><strong>模型假设</strong> : 下一年年初的员工总数=这一年年末剩余+下一年年初新增。而这一年年末剩余就等于这一年年初的总数乘上转移矩阵。这就是markov process在这个算法中的运用。<script type="math/tex">N(t+1)=N(t)*P_{ij}(t+1)+R_c(t+1)</script></p>
]]></content>
      <categories>
        <category>Stochastic Model</category>
      </categories>
      <tags>
        <tag>人力资源</tag>
        <tag>Markov Chain</tag>
      </tags>
  </entry>
  <entry>
    <title>An overview of time series forecasting models</title>
    <url>/yantongtt.github.io/2020/08/06/An-overview-of-time-series-forecasting-models/</url>
    <content><![CDATA[<p><strong>This post describes 5 forecasting models and I apply them to predict the total summer spend for leisure in NYC </strong> </p>
<p>Last semester, in my Marketing Analytics class, I have been working with Time Series Data. This data records total summer spending in NYC from 1959 to 2019. There is no trend, and no seasonality in this data. Very simple but I still wanted to review what a Time series is as well as make my understanding more concert on that. This article only provides some simple time series models. What I am trying to say is that time series predictions are difficult in reality and always require a very specialized data scientist to implement it. The models in this post are so limited that we should learn more beyond these.</p>
<a id="more"></a>
<h2 id="How-the-time-series-looks-like"><a href="#How-the-time-series-looks-like" class="headerlink" title="How the time series looks like?"></a>How the time series looks like?</h2><p>The data contains only 2 columns, one column is Date and the other column relates to total summer spending. </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>year</th>
<th>summerspend</th>
</tr>
</thead>
<tbody>
<tr>
<td>1959</td>
<td>22627333.2</td>
</tr>
<tr>
<td>1960</td>
<td>30509799.9</td>
</tr>
<tr>
<td>1961</td>
<td>24408277.9</td>
</tr>
<tr>
<td>1962</td>
<td>23772213.4</td>
</tr>
<tr>
<td>1963</td>
<td>23572771.3</td>
</tr>
</tbody>
</table>
</div>
<p>It shows the summer spending on New England parks by greater NYC visitors to New England parks from 1959 till 2019. The goal is to predict the total spending for each of the next five years.</p>
<h2 id="Time-Series-Analysis"><a href="#Time-Series-Analysis" class="headerlink" title="Time Series Analysis"></a>Time Series Analysis</h2><h3 id="Step-1-Visualizing-the-time-series"><a href="#Step-1-Visualizing-the-time-series" class="headerlink" title="Step 1: Visualizing the time series"></a>Step 1: Visualizing the time series</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data[<span class="string">&#x27;summerspend&#x27;</span>].plot();</span><br></pre></td></tr></table></figure>
<p><img src="summerspend_plot.png" alt="images"></p>
<p>When we work on time series forecasting, a series need to be stationary. Stationary process has the property that the mean, variance and autocorrelation structure do not change over time. The mean is constant in this case. However, it is hard to say stable variance and autocorrelation. Before ARMA model, I will try to detect this. </p>
<p>We also visualize the data in our series through a distribution too.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.distplot(data[<span class="string">&#x27;summerspend&#x27;</span>], hist=<span class="literal">True</span>, kde=<span class="literal">True</span>, </span><br><span class="line">             bins=<span class="number">50</span>, color = <span class="string">&#x27;darkblue&#x27;</span>, </span><br><span class="line">             hist_kws=&#123;<span class="string">&#x27;edgecolor&#x27;</span>:<span class="string">&#x27;black&#x27;</span>&#125;,</span><br><span class="line">             kde_kws=&#123;<span class="string">&#x27;linewidth&#x27;</span>: <span class="number">4</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="summerspend_d.png" alt="images"></p>
<p>We can observe a near-normal distribution over consumption values.</p>
<h3 id="Step-2-Creating-Training-and-Test-Datasets-for-Modeling"><a href="#Step-2-Creating-Training-and-Test-Datasets-for-Modeling" class="headerlink" title="Step 2: Creating Training and Test Datasets for Modeling"></a>Step 2: Creating Training and Test Datasets for Modeling</h3><p>Because we need to capture the time factor in time series data, I devided total data as training data and test data by time. The first 70% older data is training data, and 30% newer data is test data.</p>
<p><img src="train_test.png" alt="images"></p>
<p>We divided data into training data and test data by time. As we can see in the this graph, blue old data is used to build models and orange new data is used for prediction.</p>
<h3 id="Step-3-Exploring-Time-Series-to-Select-Proper-Models"><a href="#Step-3-Exploring-Time-Series-to-Select-Proper-Models" class="headerlink" title="Step 3: Exploring Time Series to Select Proper Models"></a>Step 3: Exploring Time Series to Select Proper Models</h3><p>A given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.</p>
<p>In order to select a proper time series model, we should do ETS decomposition. That is extract trend and seasonality from our data. The resultant series will become stationary through this process.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.tsa.seasonal <span class="keyword">import</span> seasonal_decompose</span><br><span class="line"></span><br><span class="line">result = seasonal_decompose(train[<span class="string">&#x27;summerspend&#x27;</span>], model=<span class="string">&#x27;multiplicative&#x27;</span>)  <span class="comment"># model=&#x27;mul&#x27; also works</span></span><br><span class="line">result.plot();</span><br></pre></td></tr></table></figure>
<p><img src="ETS.png" alt="images"></p>
<p>According to the ETS decomposition, there is no significant seasonal and trend. Therefore, we will not consider the seasonal model, such as SARIMAX, Holt-Winters, Holt’s Linear Trend.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.graphics.tsaplots <span class="keyword">import</span> plot_acf</span><br><span class="line"><span class="keyword">from</span> statsmodels.graphics.tsaplots <span class="keyword">import</span> plot_pacf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">211</span>)</span><br><span class="line">fig = sm.graphics.tsa.plot_acf(train,lags=<span class="number">20</span>,ax=ax1)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">212</span>)</span><br><span class="line">fig = sm.graphics.tsa.plot_pacf(train,lags=<span class="number">20</span>,ax=ax2)</span><br></pre></td></tr></table></figure>
<p><img src="ACF.png" alt="images"></p>
<p>What’s more, this time series seems to be stationary. Both ACF and PACF fall into confidence interval abruptly, cutting off at q = 0 and p = 0,respectively. But for more precisely prediction, we will try AR(1), MA(1), ARMA(1,1) in the following modeling process</p>
<h3 id="Step-4-Modeling"><a href="#Step-4-Modeling" class="headerlink" title="Step 4: Modeling"></a>Step 4: Modeling</h3><p>We use training dataset for modeling, and test dataset to measure the performance of models. The performance indicator mainly is RMSE. But we also use AIC and BIC to measure the performances of AR(1), MA(1), ARMA(1,1) , to select the best model in ARMA.</p>
<h4 id="Method-1-Naive-Approach"><a href="#Method-1-Naive-Approach" class="headerlink" title="Method 1: Naive Approach"></a>Method 1: Naive Approach</h4><p>Consider the process is no trend and no seasonal factor, we apply the Naive Approach at first.</p>
<p>We can infer from the graph that the price of the coin is stable from the start. Many a times we are provided with a dataset, which is stable throughout it’s time period. If we want to forecast the price for the next day, we can simply take the last day value and estimate the same value for the next day. Such forecasting technique which assumes that the next expected point is equal to the last observed point is called Naive Method.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Now we will implement the Naive method to forecast the prices for test data.</span></span><br><span class="line"></span><br><span class="line">train_summerspend = np.asarray(train.summerspend)</span><br><span class="line">y_hat = test.copy()</span><br><span class="line">y_hat[<span class="string">&#x27;naive&#x27;</span>] = train_summerspend[len(train_summerspend)<span class="number">-1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train.index, train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test.index,test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat.index,y_hat[<span class="string">&#x27;naive&#x27;</span>], label=<span class="string">&#x27;Naive Forecast&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Naive Forecast&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Naive Method.png" alt="images"></p>
<p>We will now calculate RMSE to check to accuracy of our model on test data set.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RMSE = [] <span class="comment"># collect rmse of all the models</span></span><br><span class="line"></span><br><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat.naive))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment"># RMSE = 4292286.145806624</span></span><br></pre></td></tr></table></figure>
<p>We can infer from the RMSE value and the graph above, that Naive method isn’t suited for datasets with high variability. It is best suited for stable datasets. We can still improve our score by adopting different techniques.</p>
<h4 id="Method-2-Simple-Average"><a href="#Method-2-Simple-Average" class="headerlink" title="Method 2: Simple Average"></a>Method 2: Simple Average</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_hat_avg = test.copy()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>] = train[<span class="string">&#x27;summerspend&#x27;</span>].mean()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>], label=<span class="string">&#x27;Average Forecast&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="simple_average.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.avg_forecast))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment"># RMSE = 2404469.0779477805</span></span><br></pre></td></tr></table></figure>
<p>We can see the simple average can improve the score. The reason might be the time series is no trend.</p>
<h4 id="Method-3-Moving-Average"><a href="#Method-3-Moving-Average" class="headerlink" title="Method 3 Moving Average"></a>Method 3 Moving Average</h4><p>The algorithm I propose here is an attempt to find the best moving average according to the time window period we choose. We’ll try different moving averages length and find the one that minimizes RMSE. I will perform a for loop that spans among 2-period moving average to 10-period moving average. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p_number = []</span><br><span class="line">rmse_value = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">11</span>):</span><br><span class="line">        y_hat_avg = test.copy()</span><br><span class="line">        y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>] = train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(i).mean().iloc[<span class="number">-1</span>]</span><br><span class="line">        plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">        plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">        plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">        plt.plot(y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>], label=<span class="string">&#x27;Moving Average Forecast&#x27;</span>)</span><br><span class="line">        plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(i).mean(), label=<span class="string">&#x27;Train Moving Average&#x27;</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.moving_avg_forecast))</span><br><span class="line">        p_number.append(i)</span><br><span class="line">        rmse_value.append(rms)</span><br><span class="line">        print(<span class="string">&quot;RMSe of Moving Average Model when p =&quot;</span>,i,<span class="string">&quot;is&quot;</span>,rms)</span><br><span class="line">RMSE_MA = pd.DataFrame(&#123;<span class="string">&quot;p_number&quot;</span> : p_number,<span class="string">&quot;rmse_value&quot;</span> : rmse_value&#125;)</span><br><span class="line">RMSE_MA</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>p_number</th>
<th>rmse_value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>3931156</td>
</tr>
<tr>
<td>3</td>
<td>2608152</td>
</tr>
<tr>
<td>4</td>
<td>3016208</td>
</tr>
<tr>
<td>5</td>
<td>2808586</td>
</tr>
<tr>
<td>6</td>
<td>2842545</td>
</tr>
<tr>
<td>7</td>
<td>2541544</td>
</tr>
<tr>
<td>8</td>
<td>2466775</td>
</tr>
<tr>
<td>9</td>
<td>2408699</td>
</tr>
<tr>
<td>10</td>
<td>2451784</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RMSE_MA[RMSE_MA.rmse_value == RMSE_MA.rmse_value.min()]</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>p_number</th>
<th>rmse_value</th>
</tr>
</thead>
<tbody>
<tr>
<td>9</td>
<td>2408699</td>
</tr>
</tbody>
</table>
</div>
<p>In Moving Average model, we use p = 9.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_hat_avg = test.copy()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>] = train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(<span class="number">9</span>).mean().iloc[<span class="number">-1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;moving_avg_forecast&#x27;</span>], label=<span class="string">&#x27;Moving Average Forecast&#x27;</span>)</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>].rolling(i).mean(), label=<span class="string">&#x27;Train Moving Average&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="MA_model.png" alt="images"></p>
<h4 id="Method-4-Simple-Exponential-Smoothing"><a href="#Method-4-Simple-Exponential-Smoothing" class="headerlink" title="Method 4  Simple Exponential Smoothing"></a>Method 4  Simple Exponential Smoothing</h4><p>Exponential smoothing forecasting methods are similar in that a prediction is a weighted sum of past observations, but the model explicitly uses an exponentially decreasing weight for past observations. Specifically, past observations are weighted with a geometrically decreasing ratio. The formula below tells about its principle</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.tsa.api <span class="keyword">import</span> ExponentialSmoothing, SimpleExpSmoothing</span><br><span class="line">y_hat_avg = test.copy()</span><br><span class="line">fit2 = SimpleExpSmoothing(np.asarray(train[<span class="string">&#x27;summerspend&#x27;</span>])).fit(smoothing_level=<span class="number">0.6</span>,optimized=<span class="literal">False</span>)</span><br><span class="line">y_hat_avg[<span class="string">&#x27;SES&#x27;</span>] = fit2.forecast(len(test))</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;SES&#x27;</span>], label=<span class="string">&#x27;SES&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Simple_EXP.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.SES))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment">#RMSE = 3577811.749127613</span></span><br></pre></td></tr></table></figure>
<h4 id="Method-5-ARMA"><a href="#Method-5-ARMA" class="headerlink" title="Method 5  ARMA"></a>Method 5  ARMA</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ARMA_name = [<span class="string">&quot;AR(1)&quot;</span>,<span class="string">&quot;MA(1)&quot;</span>,<span class="string">&quot;ARMA(1,1)&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#AR(1)</span></span><br><span class="line">y_hat_avg = test.copy()</span><br><span class="line">fit1 = sm.tsa.ARMA(train.summerspend, order=(<span class="number">1</span>,<span class="number">0</span>)).fit()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;ARMA&#x27;</span>] = fit1.predict(start=<span class="string">&quot;2001-1-1&quot;</span>, end=<span class="string">&quot;2019-1-1&quot;</span>, dynamic=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="AR_1.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ARMA(1,1)</span></span><br><span class="line">y_hat_avg = test.copy()</span><br><span class="line">fit3 = sm.tsa.ARMA(train.summerspend, order=(<span class="number">1</span>,<span class="number">1</span>)).fit()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;ARMA&#x27;</span>] = fit3.predict(start=<span class="string">&quot;2001-1-1&quot;</span>, end=<span class="string">&quot;2019-1-1&quot;</span>, dynamic=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#MA(1)</span></span><br><span class="line"><span class="comment"># That is method 3 moving average method</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>ARMA_name</th>
<th>AIC</th>
<th>BIC</th>
</tr>
</thead>
<tbody>
<tr>
<td>AR(1)</td>
<td>1359.187</td>
<td>1364.4</td>
</tr>
<tr>
<td>MA(1)</td>
<td>1358.679</td>
<td>1363.892</td>
</tr>
<tr>
<td>ARMA(1,1)</td>
<td>1361.447</td>
<td>1368.398</td>
</tr>
</tbody>
</table>
</div>
<p>We select ARMA(1,1) with the largest AIC value and BIC value</p>
<p><img src="ARMA11.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rms = sqrt(mean_squared_error(test.summerspend, y_hat_avg.ARMA))</span><br><span class="line">RMSE.append(rms)</span><br><span class="line">print(rms) <span class="comment">#RMSE = 2415096.1564545105</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-4-Select-the-Best-Model"><a href="#Step-4-Select-the-Best-Model" class="headerlink" title="Step 4: Select the Best Model"></a>Step 4: Select the Best Model</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Naive Model</td>
<td>4292286</td>
</tr>
<tr>
<td style="text-align:left">Simple Average</td>
<td>2404469</td>
</tr>
<tr>
<td style="text-align:left">Moving Average</td>
<td>2408699</td>
</tr>
<tr>
<td style="text-align:left">Simple Exponential Smoothing</td>
<td>3577812</td>
</tr>
<tr>
<td style="text-align:left">ARMA(1,1)</td>
<td>2415096</td>
</tr>
</tbody>
</table>
</div>
<p>To minimize RMSE, I select Simple Average model. For more precisely prediction, I use all the dataset to build the model.</p>
<h3 id="Step-5-Model-for-Future-Prediction"><a href="#Step-5-Model-for-Future-Prediction" class="headerlink" title="Step 5: Model for Future Prediction"></a>Step 5: Model for Future Prediction</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Build Simple Average Model with all data</span></span><br><span class="line">pred_dates = pd.date_range(<span class="string">&#x27;2020-01-01&#x27;</span>, periods=<span class="number">5</span>, freq=<span class="string">&#x27;AS&#x27;</span>)</span><br><span class="line">pred = pd.Series(data[<span class="string">&#x27;summerspend&#x27;</span>].mean(),index=pred_dates)</span><br><span class="line">pred = pd.DataFrame(pred)</span><br><span class="line">pred.columns = [<span class="string">&quot;Forecast&quot;</span>]</span><br><span class="line">pred</span><br><span class="line"></span><br><span class="line">y_hat_avg = data.copy()</span><br><span class="line">y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>] = data[<span class="string">&#x27;summerspend&#x27;</span>].mean()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.plot(train[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Train&#x27;</span>)</span><br><span class="line">plt.plot(test[<span class="string">&#x27;summerspend&#x27;</span>], label=<span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">plt.plot(y_hat_avg[<span class="string">&#x27;avg_forecast&#x27;</span>], label=<span class="string">&#x27;Average Forecast&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="step_5.png" alt="images"></p>
<p>According to Moving Average model, we predict the total spending in summer by greater NYC visitors to New England parks for each of the next five years are 24185805.88 dollars.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The goal of this project wasn’t to fit the best possible forecasting model for this NYC summer spending case, but to give an overview of forecasting models. In a real world, data will not as simple as this one, and may shows complex seasonality or trend. We need make more effort on preprocessing, feature engineering and feature selection.</p>
<p> In a real world application a lot of time should be spent on preprocessing<strong>, </strong>feature engineering and feature selection.</p>
<p><strong>Thanks for reading!!! Please let me know if there is something I should add. And if you enjoy it, share it with your friends and colleagues : )</strong></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Time Series</tag>
        <tag>Forecasting Models</tag>
      </tags>
  </entry>
  <entry>
    <title>Lending Club</title>
    <url>/yantongtt.github.io/2020/08/04/Lending-Club/</url>
    <content><![CDATA[<h1 id=""><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>风控</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Exploring for Airbnb Listings in Germany - Price Prediction</title>
    <url>/yantongtt.github.io/2020/08/06/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany-Price-Prediction/</url>
    <content><![CDATA[<p><strong>A look into the AirBnB public dataset for price prediction and understanding of the predictive variables</strong></p>
<p>In <em>Data Cleaning Challenge</em> part, I shared my understanding of data cleaning. Now, let’s dive deeper. </p>
<p>Think about an interesting question, “if you are a host, how to maximize your profits?” This time I used a linear regression model to explore deeper into the possible factors that contribute to Airbnb rental prices. It’s okay for you to try other models like GBDT or XGBoost. In this post, I will highlight the approach I used to answer this question as well as how I explain the results of regression analysis. </p>
<a id="more"></a>
<p>Same as last post, data is sourced from the <a href="http://insideairbnb.com/get-the-data.html"><strong>Inside Airbnb</strong></a> website. And data have been prepared in <em>Data Cleaning Challenge</em> part. On that part, I handled with some missing values and also create some features. </p>
<h3 id="Step-1-Think-about-the-problem-and-dataset"><a href="#Step-1-Think-about-the-problem-and-dataset" class="headerlink" title="Step 1: Think about the problem and dataset"></a>Step 1: Think about the problem and dataset</h3><p>Before diving head first into the data and producing large correlation matrices, I always try to think of the question and get a sense of the features. Why am I doing this analysis? What’s the goal? What relationships between features and the target variable make sense? Hope this tip can help you.</p>
<p>In my preview post, missing values have been handled, and also I removed some featrues like url or host_name. As shown in the below table, almost of data is in a very neat and ordered format. Date-typed date has been transformed as number of days or only extract the year. Some of null value has become new level of its variable, and some has been binned into cagegories. I will not go through the cleaning work in detail. Hope the preview blog would be helpful for you.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>host_since</th>
<th>host_location</th>
<th>host_response_time</th>
<th>host_response_rate</th>
<th>host_is_superhost</th>
<th>host_neighbourhood</th>
<th>host_listings_count</th>
<th>host_total_listings_count</th>
<th>host_has_profile_pic</th>
<th>host_identity_verified</th>
<th>street</th>
<th>neighbourhood</th>
<th>neighbourhood_cleansed</th>
<th>neighbourhood_group_cleansed</th>
<th>city</th>
<th>state</th>
<th>zipcode</th>
<th>market</th>
<th>smart_location</th>
<th>country_code</th>
<th>country</th>
<th>is_location_exact</th>
<th>property_type</th>
<th>room_type</th>
<th>accommodates</th>
<th>bathrooms</th>
<th>bedrooms</th>
<th>beds</th>
<th>bed_type</th>
<th>square_feet</th>
<th>price</th>
<th>weekly_price</th>
<th>monthly_price</th>
<th>security_deposit</th>
<th>cleaning_fee</th>
<th>guests_included</th>
<th>extra_people</th>
<th>minimum_nights</th>
<th>maximum_nights</th>
<th>calendar_updated</th>
<th>has_availability</th>
<th>availability_30</th>
<th>availability_60</th>
<th>availability_90</th>
<th>availability_365</th>
<th>calendar_last_scraped</th>
<th>number_of_reviews</th>
<th>first_review</th>
<th>last_review</th>
<th>review_scores_rating</th>
<th>review_scores_accuracy</th>
<th>review_scores_cleanliness</th>
<th>review_scores_checkin</th>
<th>review_scores_communication</th>
<th>review_scores_location</th>
<th>review_scores_value</th>
<th>requires_license</th>
<th>license</th>
<th>instant_bookable</th>
<th>is_business_travel_ready</th>
<th>cancellation_policy</th>
<th>require_guest_profile_picture</th>
<th>require_guest_phone_verification</th>
<th>calculated_host_listings_count</th>
<th>reviews_per_month</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>2008</td>
<td>Coledale, New South Wales, Australia</td>
<td>within a day</td>
<td>high</td>
<td>f</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg S√ºdwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10405</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Apartment</td>
<td>Entire home/apt</td>
<td>4</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>Real Bed</td>
<td>720</td>
<td>90</td>
<td>offer discount</td>
<td>offer discount</td>
<td>200</td>
<td>50</td>
<td>2</td>
<td>20</td>
<td>62</td>
<td>1125</td>
<td>7</td>
<td>t</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>220</td>
<td>2018/11/7</td>
<td>143</td>
<td>2009</td>
<td>2017</td>
<td>92</td>
<td>9</td>
<td>9</td>
<td>9</td>
<td>9</td>
<td>10</td>
<td>9</td>
<td>t</td>
<td>No</td>
<td>t</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>1.25</td>
</tr>
<tr>
<td>5</td>
<td>2009</td>
<td>Berlin, Berlin, Germany</td>
<td>within an hour</td>
<td>high</td>
<td>t</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Helmholtzplatz</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10437</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Apartment</td>
<td>Private room</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>Real Bed</td>
<td>N/A</td>
<td>42</td>
<td>no discount</td>
<td>no discount</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>24</td>
<td>2</td>
<td>10</td>
<td>3</td>
<td>t</td>
<td>15</td>
<td>26</td>
<td>26</td>
<td>26</td>
<td>2018/11/7</td>
<td>197</td>
<td>2009</td>
<td>2018</td>
<td>96</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>9</td>
<td>t</td>
<td>No</td>
<td>f</td>
<td>f</td>
<td>moderate</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>1.75</td>
</tr>
<tr>
<td>6</td>
<td>2009</td>
<td>Berlin, Berlin, Germany</td>
<td>within a few hours</td>
<td>high</td>
<td>f</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg S√ºdwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10405</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>f</td>
<td>Apartment</td>
<td>Entire home/apt</td>
<td>7</td>
<td>2.5</td>
<td>4</td>
<td>7</td>
<td>Real Bed</td>
<td>N/A</td>
<td>180</td>
<td>offer discount</td>
<td>no discount</td>
<td>400</td>
<td>80</td>
<td>5</td>
<td>10</td>
<td>6</td>
<td>14</td>
<td>14</td>
<td>t</td>
<td>0</td>
<td>7</td>
<td>7</td>
<td>137</td>
<td>2018/11/7</td>
<td>6</td>
<td>2015</td>
<td>2018</td>
<td>100</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>t</td>
<td>Yes</td>
<td>f</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>0.15</td>
</tr>
<tr>
<td>7</td>
<td>2009</td>
<td>Berlin, Berlin, Germany</td>
<td>within a day</td>
<td>high</td>
<td>f</td>
<td>Prenzlauer Berg</td>
<td>3</td>
<td>3</td>
<td>t</td>
<td>f</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg Nordwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10437</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Apartment</td>
<td>Entire home/apt</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>Real Bed</td>
<td>N/A</td>
<td>70</td>
<td>offer discount</td>
<td>offer discount</td>
<td>500</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>90</td>
<td>1125</td>
<td>1</td>
<td>t</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>129</td>
<td>2018/11/7</td>
<td>23</td>
<td>2010</td>
<td>2018</td>
<td>93</td>
<td>10</td>
<td>10</td>
<td>9</td>
<td>10</td>
<td>9</td>
<td>9</td>
<td>t</td>
<td>No</td>
<td>f</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>3</td>
<td>0.23</td>
</tr>
<tr>
<td>10</td>
<td>2010</td>
<td>Berlin, Berlin, Germany</td>
<td>within an hour</td>
<td>high</td>
<td>t</td>
<td>Prenzlauer Berg</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td>t</td>
<td>Berlin, Germany</td>
<td>Prenzlauer Berg</td>
<td>Prenzlauer Berg S√ºdwest</td>
<td>Pankow</td>
<td>Berlin</td>
<td>Berlin</td>
<td>10405</td>
<td>Berlin</td>
<td>Berlin, Germany</td>
<td>DE</td>
<td>Germany</td>
<td>t</td>
<td>Other</td>
<td>Private room</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>Real Bed</td>
<td>N/A</td>
<td>45</td>
<td>offer discount</td>
<td>offer discount</td>
<td>0</td>
<td>18</td>
<td>1</td>
<td>26</td>
<td>3</td>
<td>30</td>
<td>7</td>
<td>t</td>
<td>8</td>
<td>18</td>
<td>42</td>
<td>42</td>
<td>2018/11/7</td>
<td>279</td>
<td>2010</td>
<td>2018</td>
<td>96</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>t</td>
<td>No</td>
<td>f</td>
<td>f</td>
<td>strict_14_with_grace_period</td>
<td>f</td>
<td>f</td>
<td>1</td>
<td>2.83</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Step-2-Feature-Selection"><a href="#Step-2-Feature-Selection" class="headerlink" title="Step 2: Feature Selection"></a>Step 2: Feature Selection</h3><p>As mentioned in last blog, features has been engineered, including transform the format and create more effecient and useful features. Now is the step of feature selection. Not all the variables will be used in the linear regression model. There are three reasons. One is that some data is unique for each datapoint like id, host_id. These features are very easy  to identify, and I have been removed in data cleaning. The second thing is another extreme situation, where all the value for every datapoint is same or constant. These features are named as “Zero Variance Feature”, which are useless and redundant. I will detect them by primary value analysis. When one feature’s primary value ratio is over a critical level, the feature has less predictable ability. The third one is that there are strong correlations between variables. I use correlation coefficient and VIF to detect</p>
<h4 id="1-Primary-Value-Ratio"><a href="#1-Primary-Value-Ratio" class="headerlink" title="1. Primary Value Ratio"></a>1. Primary Value Ratio</h4><p>Delect the feature whose primary value ratio over 80%</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">RowNumber = nrow(PB_lm)</span><br><span class="line">cols = colnames(PB_lm)</span><br><span class="line">Primary_Value_Ratio = c()</span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> cols)&#123;</span><br><span class="line">        Primary_Value_Ratio = c(Primary_Value_Ratio,max(table(PB_lm[i]))/RowNumber) <span class="comment"># calculate the ratio</span></span><br><span class="line">&#125;</span><br><span class="line">Primary_Value = data.frame(cols,Primary_Value_Ratio)</span><br><span class="line">Primary_Value[Primary_Value$Primary_Value_Ratio&gt;<span class="number">0.8</span>,]</span><br><span class="line"></span><br><span class="line">PB_lm = select(PB_lm,-Primary_Value[Primary_Value$Primary_Value_Ratio&gt;<span class="number">0.8</span>,]$cols)</span><br></pre></td></tr></table></figure>
<h4 id="2-Correlation-Analysis"><a href="#2-Correlation-Analysis" class="headerlink" title="2. Correlation Analysis"></a>2. Correlation Analysis</h4><p>Correlation analysis is a statistical method used to evaluate the strength of relationship between two quantitative variables. A high correlation means that two or more variables have a strong relationship with each other, while a weak correlation means that the variables are hardly related. In linear regression model, one model assumption is that variables should be independent of one another. Otherwises, one variable will be explained by other. </p>
<p><img src="correlation.png" alt="images"></p>
<h4 id="3-Skewness-of-Dependent-Variable"><a href="#3-Skewness-of-Dependent-Variable" class="headerlink" title="3. Skewness of Dependent Variable"></a>3. Skewness of Dependent Variable</h4><p>Before we detect multicollinearity problem, I transformed price into normal distribution. Price follows right-skewed Distribution. It is a common phenomenon for most real-life variables. However, for linear regression model, it is essential for residual of the model to follow normal distribution. And then the response variable will also follow. Here I used log transformation to make response variable into normal distribution. The new repsonce variable should be log(price). For log transforming, the data point which price is 0 should be deleted. </p>
<h3 id="Step-3-Run-OLS-and-check-for-linear-regression-assumptions"><a href="#Step-3-Run-OLS-and-check-for-linear-regression-assumptions" class="headerlink" title="Step 3: Run OLS and check for linear regression assumptions"></a>Step 3: Run OLS and check for linear regression assumptions</h3><p>The OLS model is the most common estimation method for linear models, and will provide us with the simplest linear regression model to base our future models off of. It’s always good to start simple then add complexity. In addition, regression model is a powerful analysis that can analyze multiple variables simultaneously to answer complex research questions. However, if you don’t satisfy the OLS assumptions, you might not be able to trust the results. OLS model is a great place to check for linear regression assumptions.</p>
<h4 id="1-Get-Variance-Inflation-Factors-VIFs-to-Detect-Multicollinearity"><a href="#1-Get-Variance-Inflation-Factors-VIFs-to-Detect-Multicollinearity" class="headerlink" title="1. Get Variance Inflation Factors (VIFs) to Detect Multicollinearity"></a>1. Get Variance Inflation Factors (VIFs) to Detect Multicollinearity</h4><figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Build the initial model</span></span><br><span class="line">lm_1 &lt;- lm(log_price ~., data = train_data3) <span class="comment"># data should been splitted into training dataset and test dataset.</span></span><br><span class="line"></span><br><span class="line">lendfitback &lt;- step(lm_1,direction = <span class="string">&quot;backward&quot;</span>) </span><br><span class="line">summary(lendfitback)</span><br><span class="line">vif(lendfitback,digits = <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>VIFs are not produced by the OLS table so you should manually extract them. They are a great way to check for multicollinearity in your model. Multicollinearity is when there is high correlation between your features. It is an assumption of linear regression that your data does not have multicollinearity, so make sure to check this. You want your VIFs under 7.</p>
<p>Here, VIFs values of neighbourhood_cleansed, zipcode, first_review are large, which indicates that there is significant Multicollinearity problem with 3 features. Therefore, we delete them and rebuild a new model. </p>
<h4 id="2-Check-Residuals-of-Model"><a href="#2-Check-Residuals-of-Model" class="headerlink" title="2. Check Residuals of Model"></a>2. Check Residuals of Model</h4><p>Also, another important asssumption is about residuals.</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Build the second model without 3 multicollinearity features</span></span><br><span class="line">lm_2 &lt;- lm(log_price ~. - neighbourhood_cleansed - zipcode-first_review, data = train_data3)</span><br><span class="line">lendfitback_2 &lt;- step(lm_2,direction = <span class="string">&quot;backward&quot;</span>)</span><br><span class="line">plot(lendfitback_2)</span><br></pre></td></tr></table></figure>
<h5 id="Resuduals-vs-Fitted"><a href="#Resuduals-vs-Fitted" class="headerlink" title="Resuduals vs. Fitted"></a>Resuduals vs. Fitted</h5><p>Even though residuals of some outliers over the range of [-2,2], most of data points randomly distributes around 0. This model fits the data well</p>
<p><img src="Resuduals_vs._Fitted.png" alt="images"></p>
<h5 id="Normal-Q-Q-Plot"><a href="#Normal-Q-Q-Plot" class="headerlink" title="Normal Q_Q Plot"></a>Normal Q_Q Plot</h5><p>Some data points deviate from the diagonal, so residuals do not follow normal distribution strictly. </p>
<p><img src="Normal_QQ_Plot.png" alt="images"></p>
<h5 id="Scale-Location"><a href="#Scale-Location" class="headerlink" title="Scale-Location"></a>Scale-Location</h5><p>Seem to be constant with no trend. There is no serious problem in Heteroskedasticity.</p>
<p><img src="Scale-Location.png" alt="images"></p>
<h5 id="Residual-Independence"><a href="#Residual-Independence" class="headerlink" title="Residual Independence"></a>Residual Independence</h5><figure class="highlight r"><table><tr><td class="code"><pre><span class="line">durbinWatsonTest(lendfitback_2)</span><br></pre></td></tr></table></figure>
<p>In Durbin Watson Test, the p-value is larger than 0.05, we can assume errors are independent. In conclusion, the residuals can be regarded as stochastic error.</p>
<h3 id="Step-4-Prediction-in-valid-dataset-and-Summary"><a href="#Step-4-Prediction-in-valid-dataset-and-Summary" class="headerlink" title="Step 4: Prediction in valid dataset and Summary"></a>Step 4: Prediction in valid dataset and Summary</h3><p>After we check the residuals, we know the model is satisfied OLS assumptions. And we use the model to predict on validation dataset.</p>
<p><img src="summary.png" alt="images"></p>
<p>The adjusted r-squared is only 0.5277. This means only 52.77% variation has been explained by the multiple regression model. In general, the higher the R-squared, the better the model fits your data. Even though the R-squared is not closing to 1, we also can infer the sample data are well correspond to the fitted (assumed) model. In fact, R-squared doesn’t tell us the entire story. After we check the residual plots, we know the residuals independent and identically distributed in normal distribution. That is, this model fits the data well.</p>
<p>In valid dataset, RMSE is 0.4206808. The standard deviation of the predictions from the actual values in valid dataset would be 0.4206808. What’s more, RMSE in training dataset is 0.3775776. The difference between these two number is small. This tells us the model is good-fit and has generalization ability. The model has the ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.</p>
<p><strong>Thanks for reading!</strong></p>
<p><strong>Please let me know if there is something I should add. And if you enjoy it, share it with your friends and colleagues : )</strong></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Airbnb Listings</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Exploring for Airbnb Dataset in Germany - Data Cleaning Challenge</title>
    <url>/yantongtt.github.io/2020/08/04/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany/</url>
    <content><![CDATA[<p>Even though these short term rentals may no longer be booked for vacation or leisure purposes because of coronavirus crackdown recently, we can not deny that Airbnb has seen a meteoric growth since its inception in 2008 with the number of rentals listed on its website growting exponentially every year. I wish this bussiness may flourish again one day, because it definitely makes a cozy place for living or travelling. In the past </p>
<p>I will be working with Prenzlauer Berg data, one neighborhood of Berlin. Located in the district of Pankow, Prenzlauer Berg is one of the most charismatic neighborhoods there, with countless of bars and cafes. For many reasons, Prenzlauer Berg is a common choice as a base for the visitors of Berlin. There are different varieties of Airbnb properties in this area, so let us driven into it!</p>
<a id="more"></a>
<p>Acturally, this project is done in R three months ago, including data preparation &amp; exploration, rental price prediction, clustering analysis to place rentals. I will write three blogs to illustrate my project here. </p>
<p>In this blog, I will perform an exploration anlysis at first. The Airbnb dataset is sourced from <a href="http://insideairbnb.com/get-the-data.html"><strong>Inside Airbnb</strong></a> website, including detailed listings data. </p>
<p><strong>A quick glance at the data shows that there are:</strong></p>
<ul>
<li>3899 unique listing in Prenzlauer Berg in total. And the first rental was up in August, 2008 by a host from Florida. </li>
<li>Rental price is ranging from 0 dollar to 5000 dollars for one night. Listing with $ 5000 price tag, a hostel with a little infomation about host and hostel, may be one outlier in dataset. </li>
</ul>
<p>In my oppinion, Data Cleaning is the most fundamental but essential work in data analysis. Not only because incorrect data can reduce the modeling effectiveness, but also because we can explore more deeper into this dataset. They exist side by side and play a part together. Without that understanding, we have no basis from which to make decisions about what data is relevant as we clean and prepare our data.</p>
<p>Data cleaning is the process of cleaning or standardising the data to make it ready for analysis. Most of times, we deal with discrepancies such as incorrect data formats, missing data, errors. It is irrational to delete data with missing values because they may reveal some information. Besides the type errors, there are still many reasons to make it. The Airbnb dataset is . It’s an interesting challenge. In this blog, I will illustrate how I cope with them.</p>
<h2 id="Handling-missing-values"><a href="#Handling-missing-values" class="headerlink" title="Handling missing values"></a>Handling missing values</h2><p>See how many missing data points we have</p>
<p>In the raw dataset, there are too many missing values in the form of entire rows with all NA value. We just delete them because there is no meaning. After we delete 1131 NA rows, let’s see how many we have in each column.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Feature</th>
<th>Number of NA</th>
<th>How it looks like</th>
</tr>
</thead>
<tbody>
<tr>
<td>square_feet</td>
<td>2695</td>
<td>NA 720 NA NA NA NA NA NA NA 1012</td>
</tr>
<tr>
<td>license</td>
<td>2564</td>
<td><NA> 03/Z/RA/003410-18 <NA></td>
</tr>
<tr>
<td>monthly_price</td>
<td>2392</td>
<td>“$1,000.00”, “​$1,001.00”, NA</td>
</tr>
<tr>
<td>weekly_price</td>
<td>2253</td>
<td>“$1,000.00”,”$1,050.00”,NA</td>
</tr>
<tr>
<td>notes</td>
<td>1894</td>
<td>“ I have a cat, please be aware if you have an allergy.”, NA</td>
</tr>
<tr>
<td>interaction</td>
<td>1530</td>
<td>“ feel free to contact me or my person of trust any time”,NA</td>
</tr>
<tr>
<td>host_response_time</td>
<td>1502</td>
<td>“a few days or more”, “within a week”, NA</td>
</tr>
<tr>
<td>host_response_rate</td>
<td>1502</td>
<td>“0%,”10%”,”100%”, NA</td>
</tr>
<tr>
<td>access</td>
<td>1462</td>
<td>“ Available inside the room there will be a kettle …”, NA</td>
</tr>
<tr>
<td>neighborhood_overview</td>
<td>1335</td>
<td>“ A mix of Celebrities the Young Creative Scene…”, NA</td>
</tr>
<tr>
<td>house_rules</td>
<td>1313</td>
<td>“<em> keine laute Musik </em> bitte nicht mit…” ,NA</td>
</tr>
<tr>
<td>host_about</td>
<td>1271</td>
<td>“We love to travel ourselves a lot and prefer to stay in  apartments…” NA</td>
</tr>
<tr>
<td>transit</td>
<td>1152</td>
<td>“The apartment is situated 3min on foot from the Station Hermannstr.  “,NA</td>
</tr>
<tr>
<td>security_deposit</td>
<td>1064</td>
<td>“$0.00”,”$1,000.00”, NA</td>
</tr>
<tr>
<td>cleaning_fee</td>
<td>768</td>
<td>“$0.00”,”$10.00”,NA</td>
</tr>
<tr>
<td>host_neighbourhood</td>
<td>606</td>
<td>“Adlershof”,”Alt-Hohenschönhausen”,NA</td>
</tr>
<tr>
<td>review_scores_value</td>
<td>505</td>
<td>NA 9 9 10 9 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_checkin</td>
<td>505</td>
<td>NA 9 10 10 9 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_location</td>
<td>504</td>
<td>NA 10 10 10 9 10 10 10 NA 10</td>
</tr>
<tr>
<td>review_scores_communication</td>
<td>502</td>
<td>NA 9 10 10 10 10 8 9 NA 9</td>
</tr>
<tr>
<td>review_scores_accuracy</td>
<td>502</td>
<td>NA 9 10 10 10 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_cleanliness</td>
<td>501</td>
<td>NA 9 10 10 10 10 9 10 NA 9</td>
</tr>
<tr>
<td>review_scores_rating</td>
<td>500</td>
<td>NA 92 96 100 93 96 87 94 NA 91</td>
</tr>
<tr>
<td>reviews_per_month</td>
<td>445</td>
<td>NA 1.25 1.75 0.15 0.23 2.83 0.75 0.18</td>
</tr>
<tr>
<td>first_review</td>
<td>445</td>
<td>2009-06-20,”2009-08-18”,NA</td>
</tr>
<tr>
<td>last_review</td>
<td>444</td>
<td>2010-09-16,”2011-01-26”,NA</td>
</tr>
<tr>
<td>summary</td>
<td>130</td>
<td>“Ich bin sehr stolz darauf meine Wohnung freundlichen  Berlinbesuchern”, NA</td>
</tr>
<tr>
<td>zipcode</td>
<td>83</td>
<td>10115,”10115\n10115”,NA</td>
</tr>
<tr>
<td>state</td>
<td>19</td>
<td>“berlin”,”Berlin”,NA</td>
</tr>
<tr>
<td>description</td>
<td>16</td>
<td>“ 24m2 room in Neukoelln with large double bed, …” NA</td>
</tr>
<tr>
<td>host_location</td>
<td>11</td>
<td>“Berlin, Berlin, Germany “,”Coledale, New South Wales,  Australia”, NA</td>
</tr>
<tr>
<td>market</td>
<td>9</td>
<td>Berlin,”Juarez”,NA</td>
</tr>
<tr>
<td>name</td>
<td>8</td>
<td>“\tThe right place for your  stay”, NA</td>
</tr>
<tr>
<td>bathrooms</td>
<td>7</td>
<td>NA 1.0 1.0 2.5 1.0 1.0</td>
</tr>
<tr>
<td>beds</td>
<td>2</td>
<td>NA 2 2   7 1 1</td>
</tr>
<tr>
<td>host_total_listings_count</td>
<td>1</td>
<td>NA 1 1   1 3 1</td>
</tr>
<tr>
<td>host_since</td>
<td>1</td>
<td>“2008-10-19”, “2009-05-16”,  “2009-08-25”,”2009-11-18”, NA</td>
</tr>
<tr>
<td>host_name</td>
<td>1</td>
<td>“Britta”,”Bright”,”Philipp”, “ Chris +  Oliver”, “Wolfram”,NA</td>
</tr>
<tr>
<td>host_listings_count</td>
<td>1</td>
<td>NA 1 1 1 3 1 1 2 NA 2 …</td>
</tr>
<tr>
<td>host_is_superhost</td>
<td>1</td>
<td>“f”,”t”, NA</td>
</tr>
<tr>
<td>host_identity_verified</td>
<td>1</td>
<td>“f”,”t”, NA</td>
</tr>
<tr>
<td>host_has_profile_pic</td>
<td>1</td>
<td>“f”,”t”, NA</td>
</tr>
<tr>
<td>city</td>
<td>1</td>
<td>Berlin Berlin Berlin Berlin Berlin  NA</td>
</tr>
<tr>
<td>bedrooms</td>
<td>1</td>
<td>1 1 4 0 1 2 2 2 0 2</td>
</tr>
<tr>
<td>street</td>
<td>0</td>
<td>“\nKreuzberg, Berlin, Germany”…</td>
</tr>
<tr>
<td>smart_location</td>
<td>0</td>
<td>“\nKreuzberg, Germany”…</td>
</tr>
<tr>
<td>room_type</td>
<td>0</td>
<td>Entire home/apt  Private  room   Shared room</td>
</tr>
<tr>
<td>requires_license</td>
<td>0</td>
<td>f  t</td>
</tr>
<tr>
<td>require_guest_profile_picture</td>
<td>0</td>
<td>f  t</td>
</tr>
<tr>
<td>require_guest_phone_verification</td>
<td>0</td>
<td>f  t</td>
</tr>
<tr>
<td>property_type</td>
<td>0</td>
<td>Apartment  Apartment  Apartment   Apartment  Condominium …</td>
</tr>
<tr>
<td>price</td>
<td>0</td>
<td>$0.00,”$1,000.00”</td>
</tr>
<tr>
<td>number_of_reviews</td>
<td>0</td>
<td>143 197 6 23 279 56 18 163 28 69</td>
</tr>
<tr>
<td>neighbourhood_group_cleansed</td>
<td>0</td>
<td>Charlottenburg-Wilm.,…</td>
</tr>
<tr>
<td>neighbourhood_cleansed</td>
<td>0</td>
<td>Adlershof,”Albrechtstr.”…</td>
</tr>
<tr>
<td>neighbourhood</td>
<td>0</td>
<td>Adlershof,”Alt-Hohenschönhausen” …</td>
</tr>
<tr>
<td>minimum_nights</td>
<td>0</td>
<td>62 2 6 90 3 3 3 3 90 60</td>
</tr>
<tr>
<td>maximum_nights</td>
<td>0</td>
<td>1125 10 14 1125 30 30 21 1125 1125 365</td>
</tr>
<tr>
<td>longitude</td>
<td>0</td>
<td>13.4 13.4 13.4 13.4 13.4</td>
</tr>
<tr>
<td>latitude</td>
<td>0</td>
<td>52.5 52.5 52.5 52.5 52.5</td>
</tr>
<tr>
<td>last_scraped</td>
<td>0</td>
<td>2018-11-07 2018-11-09</td>
</tr>
<tr>
<td>is_location_exact</td>
<td>0</td>
<td>“f”,”t”</td>
</tr>
<tr>
<td>is_business_travel_ready</td>
<td>0</td>
<td>“f”,”t”</td>
</tr>
<tr>
<td>instant_bookable</td>
<td>0</td>
<td>“f”,”t”</td>
</tr>
<tr>
<td>id</td>
<td>0</td>
<td>3176 7071 9991 14325 17409 20858 24569</td>
</tr>
<tr>
<td>host_verifications</td>
<td>0</td>
<td>“[‘email’, ‘facebook’, ‘jumio’, ‘offline_government_id’,  ‘government_id’]”…</td>
</tr>
<tr>
<td>host_id</td>
<td>0</td>
<td>3718 17391 33852 55531 67590</td>
</tr>
<tr>
<td>has_availability</td>
<td>0</td>
<td>“t”</td>
</tr>
<tr>
<td>guests_included</td>
<td>0</td>
<td>2 1 5 1 1 2 2 4 1 2</td>
</tr>
<tr>
<td>extra_people</td>
<td>0</td>
<td>$0.00”,”$10.00”,…</td>
</tr>
<tr>
<td>country_code</td>
<td>0</td>
<td>“DE”</td>
</tr>
<tr>
<td>country</td>
<td>0</td>
<td>“Germany”</td>
</tr>
<tr>
<td>cancellation_policy</td>
<td>0</td>
<td>“flexible”,”moderate”,..</td>
</tr>
<tr>
<td>calendar_updated</td>
<td>0</td>
<td>“ 1 week ago”, “10 months ago”, “11 months  ago”, “12 months ago”…</td>
</tr>
<tr>
<td>calendar_last_scraped</td>
<td>0</td>
<td>“2018-11-07”,”2018-11-09”</td>
</tr>
<tr>
<td>calculated_host_listings_count</td>
<td>0</td>
<td>1  2  3    4  5  6    7  8  9   10  11  12   13  15  16   19  45</td>
</tr>
<tr>
<td>bed_type</td>
<td>0</td>
<td>Airbed     Couch     Futon Pull-out Sofa   Real Bed</td>
</tr>
<tr>
<td>availability_90</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>availability_60</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>availability_365</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>availability_30</td>
<td>0</td>
<td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td>
</tr>
<tr>
<td>amenities</td>
<td>0</td>
<td>{Internet,Wifi,Kitchen,”Buzzer/wireless intercom”,Crib}…</td>
</tr>
<tr>
<td>accommodates</td>
<td>0</td>
<td>1  2    3  4  5    6  7  8    9  10  11   12  16</td>
</tr>
</tbody>
</table>
</div>
<p> That seems like a lot! For almost cells in this dataset are empty. In the next step, we are going to take a closer look at some of the columns with missing values and try to fighure out what might be going on with them,</p>
<h3 id="Figure-out-why-the-data-is-missing"><a href="#Figure-out-why-the-data-is-missing" class="headerlink" title="Figure out why the data is missing"></a>Figure out why the data is missing</h3><p>This is the point at which we get into the part of data science that I like to call “data intution”, by which I mean “really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis”. It can be a frustrating part of data science, especially if you’re newer to the field and don’t have a lot of experience. For dealing with missing values, you’ll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:</p>
<p>Before we impute the missing value, it is a critical point to get into this part. This helps us to get “data intuition” by really looking at your dataset and trying to figure out why it is this way and how that will affect your analysis. Keep the feature or not is a really important question for analysis, and you can ask yourself <strong>“Is this value missing becuase it wasn’t recorded or becuase it dosen’t exist?”</strong> </p>
<p>For the first thing, like square_feet of house is existed but some hosts do not input this kind of data for many possible reasons, we should consider to impute them in a proper way or delete them directly. These value you probaly do not want to keep them as NA. Also, you can try to guess what it might have been based on the other values in that column and row. However, if the value is missing because it doesn’t exist, for example monthly_price and weekly_price for some listings, we should consider feature engineering to transform them into more meaningful features. Some listings do not have monthly prices or weekly price because they do not offer the discounts for long-term renting. It tells us a lot! Something like “probably the host want to make profit by offering discount because his/her house is more suitable for  long-term accommodation. Maybe no visitor who traveling here wants to live in this place. Or the host think only rent for one day can not get profit, and he/she have to clean or manage this accommodation everyday. In this way, I made new features about how many discounts that the house provide. (I will talk it in the latter content.)</p>
<h3 id="Ways-to-handle-missing-values"><a href="#Ways-to-handle-missing-values" class="headerlink" title="Ways to handle missing values"></a>Ways to handle missing values</h3><p>Let me talk about the way to handle missing values in many situations. </p>
<ul>
<li><strong>Drop Columns with Missing Values</strong></li>
</ul>
<p>Even though square_feet is an significant variable toward rent price, there are too many missing value here. inproper imputation may bring more bias. Let take mean imputation for an example. If we impute missing value as mean value, and regard it is missing randomly, then the mean estimator is unbiased. This is the logic to achieve the mean i putation. However, if we do in this way, we do not keep the relationships with other variables. Statistically, the estimator is unbiased, but the standard deviation is biased. And most of our interest is to find the relationships, so mean imputation is not a good choice.  In this case, we can not use simple option to fill square_feet with the null value. Also, we can not find any straight relationships with this variable.  Take many factors into consideration, we just drop this column directly. </p>
<ul>
<li><p><strong>Feature Engineering</strong></p>
<ul>
<li>Whether or not</li>
</ul>
<p>license is a feature discribe the host’s license. If host has that, they should upload the exact license code here. Acturally, we don’t care about the exact text information on that. Whether has license or not is more meaningful for us. Therefore, I transform the license code as “T”, and missing value as “F”. “T” is the abbreviation of “true”, representing the host has license code for the accomendation. While “F” is “false”, which means they do have license code. </p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">PB_3$license[which(is.na(PB_3$license)==<span class="literal">F</span>)] = <span class="string">&quot;Yes&quot;</span></span><br><span class="line">PB_3$license[which(is.na(PB_3$license)==<span class="literal">T</span>)] = <span class="string">&quot;No&quot;</span></span><br></pre></td></tr></table></figure>
<p>The dataset still has some variables which can handle in the same method.</p>
<ul>
<li>Date to Time </li>
</ul>
<p>First review date is an important feature for analysis. It tells us how long does the host operating this accomendation. However, as a category variable, first_review will be re-encoded as many 0/1 variables. We can transform the date as days, a numeric variable. How to transform? Create a new feature.</p>
<p>Here, I created a new feature Business_interval, which measures number of day after this airbnb has first review. Also, same method works on other date variables, like last_review, host_since. I created a feature named No_Business_interval. This measures number of day after the last review. Longer interval means this airbnb is no business recently. And the third feature is operation_interval, which is number of day after host_since. This measure how long that the airbnb is. </p>
<p>There are missing value in first_review and last_review the two variables. The reason would be there is no review. Therefore, the time interval would be 0.</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">first_review = as.Date(PB_2$first_review,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">last_review = as.Date(PB_2$last_review,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">host_since = as.Date(PB_2$host_since,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">Business_interval = today() - first_review</span><br><span class="line">No_Business_interval = today() - last_review</span><br><span class="line">operation_interval = today() - host_since</span><br><span class="line"></span><br><span class="line">Business_interval[is.na(Business_interval) == <span class="literal">T</span>] = <span class="number">0</span></span><br><span class="line">No_Business_interval[is.na(No_Business_interval) == <span class="literal">T</span>] = <span class="number">0</span> </span><br><span class="line">operation_interval[is.na(operation_interval) == <span class="literal">T</span>] = <span class="number">0</span> </span><br></pre></td></tr></table></figure>
<ul>
<li>Binning into category</li>
</ul>
<p>In my oppinion, missing is also information on that feature. I dont like delete or impute them directly. Sometimes, I’d like to keep them, making a new value for them. That is, I will create a new level named, “N/A” for the missing value. However, problems come out. If we use “N/A” to present the missing value, then the variable will be categorical feature. It is okay for the features with a small amount of values like gender. After created new level, gender can be “female”, “male”, “don’t know”. But for numeric variables like host_response_rate, value is numeric ranging from 0 to 100%. In this case, I created new level as “N/A” (not available), and binned the rate into two group, high rate(host_response_rate &gt;= 80%) and low rate(host_response_rate &lt; 80%). The information will be reducted but it is more effeciently for analysis and do not influence the analysis a lot. </p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># transform missing value to &quot;N/A&quot;(not available). create a level for null value</span></span><br><span class="line">PB_3$host_response_time[is.na(PB_3$host_response_time)] &lt;- <span class="string">&quot;N/A&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># host_response_rate &#123;high: host_response_rate &gt;= 80%&#125;, &#123;low: host_response_rate &lt; 80%&#125;, &#123;N/A: not available&#125;</span></span><br><span class="line">PB_3$host_response_rate &lt;- as.numeric(sub(<span class="string">&quot;%&quot;</span>,<span class="string">&quot;&quot;</span>,PB_3$host_response_rate))/<span class="number">100</span></span><br><span class="line">summary(as.factor(PB_3$host_response_rate))</span><br><span class="line">PB_3$host_response_rate[PB_3$host_response_rate&gt;= <span class="number">0.8</span>] &lt;- <span class="string">&quot;high&quot;</span></span><br><span class="line">PB_3$host_response_rate[PB_3$host_response_rate&lt; <span class="number">0.8</span>] &lt;- <span class="string">&quot;low&quot;</span></span><br><span class="line">PB_3$host_response_rate[is.na(PB_3$host_response_rate)] &lt;- <span class="string">&quot;N/A&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Other feature engineering</li>
</ul>
<p>As I mention above, we can use monthly_price and weekly_price to create new features about the discount that host offered. In this step, we can not use the exact number from weekly_price and monthly_price. Because the two prices are calculated after we decide the daily price. That is, it will be higher when daily price is high. However, we can measure whether the airbnb provide the discount or not. If providing, we use the value as “offer discount”, and if not, the value will be “no discount”.</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(PB_lm$weekly_price)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (is.na(PB_lm$weekly_price[i]) == <span class="literal">T</span>)&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = PB_lm$price[i]*<span class="number">7</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">weekly_discount_ratio = (PB_lm$weekly_price)/(PB_lm$price*<span class="number">7</span>)</span><br><span class="line">weekly_discount_ratio[is.na(weekly_discount_ratio) == <span class="literal">T</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment">#summary(weekly_discount_ratio)</span></span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(weekly_discount_ratio))&#123;</span><br><span class="line">        <span class="keyword">if</span> (weekly_discount_ratio[i] &lt; <span class="number">1</span> )&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = <span class="string">&quot;offer discount&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = <span class="string">&quot;no discount&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">PB_lm$weekly_price = as.factor(PB_lm$weekly_price)</span><br><span class="line"><span class="comment">#summary(PB_lm$weekly_price)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><strong>Filling in missing values automatically</strong></li>
</ul>
<p>Sure, I mentioned that I do not like mean imputation in <em>Drop Columns with Missing Values</em>. But the situation is on condition that mean value is meaningless. Acturally, we imputate the missing value using the idea of maximum likelihood. Some features like host_has_profile_pic, presenting whether host has profile picture or not, have most value of “t”. Most of host upload their profile picture on the sites. Therefore, we can use mode value to impute the missing value. </p>
<h2 id="Inconsistent-Data-Entry"><a href="#Inconsistent-Data-Entry" class="headerlink" title="Inconsistent Data Entry"></a>Inconsistent Data Entry</h2><h2 id="Scale-and-Normalize-Data"><a href="#Scale-and-Normalize-Data" class="headerlink" title="Scale and Normalize Data"></a>Scale and Normalize Data</h2><h2 id="Parsing-Dates"><a href="#Parsing-Dates" class="headerlink" title="Parsing Dates"></a>Parsing Dates</h2>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Airbnb Listings</tag>
        <tag>Exploratory Analysis</tag>
        <tag>Data Cleaning</tag>
        <tag>Feature Engineering</tag>
      </tags>
  </entry>
</search>
