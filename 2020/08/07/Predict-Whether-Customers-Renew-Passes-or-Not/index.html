<!DOCTYPE html>
<html lang="zh">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/yantongtt.github.io/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/yantongtt.github.io/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/yantongtt.github.io/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/yantongtt.github.io/images/logo.svg" color="#222">

<link rel="stylesheet" href="/yantongtt.github.io/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Raleway:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/yantongtt.github.io/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/yantongtt.github.io/lib/pace/pace-theme-corner-indicator.min.css">
  <script src="/yantongtt.github.io/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jerilyt.github.io","root":"/yantongtt.github.io/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Instead of simple upmarket pricing system, some amusement parks adopt the price steategy to attract more customer percepetions and optimize the total revenue with “fast-pass” tickets. Remember when we">
<meta property="og:type" content="article">
<meta property="og:title" content="Does Your Customer Want to Renew Fast-Pass?">
<meta property="og:url" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/index.html">
<meta property="og:site_name" content="Yantong&#39;s Blog">
<meta property="og:description" content="Instead of simple upmarket pricing system, some amusement parks adopt the price steategy to attract more customer percepetions and optimize the total revenue with “fast-pass” tickets. Remember when we">
<meta property="og:locale">
<meta property="og:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/corr.png">
<meta property="og:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/lr_result.png">
<meta property="og:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/lr_ROC.png">
<meta property="og:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/rf_result.png">
<meta property="og:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/rf_ROC.png">
<meta property="og:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/xgb_result.png">
<meta property="og:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/xbg_ROC.png">
<meta property="article:published_time" content="2020-08-07T07:48:12.000Z">
<meta property="article:modified_time" content="2020-08-07T07:52:29.892Z">
<meta property="article:author" content="Yantong Li">
<meta property="article:tag" content="XGBoost">
<meta property="article:tag" content="Classification">
<meta property="article:tag" content="Logistic Regression">
<meta property="article:tag" content="Random Forest">
<meta property="article:tag" content="Marketing Strategy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/corr.png">

<link rel="canonical" href="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh'
  };
</script>

  <title>Does Your Customer Want to Renew Fast-Pass? | Yantong's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/yantongtt.github.io/atom.xml" title="Yantong's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>
<a target="_blank" rel="noopener" href="https://www.github.com/jerilyt" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/yantongtt.github.io/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yantong's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/yantongtt.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/yantongtt.github.io/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/yantongtt.github.io/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/yantongtt.github.io/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh">
    <link itemprop="mainEntityOfPage" href="https://jerilyt.github.io/yantongtt.github.io/2020/08/07/Predict-Whether-Customers-Renew-Passes-or-Not/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/yantongtt.github.io/images/avatar.png">
      <meta itemprop="name" content="Yantong Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yantong's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Does Your Customer Want to Renew Fast-Pass?
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-08-07 15:48:12 / Modified: 15:52:29" itemprop="dateCreated datePublished" datetime="2020-08-07T15:48:12+08:00">2020-08-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Instead of simple upmarket pricing system, some amusement parks adopt the price steategy to attract more customer percepetions and optimize the total revenue with “fast-pass” tickets. Remember when we went to the Universal, some of us must be willing to pay extra fee on the fast-pass for accessing to theme park in advance. That saves a lot of time! </p>
<p>Seasonal fast-pass is another type which allows visitors renewing for next season. In fact, even though the visitors enjoyed theme park, only fewer part  of them are willing to renew the fast-pass. In this case, the analysis on building a model that classifies households as season pass renewers or non-season pass renewers may help managers better understand their target customers. That is, understanding what consumers want and need is an ongoing imperative for marketing strategy. Machine Learning can make this job much easier and efficient.</p>
<p>In this post, I will present some frequently-used machine learing algorithms, which are Logistic Regression, Random Forest and XGBoost,  to develop prediction models. </p>
<a id="more"></a>
<h2 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a>Exploratory Data Analysis</h2><p>This dataset records the historical spending and personal information of Greater NYC households that held theme park family season passes. We start with checking out how our data looks like and visualize how it interacts with our label (the final column, renew or not). Let’s start with importing our data and print the random ten rows:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>householdID</th>
<th>visits</th>
<th>avgrides_perperson</th>
<th>avgmerch_perperson</th>
<th>avggoldzone_perperson</th>
<th>avgfood_perperson</th>
<th>goldzone_playersclub</th>
<th>own_car</th>
<th>homestate</th>
<th>FB_Like</th>
<th>renew</th>
</tr>
</thead>
<tbody>
<tr>
<td>2913</td>
<td>2</td>
<td>6.3</td>
<td>26.6</td>
<td>109.6</td>
<td>39.8</td>
<td>0</td>
<td>1</td>
<td>NY</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1613</td>
<td>4</td>
<td>12.4</td>
<td>15.6</td>
<td>82.8</td>
<td>62.5</td>
<td>0</td>
<td>1</td>
<td>NY</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1216</td>
<td>3</td>
<td>11.2</td>
<td>36.3</td>
<td>62.5</td>
<td>6.8</td>
<td>0</td>
<td>1</td>
<td>CT</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1155</td>
<td>1</td>
<td>8.1</td>
<td>19.2</td>
<td>9.6</td>
<td>42.2</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1110</td>
<td>18</td>
<td>11</td>
<td>10.7</td>
<td>69.9</td>
<td>16.2</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>865</td>
<td>3</td>
<td>11.5</td>
<td>73.4</td>
<td>166.3</td>
<td>4.6</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>852</td>
<td>2</td>
<td>13.2</td>
<td>25.9</td>
<td>26.7</td>
<td>43.3</td>
<td>0</td>
<td>1</td>
<td>NJ</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>586</td>
<td>1</td>
<td>9.7</td>
<td>52.8</td>
<td>26.7</td>
<td>39.3</td>
<td>0</td>
<td>0</td>
<td>NJ</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>346</td>
<td>9</td>
<td>10.4</td>
<td>25.4</td>
<td>132.6</td>
<td>8.6</td>
<td>0</td>
<td>1</td>
<td>CT</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>172</td>
<td>2</td>
<td>2.2</td>
<td>26.1</td>
<td>40</td>
<td>12.4</td>
<td>0</td>
<td>1</td>
<td>CT</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>A better way to see all the columns and their data type is using <strong>.info()</strong> method:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">nyc_historical.info()</span><br><span class="line"><span class="comment"># &lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span></span><br><span class="line"><span class="comment"># RangeIndex: 3200 entries, 0 to 3199</span></span><br><span class="line"><span class="comment"># Data columns (total 11 columns):</span></span><br><span class="line"><span class="comment">#  #   Column                 Non-Null Count  Dtype  </span></span><br><span class="line"><span class="comment"># ---  ------                 --------------  -----  </span></span><br><span class="line"><span class="comment">#  0   householdID            3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  1   visits                 3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  2   avgrides_perperson     3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  3   avgmerch_perperson     3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  4   avggoldzone_perperson  3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  5   avgfood_perperson      3200 non-null   float64</span></span><br><span class="line"><span class="comment">#  6   goldzone_playersclub   3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  7   own_car                3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  8   homestate              3200 non-null   object </span></span><br><span class="line"><span class="comment">#  9   FB_Like                3200 non-null   int64  </span></span><br><span class="line"><span class="comment">#  10  renew                  3200 non-null   int64  </span></span><br><span class="line"><span class="comment"># dtypes: float64(4), int64(6), object(1)</span></span><br><span class="line"><span class="comment"># memory usage: 275.1+ KB</span></span><br></pre></td></tr></table></figure>
<p>There is no missing value. Also, our dataset falls under two categories: one is categorical features: own_car, homestate, FB_Like renew, and another is numerical features: visits, avgrides_perperson, avgmerch_perperson… </p>
<h3 id="Target-Variable"><a href="#Target-Variable" class="headerlink" title="Target Variable"></a>Target Variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nyc_historical.renew.value_counts()</span><br><span class="line"><span class="comment"># 1    2126</span></span><br><span class="line"><span class="comment"># 0    1074</span></span><br><span class="line"><span class="comment"># Name: renew, dtype: int64</span></span><br></pre></td></tr></table></figure>
<p>This dataset is imbalanced, nearly 66% householders renewing the pass card. But fortunately, it is an extreme case. Dealing with slight imbalanced data, we will measure model performance with recall, F1 score, ROC curve, and its AUC</p>
<h3 id="Home-State"><a href="#Home-State" class="headerlink" title="Home State"></a>Home State</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nyc_historical.homestate.value_counts()</span><br><span class="line"><span class="comment"># NJ    1076</span></span><br><span class="line"><span class="comment"># NY    1065</span></span><br><span class="line"><span class="comment"># CT    1059</span></span><br><span class="line"><span class="comment"># Name: homestate, dtype: int64</span></span><br></pre></td></tr></table></figure>
<p>That seems like our sample source are balanced in terms of regional distribution. </p>
<p>Now, let’s start our feature engineering!</p>
<h2 id="Feature-Engineering-and-Selection"><a href="#Feature-Engineering-and-Selection" class="headerlink" title="Feature Engineering and Selection"></a>Feature Engineering and Selection</h2><p>As a side note, in the dataset we have, homestate column is string with three values. We convert it to integer to make it easier to use in our analysis.</p>
<h3 id="One-Hot-Encoding"><a href="#One-Hot-Encoding" class="headerlink" title="One-Hot Encoding"></a>One-Hot Encoding</h3><p>A method to create new columns out of categorical ones by assigning 0 &amp; 1s</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">homestate_dummy = pd.get_dummies(nyc_historical.homestate)</span><br><span class="line">homestate_dummy = homestate_dummy.drop([<span class="string">&quot;CT&quot;</span>], axis=<span class="number">1</span>)</span><br><span class="line">homestate_dummy.rename(columns = &#123;<span class="string">&#x27;NJ&#x27;</span>:<span class="string">&#x27;homestate_NJ&#x27;</span>&#125;, inplace = <span class="literal">True</span>) </span><br><span class="line">homestate_dummy.rename(columns = &#123;<span class="string">&#x27;NY&#x27;</span>:<span class="string">&#x27;homestate_NY&#x27;</span>&#125;, inplace = <span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">renew_data = pd.concat([nyc_historical,homestate_dummy],axis = <span class="number">1</span>)</span><br><span class="line">renew_data = renew_data.drop([<span class="string">&#x27;homestate&#x27;</span>],axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><h4 id="1-Compute-the-primary-value-ratio-of-each-variable"><a href="#1-Compute-the-primary-value-ratio-of-each-variable" class="headerlink" title="1. Compute the primary value ratio of each variable."></a>1. Compute the primary value ratio of each variable.</h4><p>If the ratio is larger than 85%, just delete it. We only keep the variables which can identify the target variable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">primaryvalue_ratio</span>(<span class="params">data</span>):</span></span><br><span class="line">    recordcount = data.shape[<span class="number">0</span>]  <span class="comment">#number of row</span></span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> data.columns:</span><br><span class="line">        primaryvalue = data[col].value_counts().index[<span class="number">0</span>]</span><br><span class="line">        ratio = float(data[col].value_counts().iloc[<span class="number">0</span>])/recordcount</span><br><span class="line">        x.append([primaryvalue,ratio])</span><br><span class="line">    feature_primaryvalue_ratio = pd.DataFrame(x,index = data.columns)</span><br><span class="line">    feature_primaryvalue_ratio.columns = [<span class="string">&quot;primaryvalue&quot;</span>,<span class="string">&quot;primaryvalue_ratio&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> feature_primaryvalue_ratio</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = primaryvalue_ratio(X_train)</span><br><span class="line">d.sort_values([<span class="string">&#x27;primaryvalue_ratio&#x27;</span>], ascending=[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>primaryvalue</th>
<th>primaryvalue_ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>goldzone_playersclub</td>
<td>0</td>
<td>0.821429</td>
</tr>
<tr>
<td>own_car</td>
<td>1</td>
<td>0.750446</td>
</tr>
<tr>
<td>homestate_NJ</td>
<td>0</td>
<td>0.663393</td>
</tr>
<tr>
<td>homestate_NY</td>
<td>0</td>
<td>0.662054</td>
</tr>
<tr>
<td>FB_Like</td>
<td>0</td>
<td>0.5375</td>
</tr>
<tr>
<td>visits</td>
<td>2</td>
<td>0.144643</td>
</tr>
<tr>
<td>avgrides_perperson</td>
<td>10</td>
<td>0.022321</td>
</tr>
<tr>
<td>avgfood_perperson</td>
<td>18.4</td>
<td>0.005804</td>
</tr>
<tr>
<td>avgmerch_perperson</td>
<td>23.1</td>
<td>0.004464</td>
</tr>
<tr>
<td>avggoldzone_perperson</td>
<td>85.8</td>
<td>0.003571</td>
</tr>
</tbody>
</table>
</div>
<p>As we can see, there is no variable being demonated by its primary value. Therefore, we keep all the variables. (we set the criticle point as 85%)</p>
<h4 id="2-Check-the-missing-value"><a href="#2-Check-the-missing-value" class="headerlink" title="2. Check the missing value"></a>2. Check the missing value</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">na_table = X_train.isnull().sum(axis = <span class="number">0</span>)/X_train.shape[<span class="number">0</span>]</span><br><span class="line">na_table <span class="comment"># there is no missing value</span></span><br><span class="line"><span class="comment"># visits                   0.0</span></span><br><span class="line"><span class="comment"># avgrides_perperson       0.0</span></span><br><span class="line"><span class="comment"># avgmerch_perperson       0.0</span></span><br><span class="line"><span class="comment"># avggoldzone_perperson    0.0</span></span><br><span class="line"><span class="comment"># avgfood_perperson        0.0</span></span><br><span class="line"><span class="comment"># goldzone_playersclub     0.0</span></span><br><span class="line"><span class="comment"># own_car                  0.0</span></span><br><span class="line"><span class="comment"># FB_Like                  0.0</span></span><br><span class="line"><span class="comment"># homestate_NJ             0.0</span></span><br><span class="line"><span class="comment"># homestate_NY             0.0</span></span><br><span class="line"><span class="comment"># dtype: float64</span></span><br></pre></td></tr></table></figure>
<p>There is no missing value.</p>
<h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><h3 id="Traning-Test-Split"><a href="#Traning-Test-Split" class="headerlink" title="Traning Test Split"></a>Traning Test Split</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X = renew_data[[<span class="string">&quot;visits&quot;</span>,<span class="string">&quot;avgrides_perperson&quot;</span>,<span class="string">&quot;avgmerch_perperson&quot;</span>,<span class="string">&quot;avggoldzone_perperson&quot;</span>,<span class="string">&quot;avgfood_perperson&quot;</span>,<span class="string">&quot;goldzone_playersclub&quot;</span>,</span><br><span class="line"><span class="string">&quot;own_car&quot;</span>,<span class="string">&quot;FB_Like&quot;</span>,<span class="string">&quot;homestate_NJ&quot;</span>,<span class="string">&quot;homestate_NY&quot;</span>]]</span><br><span class="line">y = renew_data[<span class="string">&quot;renew&quot;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">21</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Part-I-Logistic-Regression-Model"><a href="#Part-I-Logistic-Regression-Model" class="headerlink" title="Part I: Logistic Regression Model"></a>Part I: Logistic Regression Model</h3><p>Predicting renew ro not is a binary classification problem. Along with being a robust model, Logistic Regression provides interpretable outcomes. Let me sort out our steps to follow for building a Logistic Regression model:</p>
<ol>
<li>Prepare the data (inputs for the model)</li>
<li>Fit the model and see the model summary</li>
</ol>
<h4 id="Prepare-the-data-inputs-for-the-model"><a href="#Prepare-the-data-inputs-for-the-model" class="headerlink" title="Prepare the data (inputs for the model)"></a>Prepare the data (inputs for the model)</h4><p>Logistic regression is a linear model, therefore, we should do the correlation analysis to remove the multicorreltion</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cor_table = X_train.corr()</span><br><span class="line">cor_table</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>visits</th>
<th>avgrides_perperson</th>
<th>avgmerch_perperson</th>
<th>avggoldzone_perperson</th>
<th>avgfood_perperson</th>
<th>goldzone_playersclub</th>
<th>own_car</th>
<th>FB_Like</th>
<th>homestate_NJ</th>
<th>homestate_NY</th>
</tr>
</thead>
<tbody>
<tr>
<td>visits</td>
<td>1</td>
<td>0.025168</td>
<td>0.006725</td>
<td>0.013401</td>
<td>0.00542</td>
<td>-0.00215</td>
<td>0.005085</td>
<td>0.00136</td>
<td>0.00836</td>
<td>-0.02133</td>
</tr>
<tr>
<td>avgrides_perperson</td>
<td>0.025168</td>
<td>1</td>
<td>-0.02006</td>
<td>-0.01301</td>
<td>-0.01464</td>
<td>-0.02224</td>
<td>-0.01931</td>
<td>-0.02509</td>
<td>0.005241</td>
<td>0.0181</td>
</tr>
<tr>
<td>avgmerch_perperson</td>
<td>0.006725</td>
<td>-0.02006</td>
<td>1</td>
<td>-0.00335</td>
<td>-0.01869</td>
<td>0.011748</td>
<td>-0.00907</td>
<td>-0.01407</td>
<td>0.003229</td>
<td>0.01602</td>
</tr>
<tr>
<td>avggoldzone_perperson</td>
<td>0.013401</td>
<td>-0.01301</td>
<td>-0.00335</td>
<td>1</td>
<td>-0.03123</td>
<td>-0.02063</td>
<td>0.012132</td>
<td>-0.02225</td>
<td>0.017494</td>
<td>0.025145</td>
</tr>
<tr>
<td>avgfood_perperson</td>
<td>0.00542</td>
<td>-0.01464</td>
<td>-0.01869</td>
<td>-0.03123</td>
<td>1</td>
<td>0.005289</td>
<td>-0.02512</td>
<td>0.037274</td>
<td>0.028503</td>
<td>-0.024</td>
</tr>
<tr>
<td>goldzone_playersclub</td>
<td>-0.00215</td>
<td>-0.02224</td>
<td>0.011748</td>
<td>-0.02063</td>
<td>0.005289</td>
<td>1</td>
<td>-0.03011</td>
<td>0.007014</td>
<td>0.018148</td>
<td>-0.0029</td>
</tr>
<tr>
<td>own_car</td>
<td>0.005085</td>
<td>-0.01931</td>
<td>-0.00907</td>
<td>0.012132</td>
<td>-0.02512</td>
<td>-0.03011</td>
<td>1</td>
<td>0.028009</td>
<td>-0.01711</td>
<td>-0.00674</td>
</tr>
<tr>
<td>FB_Like</td>
<td>0.00136</td>
<td>-0.02509</td>
<td>-0.01407</td>
<td>-0.02225</td>
<td>0.037274</td>
<td>0.007014</td>
<td>0.028009</td>
<td>1</td>
<td>0.009995</td>
<td>-0.03239</td>
</tr>
<tr>
<td>homestate_NJ</td>
<td>0.00836</td>
<td>0.005241</td>
<td>0.003229</td>
<td>0.017494</td>
<td>0.028503</td>
<td>0.018148</td>
<td>-0.01711</td>
<td>0.009995</td>
<td>1</td>
<td>-0.50893</td>
</tr>
<tr>
<td>homestate_NY</td>
<td>-0.02133</td>
<td>0.0181</td>
<td>0.01602</td>
<td>0.025145</td>
<td>-0.024</td>
<td>-0.0029</td>
<td>-0.00674</td>
<td>-0.03239</td>
<td>-0.50893</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">13</span>))</span><br><span class="line">sns.heatmap(cor_table.corr())</span><br></pre></td></tr></table></figure>
<p><img src="corr.png" alt="images"></p>
<p>As we can see, there is no high correlations in our dataset. Now, let’s build a logistic regression model.</p>
<h4 id="Fit-the-model-and-see-the-model-summary"><a href="#Fit-the-model-and-see-the-model-summary" class="headerlink" title="Fit the model and see the model summary"></a>Fit the model and see the model summary</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">logmodel = LogisticRegression()</span><br><span class="line">logmodel.fit(X_train,y_train)</span><br><span class="line">logmodel.coef_</span><br><span class="line"><span class="comment"># array([[ 0.11772918,  0.03811318,  0.00485627,  0.00292365,  0.00243818,  0.52549573,  0.86123004, -0.10407854, -0.41440561, -0.27656558]])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prediction</span></span><br><span class="line">predictions = logmodel.predict(X_test)</span><br><span class="line"><span class="comment"># confusion matrix</span></span><br><span class="line">mat = confusion_matrix(predictions, y_test)</span><br><span class="line">sns.heatmap(mat, square=<span class="literal">True</span>, annot=<span class="literal">True</span>, cbar=<span class="literal">False</span>,fmt=<span class="string">&#x27;.20g&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Actual Result&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Predicted Result&quot;</span>)</span><br><span class="line">a, b = plt.ylim() </span><br><span class="line">a += <span class="number">0.5</span> </span><br><span class="line">b -= <span class="number">0.5</span></span><br><span class="line">plt.ylim(a, b)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="lr_result.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pred_train = logmodel.predict(X_train)</span><br><span class="line">print(classification_report(y_train, pred_train,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.6476    0.3025    0.4124       747</span></span><br><span class="line"><span class="comment">#            1     0.7245    0.9176    0.8097      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7125      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.6860    0.6101    0.6111      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.6988    0.7125    0.6772      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.6214    0.2661    0.3726       327</span></span><br><span class="line"><span class="comment">#            1     0.7073    0.9163    0.7983       633</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.6948       960</span></span><br><span class="line"><span class="comment">#    macro avg     0.6644    0.5912    0.5855       960</span></span><br><span class="line"><span class="comment"># weighted avg     0.6781    0.6948    0.6533       960</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predslog_lr = logmodel.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">metrics.roc_auc_score(y_test,predslog_lr, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 0.6828171273147141</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc</span>(<span class="params">labels, predict_prob</span>):</span></span><br><span class="line">    false_positive_rate,true_positive_rate,thresholds=roc_curve(labels, predict_prob)</span><br><span class="line">    roc_auc=auc(false_positive_rate, true_positive_rate)</span><br><span class="line">    plt.title(<span class="string">&#x27;ROC&#x27;</span>)</span><br><span class="line">    plt.plot(false_positive_rate, true_positive_rate,<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;AUC = %0.4f&#x27;</span>% roc_auc)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">    plt.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],<span class="string">&#x27;r--&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;TPR&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;FPR&#x27;</span>)</span><br><span class="line"> plot_roc(y_test,predslog_lr)</span><br></pre></td></tr></table></figure>
<p><img src="lr_ROC.png" alt="images"></p>
<p>We have two important outcomes from this report. </p>
<p>One is the coefficients of each feature, telling us which characteristics make customers renew the fast-pass? Logistic regression tells us <strong>avgrides_perperson</strong> is more important to the lable variable. The variable,  <strong>avgrides_perperson</strong>, means <em>what was the average number of rides per day, per person, among all family members who used the pass?</em> Primary value is <em>10</em> and coefficient of the variable is 0.86123004. An increase of one average ride in <strong>avgrides_perperson</strong>, the odds of renew season fast-pass will increase by 0.86123004. </p>
<p>The another one is the how the model performs. The performance matric is an essential criterion for model selection.</p>
<h3 id="Part-II-Random-Forest-Model"><a href="#Part-II-Random-Forest-Model" class="headerlink" title="Part II: Random Forest Model"></a>Part II: Random Forest Model</h3><h4 id="Build-a-Initial-Model"><a href="#Build-a-Initial-Model" class="headerlink" title="Build a Initial Model"></a>Build a Initial Model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf_rf=RandomForestClassifier(random_state = <span class="number">654</span>)</span><br><span class="line">clf_rf.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<h4 id="Hyperparameter-Tuning-the-Random-Forest"><a href="#Hyperparameter-Tuning-the-Random-Forest" class="headerlink" title="Hyperparameter Tuning the Random Forest"></a>Hyperparameter Tuning the Random Forest</h4><p>Maybe some of you will ask what is the diffenrence between hyperparameter and parameter? When I started my journey of data analytics, I knew the word parameter to be defined as a value which model itself generates. It would be quite confusing to name what we input into models. This is why the term <em>hyper-parameter</em> came out. Hyper-parameters are input into model in the hope of making the model more accurate. The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance.</p>
<p>This is the concept of hyperparameter. Actually, it has been used in all machine learning models. But in this post, I only present it in Random Forest and XGBoost. Hyperparameter Tuning is more important to these alsogorithm than Logistic Regression. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">200</span>],   <span class="comment"># large n_estimators can predict more precise. Therefore, I only consider a large number, 200 trees.</span></span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], </span><br><span class="line">    <span class="string">&#x27;max_features&#x27;</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],  <span class="comment"># In general, max_features should be set as sqrt of n_feature. sqrt(10) = 3 or 4</span></span><br><span class="line">    <span class="string">&#x27;min_samples_leaf&#x27;</span>: [<span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>],  <span class="comment"># smaller number of leaf would tend to capture the noise of dataset. Therefore, I set []</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CV_rfc = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv= <span class="number">5</span>)</span><br><span class="line">CV_rfc.fit(X_train, y_train)</span><br><span class="line">print(CV_rfc.best_params_)</span><br><span class="line"><span class="comment"># &#123;&#x27;max_depth&#x27;: 8, &#x27;max_features&#x27;: 4, &#x27;min_samples_leaf&#x27;: 8, &#x27;n_estimators&#x27;: 200&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf_rf=RandomForestClassifier(n_estimators=<span class="number">200</span>, max_depth=<span class="number">8</span>, max_features=<span class="number">4</span>, min_samples_leaf=<span class="number">8</span>, random_state=<span class="number">654</span>)</span><br><span class="line">clf_rf.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feature_imp_df = pd.DataFrame(list(zip(clf_rf.feature_importances_, X_train)))</span><br><span class="line">feature_imp_df.columns = [<span class="string">&#x27;feature importance&#x27;</span>, <span class="string">&#x27;feature&#x27;</span>]</span><br><span class="line">feature_imp_df = feature_imp_df.sort_values(by=<span class="string">&#x27;feature importance&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">feature_imp_df</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>feature  importance</th>
<th>featurere</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.27666</td>
<td>visits</td>
</tr>
<tr>
<td>1</td>
<td>0.115035</td>
<td>avgrides_perperson</td>
</tr>
<tr>
<td>3</td>
<td>0.114043</td>
<td>avggoldzone_perperson</td>
</tr>
<tr>
<td>8</td>
<td>0.111199</td>
<td>homestate_NJ</td>
</tr>
<tr>
<td>2</td>
<td>0.109533</td>
<td>avgmerch_perperson</td>
</tr>
<tr>
<td>6</td>
<td>0.106009</td>
<td>own_car</td>
</tr>
<tr>
<td>4</td>
<td>0.090761</td>
<td>avgfood_perperson</td>
</tr>
<tr>
<td>9</td>
<td>0.034174</td>
<td>homestate_NY</td>
</tr>
<tr>
<td>5</td>
<td>0.029154</td>
<td>goldzone_playersclub</td>
</tr>
<tr>
<td>7</td>
<td>0.013431</td>
<td>FB_Like</td>
</tr>
</tbody>
</table>
</div>
<p>According to feature importance value, we can figure out the top 5 important features, which are avggoldzone_perperson, avgmerch_perperson, visits, avgfood_perperson and avgrides_perperson. These features have important value larger than 0.16, which means they are strongly predictable.</p>
<p>In general, when we have large size of features, we keep those have an importance of more than 0.15. However, our dataset is only have 10 variables. It doesn’t matter to keep all the variables in random forest and the following XGBoost because they are not linear model. And more variables will keep more information. Therefore, we keep all the feature.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prediction</span></span><br><span class="line">predictions = clf_rf.predict(X_test)</span><br><span class="line"><span class="comment"># confusion matrix</span></span><br><span class="line">mat = confusion_matrix(predictions, y_test)</span><br><span class="line">sns.heatmap(mat, square=<span class="literal">True</span>, annot=<span class="literal">True</span>, cbar=<span class="literal">False</span>,fmt=<span class="string">&#x27;.20g&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Actual Result&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Predicted Result&quot;</span>)</span><br><span class="line">a, b = plt.ylim() </span><br><span class="line">a += <span class="number">0.5</span> </span><br><span class="line">b -= <span class="number">0.5</span></span><br><span class="line">plt.ylim(a, b)</span><br><span class="line">plt.show()rf_</span><br></pre></td></tr></table></figure>
<p><img src="rf_result.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">predictions = clf_rf.predict(X_train)</span><br><span class="line">print(classification_report(y_train, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment">#  </span></span><br><span class="line"><span class="comment">#            0     0.9233    0.4029    0.5610       747</span></span><br><span class="line"><span class="comment">#            1     0.7670    0.9833    0.8618      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7897      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.8451    0.6931    0.7114      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.8191    0.7897    0.7615      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">predictions = clf_rf.predict(X_test)</span><br><span class="line">print(classification_report(y_test, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.8000    0.3058    0.4425       327</span></span><br><span class="line"><span class="comment">#            1     0.7281    0.9605    0.8283       633</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7375       960</span></span><br><span class="line"><span class="comment">#    macro avg     0.7641    0.6332    0.6354       960</span></span><br><span class="line"><span class="comment"># weighted avg     0.7526    0.7375    0.6969       960</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predslog_rf = clf_rf.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">metrics.roc_auc_score(y_test,predslog_rf, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># rf0.6974892628181901</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_roc(y_test,predslog_rf)</span><br></pre></td></tr></table></figure>
<p><img src="rf_ROC.png" alt="images"></p>
<h3 id="Part-III-XGBoost-Model"><a href="#Part-III-XGBoost-Model" class="headerlink" title="Part III : XGBoost Model"></a>Part III : XGBoost Model</h3><h4 id="Build-initial-model"><a href="#Build-initial-model" class="headerlink" title="Build initial model"></a>Build initial model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf_xgb=xgb.XGBClassifier(random_state=<span class="number">654</span>)</span><br><span class="line">clf_xgb.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<h4 id="Hyoeroarameters-tuning"><a href="#Hyoeroarameters-tuning" class="headerlink" title="Hyoeroarameters tuning"></a>Hyoeroarameters tuning</h4><p>In XGBoost, I tuned hyperparameters in 6 steps.</p>
<h5 id="step-1-n-estimators"><a href="#step-1-n-estimators" class="headerlink" title="step 1: n_estimators"></a>step 1: n_estimators</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">500</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>,<span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary:logistic&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;n_estimators&#x27;: 200&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step-2-max-depth-amp-min-child-weight"><a href="#Step-2-max-depth-amp-min-child-weight" class="headerlink" title="Step 2: max_depth &amp; min_child_weight"></a>Step 2: max_depth &amp; min_child_weight</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;max_depth&#x27;</span>: list(range(<span class="number">1</span>,<span class="number">5</span>,<span class="number">1</span>)), <span class="string">&#x27;min_child_weight&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;max_depth&#x27;: 3, &#x27;min_child_weight&#x27;: 5&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step-3-gamma"><a href="#Step-3-gamma" class="headerlink" title="Step 3: gamma"></a>Step 3: gamma</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;gamma&#x27;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;gamma&#x27;: 0.6&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="step-4-subsample-colsample-bytree"><a href="#step-4-subsample-colsample-bytree" class="headerlink" title="step 4: subsample, colsample_bytree"></a>step 4: subsample, colsample_bytree</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;subsample&#x27;</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>], <span class="string">&#x27;colsample_bytree&#x27;</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.6</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;colsample_bytree&#x27;: 0.9, &#x27;subsample&#x27;: 0.9&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="step-5-regalpha-amp-reglambda"><a href="#step-5-regalpha-amp-reglambda" class="headerlink" title="step 5: regalpha &amp; reglambda"></a>step 5: regalpha &amp; reglambda</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;reg_alpha&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;reg_lambda&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.6</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;reg_alpha&#x27;: 3, &#x27;reg_lambda&#x27;: 0.05&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="step-6-learning-rate"><a href="#step-6-learning-rate" class="headerlink" title="step 6: learning rate"></a>step 6: learning rate</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.07</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;</span><br><span class="line">other_params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.6</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">0.05</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clf_xgb = xgb.XGBRegressor(**other_params)</span><br><span class="line">optimized_GBM = GridSearchCV(estimator=clf_xgb, param_grid=cv_params, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>, n_jobs=<span class="number">4</span>) </span><br><span class="line">optimized_GBM.fit(X_train, y_train)</span><br><span class="line">optimized_GBM.best_params_</span><br><span class="line"><span class="comment"># &#123;&#x27;learning_rate&#x27;: 0.1&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="Build-XGBoost-Model"><a href="#Build-XGBoost-Model" class="headerlink" title="Build XGBoost Model"></a>Build XGBoost Model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">clf_xgb = xgb.XGBClassifier(n_estimators=<span class="number">200</span>, max_depth=<span class="number">3</span>, </span><br><span class="line">                            learning_rate=<span class="number">0.1</span>, subsample=<span class="number">0.9</span>, colsample_bytree=<span class="number">0.9</span>,scale_pos_weight=<span class="number">3.0</span>, </span><br><span class="line">                             silent=<span class="literal">True</span>, nthread=<span class="number">-1</span>, seed=<span class="number">0</span>, missing=<span class="literal">None</span>,objective=<span class="string">&#x27;binary:logistic&#x27;</span>, </span><br><span class="line">                             reg_alpha=<span class="number">3</span>, reg_lambda=<span class="number">0.05</span>, </span><br><span class="line">                             gamma=<span class="number">0.6</span>, min_child_weight=<span class="number">5</span>, </span><br><span class="line">                             max_delta_step=<span class="number">0</span>,base_score=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">clf_xgb.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h4 id="Feature-importance"><a href="#Feature-importance" class="headerlink" title="Feature importance"></a>Feature importance</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature importance </span></span><br><span class="line">feature_imp_df = pd.DataFrame(list(zip(clf_xgb.feature_importances_, X_train)))</span><br><span class="line">feature_imp_df.columns = [<span class="string">&#x27;feature importance&#x27;</span>, <span class="string">&#x27;feature&#x27;</span>]</span><br><span class="line">feature_imp_df = feature_imp_df.sort_values(by=<span class="string">&#x27;feature importance&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">feature_imp_df</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>feature importance</th>
<th>feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>0.321522</td>
<td>homestate_NJ</td>
</tr>
<tr>
<td>6</td>
<td>0.151555</td>
<td>own_car</td>
</tr>
<tr>
<td>0</td>
<td>0.13588</td>
<td>visits</td>
</tr>
<tr>
<td>9</td>
<td>0.095966</td>
<td>homestate_NY</td>
</tr>
<tr>
<td>5</td>
<td>0.075694</td>
<td>goldzone_playersclub</td>
</tr>
<tr>
<td>1</td>
<td>0.061954</td>
<td>avgrides_perperson</td>
</tr>
<tr>
<td>3</td>
<td>0.043305</td>
<td>avggoldzone_perperson</td>
</tr>
<tr>
<td>4</td>
<td>0.041291</td>
<td>avgfood_perperson</td>
</tr>
<tr>
<td>2</td>
<td>0.038462</td>
<td>avgmerch_perperson</td>
</tr>
<tr>
<td>7</td>
<td>0.034371</td>
<td>FB_Like</td>
</tr>
</tbody>
</table>
</div>
<p>According to feature importance value, it is noticable that homestate_NJ is the most important feature. Next is own_car, visits. These variables are strongly predictable.</p>
<h4 id="Measure-performance"><a href="#Measure-performance" class="headerlink" title="Measure performance"></a>Measure performance</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prediction</span></span><br><span class="line">predictions = clf_xgb.predict(X_test)</span><br><span class="line"><span class="comment"># confusion matrix</span></span><br><span class="line">mat = confusion_matrix(predictions, y_test)</span><br><span class="line">sns.heatmap(mat, square=<span class="literal">True</span>, annot=<span class="literal">True</span>, cbar=<span class="literal">False</span>,fmt=<span class="string">&#x27;.20g&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Actual Result&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Predicted Result&quot;</span>)</span><br><span class="line">a, b = plt.ylim() </span><br><span class="line">a += <span class="number">0.5</span> </span><br><span class="line">b -= <span class="number">0.5</span></span><br><span class="line">plt.ylim(a, b)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="xgb_result.png" alt="images"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">predictions = clf_xgb.predict(X_train)</span><br><span class="line">print(classification_report(y_train, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.9598    0.2878    0.4428       747</span></span><br><span class="line"><span class="comment">#            1     0.7361    0.9940    0.8458      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7585      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.8480    0.6409    0.6443      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.8107    0.7585    0.7114      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">predictions = clf_xgb.predict(X_train)</span><br><span class="line">print(classification_report(y_train, predictions,digits=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#               precision    recall  f1-score   support</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#            0     0.9598    0.2878    0.4428       747</span></span><br><span class="line"><span class="comment">#            1     0.7361    0.9940    0.8458      1493</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#     accuracy                         0.7585      2240</span></span><br><span class="line"><span class="comment">#    macro avg     0.8480    0.6409    0.6443      2240</span></span><br><span class="line"><span class="comment"># weighted avg     0.8107    0.7585    0.7114      2240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predslog_rf = clf_xgb.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">metrics.roc_auc_score(y_test,predslog_rf, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 0.6981704518553946</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_roc(y_test,predslog_rf)xgb</span><br></pre></td></tr></table></figure>
<p><img src="xbg_ROC.png" alt="images"></p>
<h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><p>As shown in the performance matrix, XGBoost is the best model with greatest accuracy, recall, f1 score, precision, AUC. Because this data is imbalanced, so we focus on recall, f1 score, precision and AUC.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Accuracy_test</th>
<th>Recall_test</th>
<th>F1_Score_test</th>
<th>Precision_test</th>
<th>AUC</th>
<th>Accuracy_train</th>
<th>Recall_train</th>
<th>F1_Score_train</th>
<th>Precision_train</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistic Regression</td>
<td>0.6948</td>
<td>0.9163</td>
<td>0.7983</td>
<td>0.7073</td>
<td>0.682817</td>
<td>0.7125</td>
<td>0.9176</td>
<td>0.8097</td>
<td>0.7245</td>
</tr>
<tr>
<td>Random Forest</td>
<td>0.7375</td>
<td>0.9605</td>
<td>0.8283</td>
<td>0.7281</td>
<td>0.697489</td>
<td>0.7897</td>
<td>0.9833</td>
<td>0.8618</td>
<td>0.767</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.7585</td>
<td>0.994</td>
<td>0.8458</td>
<td>0.7361</td>
<td>0.69817</td>
<td>0.7585</td>
<td>0.994</td>
<td>0.8458</td>
<td>0.7361</td>
</tr>
</tbody>
</table>
</div>
<p>To uncover the potential factors toward renewing a fast pass, we built three classification models using all the variables. These three models are Logistic Regression, Random Forest and XGBoost. We selected XGBoost as our final model for better performance. Based on the criteria, the classification models can divide individuals as two groups.</p>
<p>When we know an unknown customer information such as owning a car or not, the number of times that the pass was used during the season, we can predict whether the member will renew their card for next season or not.</p>
<h2 id="Thoughts-Beyond-Modeling"><a href="#Thoughts-Beyond-Modeling" class="headerlink" title="Thoughts Beyond Modeling"></a>Thoughts Beyond Modeling</h2><p>However, the model seems to be useless because we already know whether these customers renew or not. Even though these classification models might be useful for prediction in next season, these are more seasonal factors influencing on renewing. Situations are quite distinct in different seasons, unless we training the model with data from last year. For example, the ratio for spring customers to renew summer passes must be higher than that for the winter to renew spring passes. Winter and summer are two holiday for most students.</p>
<p>Therefore, there are more crucial things than predictions. We should be mining what important factors leading to customers renew their passes. And use these factors to promote the potential customers.</p>
<h2 id="Apply-Feature-Importance-to-Target-Potential-Customers"><a href="#Apply-Feature-Importance-to-Target-Potential-Customers" class="headerlink" title="Apply Feature Importance to Target Potential Customers"></a><strong>Apply Feature Importance to Target Potential Customers</strong></h2><p>Each model reveals the importances of features. That is, these relative scores can highlight which features may be most relevant to the target, and the converse, which features are the least relevant.</p>
<p>In Logistic Regression, these coefficients can provide the basis for a crude feature importance score. The higher the coefficient, the higher the “importance” of a feature. As the result shown, <strong>goldzone_playersclub</strong>, <strong>homestate_NJ</strong>(most negative), <strong>own_car</strong> these three features have higher coefficients. In Random Forest and XGBoost, the results are more obvious. These importance values are calculated by how to split the tree. We can obtain them by models’ outputs. In random forest, most top 5 valuable features are <strong>avggoldzone_perperson</strong>, <strong>avgmerch_perperson</strong>, <strong>visits</strong>, <strong>avgfood_perperson</strong>, <strong>avgrides_perperson</strong>. In XGBoost, <strong>homestate_NJ</strong> , <strong>own_car</strong> and <strong>visits</strong> are the most three important features. To summarize, <strong>homestate_NJ</strong>, <strong>own_car</strong>, <strong>visits</strong>, <strong>avggoldzone_perperson</strong>, <strong>avgmerch_perperson</strong> might be most relevant for customers to renew their season passes.</p>
<p>To sum up, three features should be considered.</p>
<ol>
<li><strong>homestate_NJ</strong>: here is a note that both homestate_NJ and homestate_NY have negative coefficients in Logistic Regression. In addition, the rest variable homestate_CT has been droped to avoid the multi correlation, so the coefficient is larger than homestate_NJ and homestate_NY. According to these statistics, we can infer that customers in New York and New Jersey would be busy with their works. Busy lives made them have fewer visits during last season. The second conjecture is that fewer family members in New Jersey and New York families. There are many different reasons to make them tend to believe it doesn’t worth spending money on season pass over a long period. Even though they have time on vacation, amusement parks are not usually their first leisure places. Therefore, the coefficients are negative. In order to attract more target people, we should focus on the customers living in Connecticut. These customers are more likely to renew their season passes than other two states.</li>
<li><strong>own_car</strong>: this variable is the essential to classify the customers. In addition, the coefficient is positive in Logistic Regression. Obviously, families owning cars are more willing to spend time in amusement parks. One reason is convenience. The second reason is that big families are willing to use this type of seasonal pass. They seem to be more beneficial if more family members use them. Owning a car is a common property of such a large family. According to this finding, we can make more promotions to attract families with cars. If they become our seasonal passes members for the first time, they would be more likely to renew the cards for the next seasons.</li>
<li><strong>visits</strong>: coefficient of visit is positive in Logistic Regression, and we can indicate that families with more visits are more likely to renew seasonal passes. It is reasonable that amusement park lovers are more likely to renew season passes. Lobster Land should promote the seasonal passes to those customers who have already spent a lot of time but are still not the seasonal pass members.</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The ability to identify our target customer group and get it right, is gure gold for Lobster Land. With the help of a well-trained classification model, marketers can rely on assumptions and guesswork and more on data-driven insights to find target customers precisely. According to the feature importance values, we can do more promotions for the players living in Connecticut, owning cars, or being willing to spend time in Lobsterland.</p>
<p><strong>Thanks for reading!!! Please let me know if there is something I should add. And if you enjoy it, share it with your friends and colleagues : )</strong></p>
<p><strong>另外博客的中文版本也会及时更新！谢谢！</strong></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/yantongtt.github.io/tags/XGBoost/" rel="tag"># XGBoost</a>
              <a href="/yantongtt.github.io/tags/Classification/" rel="tag"># Classification</a>
              <a href="/yantongtt.github.io/tags/Logistic-Regression/" rel="tag"># Logistic Regression</a>
              <a href="/yantongtt.github.io/tags/Random-Forest/" rel="tag"># Random Forest</a>
              <a href="/yantongtt.github.io/tags/Marketing-Strategy/" rel="tag"># Marketing Strategy</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/yantongtt.github.io/2020/08/06/An-overview-of-time-series-forecasting-models/" rel="prev" title="An Overview of Time Series Forecasting Models">
      <i class="fa fa-chevron-left"></i> An Overview of Time Series Forecasting Models
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Exploratory-Data-Analysis"><span class="nav-number">1.</span> <span class="nav-text">Exploratory Data Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Target-Variable"><span class="nav-number">1.1.</span> <span class="nav-text">Target Variable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Home-State"><span class="nav-number">1.2.</span> <span class="nav-text">Home State</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Engineering-and-Selection"><span class="nav-number">2.</span> <span class="nav-text">Feature Engineering and Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#One-Hot-Encoding"><span class="nav-number">2.1.</span> <span class="nav-text">One-Hot Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-Selection"><span class="nav-number">2.2.</span> <span class="nav-text">Feature Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Compute-the-primary-value-ratio-of-each-variable"><span class="nav-number">2.2.1.</span> <span class="nav-text">1. Compute the primary value ratio of each variable.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Check-the-missing-value"><span class="nav-number">2.2.2.</span> <span class="nav-text">2. Check the missing value</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Modeling"><span class="nav-number">3.</span> <span class="nav-text">Modeling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Traning-Test-Split"><span class="nav-number">3.1.</span> <span class="nav-text">Traning Test Split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Part-I-Logistic-Regression-Model"><span class="nav-number">3.2.</span> <span class="nav-text">Part I: Logistic Regression Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Prepare-the-data-inputs-for-the-model"><span class="nav-number">3.2.1.</span> <span class="nav-text">Prepare the data (inputs for the model)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fit-the-model-and-see-the-model-summary"><span class="nav-number">3.2.2.</span> <span class="nav-text">Fit the model and see the model summary</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Part-II-Random-Forest-Model"><span class="nav-number">3.3.</span> <span class="nav-text">Part II: Random Forest Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Build-a-Initial-Model"><span class="nav-number">3.3.1.</span> <span class="nav-text">Build a Initial Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyperparameter-Tuning-the-Random-Forest"><span class="nav-number">3.3.2.</span> <span class="nav-text">Hyperparameter Tuning the Random Forest</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Part-III-XGBoost-Model"><span class="nav-number">3.4.</span> <span class="nav-text">Part III : XGBoost Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Build-initial-model"><span class="nav-number">3.4.1.</span> <span class="nav-text">Build initial model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyoeroarameters-tuning"><span class="nav-number">3.4.2.</span> <span class="nav-text">Hyoeroarameters tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#step-1-n-estimators"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">step 1: n_estimators</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Step-2-max-depth-amp-min-child-weight"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">Step 2: max_depth &amp; min_child_weight</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Step-3-gamma"><span class="nav-number">3.4.2.3.</span> <span class="nav-text">Step 3: gamma</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#step-4-subsample-colsample-bytree"><span class="nav-number">3.4.2.4.</span> <span class="nav-text">step 4: subsample, colsample_bytree</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#step-5-regalpha-amp-reglambda"><span class="nav-number">3.4.2.5.</span> <span class="nav-text">step 5: regalpha &amp; reglambda</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#step-6-learning-rate"><span class="nav-number">3.4.2.6.</span> <span class="nav-text">step 6: learning rate</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Build-XGBoost-Model"><span class="nav-number">3.4.3.</span> <span class="nav-text">Build XGBoost Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-importance"><span class="nav-number">3.4.4.</span> <span class="nav-text">Feature importance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Measure-performance"><span class="nav-number">3.4.5.</span> <span class="nav-text">Measure performance</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Selection"><span class="nav-number">4.</span> <span class="nav-text">Model Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Thoughts-Beyond-Modeling"><span class="nav-number">5.</span> <span class="nav-text">Thoughts Beyond Modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Apply-Feature-Importance-to-Target-Potential-Customers"><span class="nav-number">6.</span> <span class="nav-text">Apply Feature Importance to Target Potential Customers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">7.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yantong Li"
      src="/yantongtt.github.io/images/avatar.png">
  <p class="site-author-name" itemprop="name">Yantong Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/yantongtt.github.io/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/yantongtt.github.io/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/yantongtt.github.io/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jerilyt" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jerilyt" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jerezlyt@gmail.com" title="E-Mail → mailto:jerezlyt@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/SherryAsako" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;SherryAsako" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/cheerytt_" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;cheerytt_" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yantong Li</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/yantongtt.github.io/lib/anime.min.js"></script>
  <script src="/yantongtt.github.io/lib/velocity/velocity.min.js"></script>
  <script src="/yantongtt.github.io/lib/velocity/velocity.ui.min.js"></script>

<script src="/yantongtt.github.io/js/utils.js"></script>

<script src="/yantongtt.github.io/js/motion.js"></script>


<script src="/yantongtt.github.io/js/schemes/pisces.js"></script>


<script src="/yantongtt.github.io/js/next-boot.js"></script>


  <script defer src="/yantongtt.github.io/lib/three/three.min.js"></script>
    <script defer src="/yantongtt.github.io/lib/three/three-waves.min.js"></script>


  




  
<script src="/yantongtt.github.io/js/local-search.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->













  

  

  

</body>
</html>
