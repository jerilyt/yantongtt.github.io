<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yantong&#39;s Blog</title>
  
  
  <link href="/yantongtt.github.io/atom.xml" rel="self"/>
  
  <link href="https://jerilyt.github.io/yantongtt.github.io/"/>
  <updated>2020-08-06T07:55:31.283Z</updated>
  <id>https://jerilyt.github.io/yantongtt.github.io/</id>
  
  <author>
    <name>Yantong Li</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Exploring for Airbnb Listings in Germany - Price Prediction</title>
    <link href="https://jerilyt.github.io/yantongtt.github.io/2020/08/06/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany-Price-Prediction/"/>
    <id>https://jerilyt.github.io/yantongtt.github.io/2020/08/06/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany-Price-Prediction/</id>
    <published>2020-08-06T07:34:14.000Z</published>
    <updated>2020-08-06T07:55:31.283Z</updated>
    
    <content type="html"><![CDATA[<p><strong>A look into the AirBnB public dataset for price prediction and understanding of the predictive variables</strong></p><p>In <em>Data Cleaning Challenge</em> part, I shared my understanding of data cleaning. Now, let’s dive deeper. </p><p>Think about an interesting question, “if you are a host, how to maximize your profits?” This time I used a linear regression model to explore deeper into the possible factors that contribute to Airbnb rental prices. It’s okay for you to try other models like GBDT or XGBoost. In this post, I will highlight the approach I used to answer this question as well as how I explain the results of regression analysis. </p><a id="more"></a><p>Same as last post, data is sourced from the <a href="http://insideairbnb.com/get-the-data.html"><strong>Inside Airbnb</strong></a> website. And data have been prepared in <em>Data Cleaning Challenge</em> part. On that part, I handled with some missing values and also create some features. </p><h3 id="Step-1-Think-about-the-problem-and-dataset"><a href="#Step-1-Think-about-the-problem-and-dataset" class="headerlink" title="Step 1: Think about the problem and dataset"></a>Step 1: Think about the problem and dataset</h3><p>Before diving head first into the data and producing large correlation matrices, I always try to think of the question and get a sense of the features. Why am I doing this analysis? What’s the goal? What relationships between features and the target variable make sense? Hope this tip can help you.</p><p>In my preview post, missing values have been handled, and also I removed some featrues like url or host_name. As shown in the below table, almost of data is in a very neat and ordered format. Date-typed date has been transformed as number of days or only extract the year. Some of null value has become new level of its variable, and some has been binned into cagegories. I will not go through the cleaning work in detail. Hope the preview blog would be helpful for you.</p><div class="table-container"><table><thead><tr><th></th><th>host_since</th><th>host_location</th><th>host_response_time</th><th>host_response_rate</th><th>host_is_superhost</th><th>host_neighbourhood</th><th>host_listings_count</th><th>host_total_listings_count</th><th>host_has_profile_pic</th><th>host_identity_verified</th><th>street</th><th>neighbourhood</th><th>neighbourhood_cleansed</th><th>neighbourhood_group_cleansed</th><th>city</th><th>state</th><th>zipcode</th><th>market</th><th>smart_location</th><th>country_code</th><th>country</th><th>is_location_exact</th><th>property_type</th><th>room_type</th><th>accommodates</th><th>bathrooms</th><th>bedrooms</th><th>beds</th><th>bed_type</th><th>square_feet</th><th>price</th><th>weekly_price</th><th>monthly_price</th><th>security_deposit</th><th>cleaning_fee</th><th>guests_included</th><th>extra_people</th><th>minimum_nights</th><th>maximum_nights</th><th>calendar_updated</th><th>has_availability</th><th>availability_30</th><th>availability_60</th><th>availability_90</th><th>availability_365</th><th>calendar_last_scraped</th><th>number_of_reviews</th><th>first_review</th><th>last_review</th><th>review_scores_rating</th><th>review_scores_accuracy</th><th>review_scores_cleanliness</th><th>review_scores_checkin</th><th>review_scores_communication</th><th>review_scores_location</th><th>review_scores_value</th><th>requires_license</th><th>license</th><th>instant_bookable</th><th>is_business_travel_ready</th><th>cancellation_policy</th><th>require_guest_profile_picture</th><th>require_guest_phone_verification</th><th>calculated_host_listings_count</th><th>reviews_per_month</th></tr></thead><tbody><tr><td>3</td><td>2008</td><td>Coledale, New South Wales, Australia</td><td>within a day</td><td>high</td><td>f</td><td>Prenzlauer Berg</td><td>1</td><td>1</td><td>t</td><td>t</td><td>Berlin, Germany</td><td>Prenzlauer Berg</td><td>Prenzlauer Berg S√ºdwest</td><td>Pankow</td><td>Berlin</td><td>Berlin</td><td>10405</td><td>Berlin</td><td>Berlin, Germany</td><td>DE</td><td>Germany</td><td>t</td><td>Apartment</td><td>Entire home/apt</td><td>4</td><td>1</td><td>1</td><td>2</td><td>Real Bed</td><td>720</td><td>90</td><td>offer discount</td><td>offer discount</td><td>200</td><td>50</td><td>2</td><td>20</td><td>62</td><td>1125</td><td>7</td><td>t</td><td>0</td><td>0</td><td>0</td><td>220</td><td>2018/11/7</td><td>143</td><td>2009</td><td>2017</td><td>92</td><td>9</td><td>9</td><td>9</td><td>9</td><td>10</td><td>9</td><td>t</td><td>No</td><td>t</td><td>f</td><td>strict_14_with_grace_period</td><td>f</td><td>f</td><td>1</td><td>1.25</td></tr><tr><td>5</td><td>2009</td><td>Berlin, Berlin, Germany</td><td>within an hour</td><td>high</td><td>t</td><td>Prenzlauer Berg</td><td>1</td><td>1</td><td>t</td><td>t</td><td>Berlin, Germany</td><td>Prenzlauer Berg</td><td>Helmholtzplatz</td><td>Pankow</td><td>Berlin</td><td>Berlin</td><td>10437</td><td>Berlin</td><td>Berlin, Germany</td><td>DE</td><td>Germany</td><td>t</td><td>Apartment</td><td>Private room</td><td>2</td><td>1</td><td>1</td><td>2</td><td>Real Bed</td><td>N/A</td><td>42</td><td>no discount</td><td>no discount</td><td>0</td><td>0</td><td>1</td><td>24</td><td>2</td><td>10</td><td>3</td><td>t</td><td>15</td><td>26</td><td>26</td><td>26</td><td>2018/11/7</td><td>197</td><td>2009</td><td>2018</td><td>96</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>9</td><td>t</td><td>No</td><td>f</td><td>f</td><td>moderate</td><td>f</td><td>f</td><td>1</td><td>1.75</td></tr><tr><td>6</td><td>2009</td><td>Berlin, Berlin, Germany</td><td>within a few hours</td><td>high</td><td>f</td><td>Prenzlauer Berg</td><td>1</td><td>1</td><td>t</td><td>t</td><td>Berlin, Germany</td><td>Prenzlauer Berg</td><td>Prenzlauer Berg S√ºdwest</td><td>Pankow</td><td>Berlin</td><td>Berlin</td><td>10405</td><td>Berlin</td><td>Berlin, Germany</td><td>DE</td><td>Germany</td><td>f</td><td>Apartment</td><td>Entire home/apt</td><td>7</td><td>2.5</td><td>4</td><td>7</td><td>Real Bed</td><td>N/A</td><td>180</td><td>offer discount</td><td>no discount</td><td>400</td><td>80</td><td>5</td><td>10</td><td>6</td><td>14</td><td>14</td><td>t</td><td>0</td><td>7</td><td>7</td><td>137</td><td>2018/11/7</td><td>6</td><td>2015</td><td>2018</td><td>100</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>t</td><td>Yes</td><td>f</td><td>f</td><td>strict_14_with_grace_period</td><td>f</td><td>f</td><td>1</td><td>0.15</td></tr><tr><td>7</td><td>2009</td><td>Berlin, Berlin, Germany</td><td>within a day</td><td>high</td><td>f</td><td>Prenzlauer Berg</td><td>3</td><td>3</td><td>t</td><td>f</td><td>Berlin, Germany</td><td>Prenzlauer Berg</td><td>Prenzlauer Berg Nordwest</td><td>Pankow</td><td>Berlin</td><td>Berlin</td><td>10437</td><td>Berlin</td><td>Berlin, Germany</td><td>DE</td><td>Germany</td><td>t</td><td>Apartment</td><td>Entire home/apt</td><td>2</td><td>1</td><td>0</td><td>1</td><td>Real Bed</td><td>N/A</td><td>70</td><td>offer discount</td><td>offer discount</td><td>500</td><td>0</td><td>1</td><td>0</td><td>90</td><td>1125</td><td>1</td><td>t</td><td>0</td><td>0</td><td>0</td><td>129</td><td>2018/11/7</td><td>23</td><td>2010</td><td>2018</td><td>93</td><td>10</td><td>10</td><td>9</td><td>10</td><td>9</td><td>9</td><td>t</td><td>No</td><td>f</td><td>f</td><td>strict_14_with_grace_period</td><td>f</td><td>f</td><td>3</td><td>0.23</td></tr><tr><td>10</td><td>2010</td><td>Berlin, Berlin, Germany</td><td>within an hour</td><td>high</td><td>t</td><td>Prenzlauer Berg</td><td>1</td><td>1</td><td>t</td><td>t</td><td>Berlin, Germany</td><td>Prenzlauer Berg</td><td>Prenzlauer Berg S√ºdwest</td><td>Pankow</td><td>Berlin</td><td>Berlin</td><td>10405</td><td>Berlin</td><td>Berlin, Germany</td><td>DE</td><td>Germany</td><td>t</td><td>Other</td><td>Private room</td><td>2</td><td>1</td><td>1</td><td>1</td><td>Real Bed</td><td>N/A</td><td>45</td><td>offer discount</td><td>offer discount</td><td>0</td><td>18</td><td>1</td><td>26</td><td>3</td><td>30</td><td>7</td><td>t</td><td>8</td><td>18</td><td>42</td><td>42</td><td>2018/11/7</td><td>279</td><td>2010</td><td>2018</td><td>96</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>t</td><td>No</td><td>f</td><td>f</td><td>strict_14_with_grace_period</td><td>f</td><td>f</td><td>1</td><td>2.83</td></tr></tbody></table></div><h3 id="Step-2-Feature-Selection"><a href="#Step-2-Feature-Selection" class="headerlink" title="Step 2: Feature Selection"></a>Step 2: Feature Selection</h3><p>As mentioned in last blog, features has been engineered, including transform the format and create more effecient and useful features. Now is the step of feature selection. Not all the variables will be used in the linear regression model. There are three reasons. One is that some data is unique for each datapoint like id, host_id. These features are very easy  to identify, and I have been removed in data cleaning. The second thing is another extreme situation, where all the value for every datapoint is same or constant. These features are named as “Zero Variance Feature”, which are useless and redundant. I will detect them by primary value analysis. When one feature’s primary value ratio is over a critical level, the feature has less predictable ability. The third one is that there are strong correlations between variables. I use correlation coefficient and VIF to detect</p><h4 id="1-Primary-Value-Ratio"><a href="#1-Primary-Value-Ratio" class="headerlink" title="1. Primary Value Ratio"></a>1. Primary Value Ratio</h4><p>Delect the feature whose primary value ratio over 80%</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">RowNumber = nrow(PB_lm)</span><br><span class="line">cols = colnames(PB_lm)</span><br><span class="line">Primary_Value_Ratio = c()</span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> cols)&#123;</span><br><span class="line">        Primary_Value_Ratio = c(Primary_Value_Ratio,max(table(PB_lm[i]))/RowNumber) <span class="comment"># calculate the ratio</span></span><br><span class="line">&#125;</span><br><span class="line">Primary_Value = data.frame(cols,Primary_Value_Ratio)</span><br><span class="line">Primary_Value[Primary_Value$Primary_Value_Ratio&gt;<span class="number">0.8</span>,]</span><br><span class="line"></span><br><span class="line">PB_lm = select(PB_lm,-Primary_Value[Primary_Value$Primary_Value_Ratio&gt;<span class="number">0.8</span>,]$cols)</span><br></pre></td></tr></table></figure><h4 id="2-Correlation-Analysis"><a href="#2-Correlation-Analysis" class="headerlink" title="2. Correlation Analysis"></a>2. Correlation Analysis</h4><p>Correlation analysis is a statistical method used to evaluate the strength of relationship between two quantitative variables. A high correlation means that two or more variables have a strong relationship with each other, while a weak correlation means that the variables are hardly related. In linear regression model, one model assumption is that variables should be independent of one another. Otherwises, one variable will be explained by other. </p><p><img src="correlation.png" alt="images"></p><h4 id="3-Skewness-of-Dependent-Variable"><a href="#3-Skewness-of-Dependent-Variable" class="headerlink" title="3. Skewness of Dependent Variable"></a>3. Skewness of Dependent Variable</h4><p>Before we detect multicollinearity problem, I transformed price into normal distribution. Price follows right-skewed Distribution. It is a common phenomenon for most real-life variables. However, for linear regression model, it is essential for residual of the model to follow normal distribution. And then the response variable will also follow. Here I used log transformation to make response variable into normal distribution. The new repsonce variable should be log(price). For log transforming, the data point which price is 0 should be deleted. </p><h3 id="Step-3-Run-OLS-and-check-for-linear-regression-assumptions"><a href="#Step-3-Run-OLS-and-check-for-linear-regression-assumptions" class="headerlink" title="Step 3: Run OLS and check for linear regression assumptions"></a>Step 3: Run OLS and check for linear regression assumptions</h3><p>The OLS model is the most common estimation method for linear models, and will provide us with the simplest linear regression model to base our future models off of. It’s always good to start simple then add complexity. In addition, regression model is a powerful analysis that can analyze multiple variables simultaneously to answer complex research questions. However, if you don’t satisfy the OLS assumptions, you might not be able to trust the results. OLS model is a great place to check for linear regression assumptions.</p><h4 id="1-Get-Variance-Inflation-Factors-VIFs-to-Detect-Multicollinearity"><a href="#1-Get-Variance-Inflation-Factors-VIFs-to-Detect-Multicollinearity" class="headerlink" title="1. Get Variance Inflation Factors (VIFs) to Detect Multicollinearity"></a>1. Get Variance Inflation Factors (VIFs) to Detect Multicollinearity</h4><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the initial model</span></span><br><span class="line">lm_1 &lt;- lm(log_price ~., data = train_data3) <span class="comment"># data should been splitted into training dataset and test dataset.</span></span><br><span class="line"></span><br><span class="line">lendfitback &lt;- step(lm_1,direction = <span class="string">&quot;backward&quot;</span>) </span><br><span class="line">summary(lendfitback)</span><br><span class="line">vif(lendfitback,digits = <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>VIFs are not produced by the OLS table so you should manually extract them. They are a great way to check for multicollinearity in your model. Multicollinearity is when there is high correlation between your features. It is an assumption of linear regression that your data does not have multicollinearity, so make sure to check this. You want your VIFs under 7.</p><p>Here, VIFs values of neighbourhood_cleansed, zipcode, first_review are large, which indicates that there is significant Multicollinearity problem with 3 features. Therefore, we delete them and rebuild a new model. </p><h4 id="2-Check-Residuals-of-Model"><a href="#2-Check-Residuals-of-Model" class="headerlink" title="2. Check Residuals of Model"></a>2. Check Residuals of Model</h4><p>Also, another important asssumption is about residuals.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the second model without 3 multicollinearity features</span></span><br><span class="line">lm_2 &lt;- lm(log_price ~. - neighbourhood_cleansed - zipcode-first_review, data = train_data3)</span><br><span class="line">lendfitback_2 &lt;- step(lm_2,direction = <span class="string">&quot;backward&quot;</span>)</span><br><span class="line">plot(lendfitback_2)</span><br></pre></td></tr></table></figure><h5 id="Resuduals-vs-Fitted"><a href="#Resuduals-vs-Fitted" class="headerlink" title="Resuduals vs. Fitted"></a>Resuduals vs. Fitted</h5><p>Even though residuals of some outliers over the range of [-2,2], most of data points randomly distributes around 0. This model fits the data well</p><p><img src="Resuduals_vs._Fitted.png" alt="images"></p><h5 id="Normal-Q-Q-Plot"><a href="#Normal-Q-Q-Plot" class="headerlink" title="Normal Q_Q Plot"></a>Normal Q_Q Plot</h5><p>Some data points deviate from the diagonal, so residuals do not follow normal distribution strictly. </p><p><img src="Normal_QQ_Plot.png" alt="images"></p><h5 id="Scale-Location"><a href="#Scale-Location" class="headerlink" title="Scale-Location"></a>Scale-Location</h5><p>Seem to be constant with no trend. There is no serious problem in Heteroskedasticity.</p><p><img src="Scale-Location.png" alt="images"></p><h5 id="Residual-Independence"><a href="#Residual-Independence" class="headerlink" title="Residual Independence"></a>Residual Independence</h5><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">durbinWatsonTest(lendfitback_2)</span><br></pre></td></tr></table></figure><p>In Durbin Watson Test, the p-value is larger than 0.05, we can assume errors are independent. In conclusion, the residuals can be regarded as stochastic error.</p><h3 id="Step-4-Prediction-in-valid-dataset-and-Summary"><a href="#Step-4-Prediction-in-valid-dataset-and-Summary" class="headerlink" title="Step 4: Prediction in valid dataset and Summary"></a>Step 4: Prediction in valid dataset and Summary</h3><p>After we check the residuals, we know the model is satisfied OLS assumptions. And we use the model to predict on validation dataset.</p><p><img src="summary.png" alt="images"></p><p>The adjusted r-squared is only 0.5277. This means only 52.77% variation has been explained by the multiple regression model. In general, the higher the R-squared, the better the model fits your data. Even though the R-squared is not closing to 1, we also can infer the sample data are well correspond to the fitted (assumed) model. In fact, R-squared doesn’t tell us the entire story. After we check the residual plots, we know the residuals independent and identically distributed in normal distribution. That is, this model fits the data well.</p><p>In valid dataset, RMSE is 0.4206808. The standard deviation of the predictions from the actual values in valid dataset would be 0.4206808. What’s more, RMSE in training dataset is 0.3775776. The difference between these two number is small. This tells us the model is good-fit and has generalization ability. The model has the ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.</p><p><strong>Thanks for reading!</strong></p><p><strong>Please let me know if there is something I should add. And if you enjoy it, share it with your friends and colleagues : )</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;A look into the AirBnB public dataset for price prediction and understanding of the predictive variables&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;Data Cleaning Challenge&lt;/em&gt; part, I shared my understanding of data cleaning. Now, let’s dive deeper. &lt;/p&gt;
&lt;p&gt;Think about an interesting question, “if you are a host, how to maximize your profits?” This time I used a linear regression model to explore deeper into the possible factors that contribute to Airbnb rental prices. It’s okay for you to try other models like GBDT or XGBoost. In this post, I will highlight the approach I used to answer this question as well as how I explain the results of regression analysis. &lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jerilyt.github.io/yantongtt.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Airbnb Listings" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/Airbnb-Listings/"/>
    
      <category term="Regression" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>Exploring for Airbnb Dataset in Germany - Data Cleaning Challenge</title>
    <link href="https://jerilyt.github.io/yantongtt.github.io/2020/08/04/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany/"/>
    <id>https://jerilyt.github.io/yantongtt.github.io/2020/08/04/Exploring-Machine-Learning-for-Airbnb-Listings-in-Germany/</id>
    <published>2020-08-04T12:18:33.000Z</published>
    <updated>2020-08-06T04:30:29.772Z</updated>
    
    <content type="html"><![CDATA[<p>Even though these short term rentals may no longer be booked for vacation or leisure purposes because of coronavirus crackdown recently, we can not deny that Airbnb has seen a meteoric growth since its inception in 2008 with the number of rentals listed on its website growting exponentially every year. I wish this bussiness may flourish again one day, because it definitely makes a cozy place for living or travelling. In the past </p><p>I will be working with Prenzlauer Berg data, one neighborhood of Berlin. Located in the district of Pankow, Prenzlauer Berg is one of the most charismatic neighborhoods there, with countless of bars and cafes. For many reasons, Prenzlauer Berg is a common choice as a base for the visitors of Berlin. There are different varieties of Airbnb properties in this area, so let us driven into it!</p><a id="more"></a><p>Acturally, this project is done in R three months ago, including data preparation &amp; exploration, rental price prediction, clustering analysis to place rentals. I will write three blogs to illustrate my project here. </p><p>In this blog, I will perform an exploration anlysis at first. The Airbnb dataset is sourced from <a href="http://insideairbnb.com/get-the-data.html"><strong>Inside Airbnb</strong></a> website, including detailed listings data. </p><p><strong>A quick glance at the data shows that there are:</strong></p><ul><li>3899 unique listing in Prenzlauer Berg in total. And the first rental was up in August, 2008 by a host from Florida. </li><li>Rental price is ranging from 0 dollar to 5000 dollars for one night. Listing with $ 5000 price tag, a hostel with a little infomation about host and hostel, may be one outlier in dataset. </li></ul><p>In my oppinion, Data Cleaning is the most fundamental but essential work in data analysis. Not only because incorrect data can reduce the modeling effectiveness, but also because we can explore more deeper into this dataset. They exist side by side and play a part together. Without that understanding, we have no basis from which to make decisions about what data is relevant as we clean and prepare our data.</p><p>Data cleaning is the process of cleaning or standardising the data to make it ready for analysis. Most of times, we deal with discrepancies such as incorrect data formats, missing data, errors. It is irrational to delete data with missing values because they may reveal some information. Besides the type errors, there are still many reasons to make it. The Airbnb dataset is . It’s an interesting challenge. In this blog, I will illustrate how I cope with them.</p><h2 id="Handling-missing-values"><a href="#Handling-missing-values" class="headerlink" title="Handling missing values"></a>Handling missing values</h2><p>See how many missing data points we have</p><p>In the raw dataset, there are too many missing values in the form of entire rows with all NA value. We just delete them because there is no meaning. After we delete 1131 NA rows, let’s see how many we have in each column.</p><div class="table-container"><table><thead><tr><th>Feature</th><th>Number of NA</th><th>How it looks like</th></tr></thead><tbody><tr><td>square_feet</td><td>2695</td><td>NA 720 NA NA NA NA NA NA NA 1012</td></tr><tr><td>license</td><td>2564</td><td><NA> 03/Z/RA/003410-18 <NA></td></tr><tr><td>monthly_price</td><td>2392</td><td>“$1,000.00”, “​$1,001.00”, NA</td></tr><tr><td>weekly_price</td><td>2253</td><td>“$1,000.00”,”$1,050.00”,NA</td></tr><tr><td>notes</td><td>1894</td><td>“ I have a cat, please be aware if you have an allergy.”, NA</td></tr><tr><td>interaction</td><td>1530</td><td>“ feel free to contact me or my person of trust any time”,NA</td></tr><tr><td>host_response_time</td><td>1502</td><td>“a few days or more”, “within a week”, NA</td></tr><tr><td>host_response_rate</td><td>1502</td><td>“0%,”10%”,”100%”, NA</td></tr><tr><td>access</td><td>1462</td><td>“ Available inside the room there will be a kettle …”, NA</td></tr><tr><td>neighborhood_overview</td><td>1335</td><td>“ A mix of Celebrities the Young Creative Scene…”, NA</td></tr><tr><td>house_rules</td><td>1313</td><td>“<em> keine laute Musik </em> bitte nicht mit…” ,NA</td></tr><tr><td>host_about</td><td>1271</td><td>“We love to travel ourselves a lot and prefer to stay in  apartments…” NA</td></tr><tr><td>transit</td><td>1152</td><td>“The apartment is situated 3min on foot from the Station Hermannstr.  “,NA</td></tr><tr><td>security_deposit</td><td>1064</td><td>“$0.00”,”$1,000.00”, NA</td></tr><tr><td>cleaning_fee</td><td>768</td><td>“$0.00”,”$10.00”,NA</td></tr><tr><td>host_neighbourhood</td><td>606</td><td>“Adlershof”,”Alt-Hohenschönhausen”,NA</td></tr><tr><td>review_scores_value</td><td>505</td><td>NA 9 9 10 9 10 9 10 NA 9</td></tr><tr><td>review_scores_checkin</td><td>505</td><td>NA 9 10 10 9 10 9 10 NA 9</td></tr><tr><td>review_scores_location</td><td>504</td><td>NA 10 10 10 9 10 10 10 NA 10</td></tr><tr><td>review_scores_communication</td><td>502</td><td>NA 9 10 10 10 10 8 9 NA 9</td></tr><tr><td>review_scores_accuracy</td><td>502</td><td>NA 9 10 10 10 10 9 10 NA 9</td></tr><tr><td>review_scores_cleanliness</td><td>501</td><td>NA 9 10 10 10 10 9 10 NA 9</td></tr><tr><td>review_scores_rating</td><td>500</td><td>NA 92 96 100 93 96 87 94 NA 91</td></tr><tr><td>reviews_per_month</td><td>445</td><td>NA 1.25 1.75 0.15 0.23 2.83 0.75 0.18</td></tr><tr><td>first_review</td><td>445</td><td>2009-06-20,”2009-08-18”,NA</td></tr><tr><td>last_review</td><td>444</td><td>2010-09-16,”2011-01-26”,NA</td></tr><tr><td>summary</td><td>130</td><td>“Ich bin sehr stolz darauf meine Wohnung freundlichen  Berlinbesuchern”, NA</td></tr><tr><td>zipcode</td><td>83</td><td>10115,”10115\n10115”,NA</td></tr><tr><td>state</td><td>19</td><td>“berlin”,”Berlin”,NA</td></tr><tr><td>description</td><td>16</td><td>“ 24m2 room in Neukoelln with large double bed, …” NA</td></tr><tr><td>host_location</td><td>11</td><td>“Berlin, Berlin, Germany “,”Coledale, New South Wales,  Australia”, NA</td></tr><tr><td>market</td><td>9</td><td>Berlin,”Juarez”,NA</td></tr><tr><td>name</td><td>8</td><td>“\tThe right place for your  stay”, NA</td></tr><tr><td>bathrooms</td><td>7</td><td>NA 1.0 1.0 2.5 1.0 1.0</td></tr><tr><td>beds</td><td>2</td><td>NA 2 2   7 1 1</td></tr><tr><td>host_total_listings_count</td><td>1</td><td>NA 1 1   1 3 1</td></tr><tr><td>host_since</td><td>1</td><td>“2008-10-19”, “2009-05-16”,  “2009-08-25”,”2009-11-18”, NA</td></tr><tr><td>host_name</td><td>1</td><td>“Britta”,”Bright”,”Philipp”, “ Chris +  Oliver”, “Wolfram”,NA</td></tr><tr><td>host_listings_count</td><td>1</td><td>NA 1 1 1 3 1 1 2 NA 2 …</td></tr><tr><td>host_is_superhost</td><td>1</td><td>“f”,”t”, NA</td></tr><tr><td>host_identity_verified</td><td>1</td><td>“f”,”t”, NA</td></tr><tr><td>host_has_profile_pic</td><td>1</td><td>“f”,”t”, NA</td></tr><tr><td>city</td><td>1</td><td>Berlin Berlin Berlin Berlin Berlin  NA</td></tr><tr><td>bedrooms</td><td>1</td><td>1 1 4 0 1 2 2 2 0 2</td></tr><tr><td>street</td><td>0</td><td>“\nKreuzberg, Berlin, Germany”…</td></tr><tr><td>smart_location</td><td>0</td><td>“\nKreuzberg, Germany”…</td></tr><tr><td>room_type</td><td>0</td><td>Entire home/apt  Private  room   Shared room</td></tr><tr><td>requires_license</td><td>0</td><td>f  t</td></tr><tr><td>require_guest_profile_picture</td><td>0</td><td>f  t</td></tr><tr><td>require_guest_phone_verification</td><td>0</td><td>f  t</td></tr><tr><td>property_type</td><td>0</td><td>Apartment  Apartment  Apartment   Apartment  Condominium …</td></tr><tr><td>price</td><td>0</td><td>$0.00,”$1,000.00”</td></tr><tr><td>number_of_reviews</td><td>0</td><td>143 197 6 23 279 56 18 163 28 69</td></tr><tr><td>neighbourhood_group_cleansed</td><td>0</td><td>Charlottenburg-Wilm.,…</td></tr><tr><td>neighbourhood_cleansed</td><td>0</td><td>Adlershof,”Albrechtstr.”…</td></tr><tr><td>neighbourhood</td><td>0</td><td>Adlershof,”Alt-Hohenschönhausen” …</td></tr><tr><td>minimum_nights</td><td>0</td><td>62 2 6 90 3 3 3 3 90 60</td></tr><tr><td>maximum_nights</td><td>0</td><td>1125 10 14 1125 30 30 21 1125 1125 365</td></tr><tr><td>longitude</td><td>0</td><td>13.4 13.4 13.4 13.4 13.4</td></tr><tr><td>latitude</td><td>0</td><td>52.5 52.5 52.5 52.5 52.5</td></tr><tr><td>last_scraped</td><td>0</td><td>2018-11-07 2018-11-09</td></tr><tr><td>is_location_exact</td><td>0</td><td>“f”,”t”</td></tr><tr><td>is_business_travel_ready</td><td>0</td><td>“f”,”t”</td></tr><tr><td>instant_bookable</td><td>0</td><td>“f”,”t”</td></tr><tr><td>id</td><td>0</td><td>3176 7071 9991 14325 17409 20858 24569</td></tr><tr><td>host_verifications</td><td>0</td><td>“[‘email’, ‘facebook’, ‘jumio’, ‘offline_government_id’,  ‘government_id’]”…</td></tr><tr><td>host_id</td><td>0</td><td>3718 17391 33852 55531 67590</td></tr><tr><td>has_availability</td><td>0</td><td>“t”</td></tr><tr><td>guests_included</td><td>0</td><td>2 1 5 1 1 2 2 4 1 2</td></tr><tr><td>extra_people</td><td>0</td><td>$0.00”,”$10.00”,…</td></tr><tr><td>country_code</td><td>0</td><td>“DE”</td></tr><tr><td>country</td><td>0</td><td>“Germany”</td></tr><tr><td>cancellation_policy</td><td>0</td><td>“flexible”,”moderate”,..</td></tr><tr><td>calendar_updated</td><td>0</td><td>“ 1 week ago”, “10 months ago”, “11 months  ago”, “12 months ago”…</td></tr><tr><td>calendar_last_scraped</td><td>0</td><td>“2018-11-07”,”2018-11-09”</td></tr><tr><td>calculated_host_listings_count</td><td>0</td><td>1  2  3    4  5  6    7  8  9   10  11  12   13  15  16   19  45</td></tr><tr><td>bed_type</td><td>0</td><td>Airbed     Couch     Futon Pull-out Sofa   Real Bed</td></tr><tr><td>availability_90</td><td>0</td><td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td></tr><tr><td>availability_60</td><td>0</td><td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td></tr><tr><td>availability_365</td><td>0</td><td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td></tr><tr><td>availability_30</td><td>0</td><td>0  1    2  3  4    5  6  7    8  9  10   11  12  13   14  15  16</td></tr><tr><td>amenities</td><td>0</td><td>{Internet,Wifi,Kitchen,”Buzzer/wireless intercom”,Crib}…</td></tr><tr><td>accommodates</td><td>0</td><td>1  2    3  4  5    6  7  8    9  10  11   12  16</td></tr></tbody></table></div><p> That seems like a lot! For almost cells in this dataset are empty. In the next step, we are going to take a closer look at some of the columns with missing values and try to fighure out what might be going on with them,</p><h3 id="Figure-out-why-the-data-is-missing"><a href="#Figure-out-why-the-data-is-missing" class="headerlink" title="Figure out why the data is missing"></a>Figure out why the data is missing</h3><p>This is the point at which we get into the part of data science that I like to call “data intution”, by which I mean “really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis”. It can be a frustrating part of data science, especially if you’re newer to the field and don’t have a lot of experience. For dealing with missing values, you’ll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:</p><p>Before we impute the missing value, it is a critical point to get into this part. This helps us to get “data intuition” by really looking at your dataset and trying to figure out why it is this way and how that will affect your analysis. Keep the feature or not is a really important question for analysis, and you can ask yourself <strong>“Is this value missing becuase it wasn’t recorded or becuase it dosen’t exist?”</strong> </p><p>For the first thing, like square_feet of house is existed but some hosts do not input this kind of data for many possible reasons, we should consider to impute them in a proper way or delete them directly. These value you probaly do not want to keep them as NA. Also, you can try to guess what it might have been based on the other values in that column and row. However, if the value is missing because it doesn’t exist, for example monthly_price and weekly_price for some listings, we should consider feature engineering to transform them into more meaningful features. Some listings do not have monthly prices or weekly price because they do not offer the discounts for long-term renting. It tells us a lot! Something like “probably the host want to make profit by offering discount because his/her house is more suitable for  long-term accommodation. Maybe no visitor who traveling here wants to live in this place. Or the host think only rent for one day can not get profit, and he/she have to clean or manage this accommodation everyday. In this way, I made new features about how many discounts that the house provide. (I will talk it in the latter content.)</p><h3 id="Ways-to-handle-missing-values"><a href="#Ways-to-handle-missing-values" class="headerlink" title="Ways to handle missing values"></a>Ways to handle missing values</h3><p>Let me talk about the way to handle missing values in many situations. </p><ul><li><strong>Drop Columns with Missing Values</strong></li></ul><p>Even though square_feet is an significant variable toward rent price, there are too many missing value here. inproper imputation may bring more bias. Let take mean imputation for an example. If we impute missing value as mean value, and regard it is missing randomly, then the mean estimator is unbiased. This is the logic to achieve the mean i putation. However, if we do in this way, we do not keep the relationships with other variables. Statistically, the estimator is unbiased, but the standard deviation is biased. And most of our interest is to find the relationships, so mean imputation is not a good choice.  In this case, we can not use simple option to fill square_feet with the null value. Also, we can not find any straight relationships with this variable.  Take many factors into consideration, we just drop this column directly. </p><ul><li><p><strong>Feature Engineering</strong></p><ul><li>Whether or not</li></ul><p>license is a feature discribe the host’s license. If host has that, they should upload the exact license code here. Acturally, we don’t care about the exact text information on that. Whether has license or not is more meaningful for us. Therefore, I transform the license code as “T”, and missing value as “F”. “T” is the abbreviation of “true”, representing the host has license code for the accomendation. While “F” is “false”, which means they do have license code. </p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PB_3$license[which(is.na(PB_3$license)==<span class="literal">F</span>)] = <span class="string">&quot;Yes&quot;</span></span><br><span class="line">PB_3$license[which(is.na(PB_3$license)==<span class="literal">T</span>)] = <span class="string">&quot;No&quot;</span></span><br></pre></td></tr></table></figure><p>The dataset still has some variables which can handle in the same method.</p><ul><li>Date to Time </li></ul><p>First review date is an important feature for analysis. It tells us how long does the host operating this accomendation. However, as a category variable, first_review will be re-encoded as many 0/1 variables. We can transform the date as days, a numeric variable. How to transform? Create a new feature.</p><p>Here, I created a new feature Business_interval, which measures number of day after this airbnb has first review. Also, same method works on other date variables, like last_review, host_since. I created a feature named No_Business_interval. This measures number of day after the last review. Longer interval means this airbnb is no business recently. And the third feature is operation_interval, which is number of day after host_since. This measure how long that the airbnb is. </p><p>There are missing value in first_review and last_review the two variables. The reason would be there is no review. Therefore, the time interval would be 0.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">first_review = as.Date(PB_2$first_review,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">last_review = as.Date(PB_2$last_review,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">host_since = as.Date(PB_2$host_since,<span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">Business_interval = today() - first_review</span><br><span class="line">No_Business_interval = today() - last_review</span><br><span class="line">operation_interval = today() - host_since</span><br><span class="line"></span><br><span class="line">Business_interval[is.na(Business_interval) == <span class="literal">T</span>] = <span class="number">0</span></span><br><span class="line">No_Business_interval[is.na(No_Business_interval) == <span class="literal">T</span>] = <span class="number">0</span> </span><br><span class="line">operation_interval[is.na(operation_interval) == <span class="literal">T</span>] = <span class="number">0</span> </span><br></pre></td></tr></table></figure><ul><li>Binning into category</li></ul><p>In my oppinion, missing is also information on that feature. I dont like delete or impute them directly. Sometimes, I’d like to keep them, making a new value for them. That is, I will create a new level named, “N/A” for the missing value. However, problems come out. If we use “N/A” to present the missing value, then the variable will be categorical feature. It is okay for the features with a small amount of values like gender. After created new level, gender can be “female”, “male”, “don’t know”. But for numeric variables like host_response_rate, value is numeric ranging from 0 to 100%. In this case, I created new level as “N/A” (not available), and binned the rate into two group, high rate(host_response_rate &gt;= 80%) and low rate(host_response_rate &lt; 80%). The information will be reducted but it is more effeciently for analysis and do not influence the analysis a lot. </p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transform missing value to &quot;N/A&quot;(not available). create a level for null value</span></span><br><span class="line">PB_3$host_response_time[is.na(PB_3$host_response_time)] &lt;- <span class="string">&quot;N/A&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># host_response_rate &#123;high: host_response_rate &gt;= 80%&#125;, &#123;low: host_response_rate &lt; 80%&#125;, &#123;N/A: not available&#125;</span></span><br><span class="line">PB_3$host_response_rate &lt;- as.numeric(sub(<span class="string">&quot;%&quot;</span>,<span class="string">&quot;&quot;</span>,PB_3$host_response_rate))/<span class="number">100</span></span><br><span class="line">summary(as.factor(PB_3$host_response_rate))</span><br><span class="line">PB_3$host_response_rate[PB_3$host_response_rate&gt;= <span class="number">0.8</span>] &lt;- <span class="string">&quot;high&quot;</span></span><br><span class="line">PB_3$host_response_rate[PB_3$host_response_rate&lt; <span class="number">0.8</span>] &lt;- <span class="string">&quot;low&quot;</span></span><br><span class="line">PB_3$host_response_rate[is.na(PB_3$host_response_rate)] &lt;- <span class="string">&quot;N/A&quot;</span></span><br></pre></td></tr></table></figure><ul><li>Other feature engineering</li></ul><p>As I mention above, we can use monthly_price and weekly_price to create new features about the discount that host offered. In this step, we can not use the exact number from weekly_price and monthly_price. Because the two prices are calculated after we decide the daily price. That is, it will be higher when daily price is high. However, we can measure whether the airbnb provide the discount or not. If providing, we use the value as “offer discount”, and if not, the value will be “no discount”.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(PB_lm$weekly_price)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (is.na(PB_lm$weekly_price[i]) == <span class="literal">T</span>)&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = PB_lm$price[i]*<span class="number">7</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">weekly_discount_ratio = (PB_lm$weekly_price)/(PB_lm$price*<span class="number">7</span>)</span><br><span class="line">weekly_discount_ratio[is.na(weekly_discount_ratio) == <span class="literal">T</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment">#summary(weekly_discount_ratio)</span></span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(weekly_discount_ratio))&#123;</span><br><span class="line">        <span class="keyword">if</span> (weekly_discount_ratio[i] &lt; <span class="number">1</span> )&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = <span class="string">&quot;offer discount&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">                PB_lm$weekly_price[i] = <span class="string">&quot;no discount&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">PB_lm$weekly_price = as.factor(PB_lm$weekly_price)</span><br><span class="line"><span class="comment">#summary(PB_lm$weekly_price)</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><strong>Filling in missing values automatically</strong></li></ul><p>Sure, I mentioned that I do not like mean imputation in <em>Drop Columns with Missing Values</em>. But the situation is on condition that mean value is meaningless. Acturally, we imputate the missing value using the idea of maximum likelihood. Some features like host_has_profile_pic, presenting whether host has profile picture or not, have most value of “t”. Most of host upload their profile picture on the sites. Therefore, we can use mode value to impute the missing value. </p><h2 id="Inconsistent-Data-Entry"><a href="#Inconsistent-Data-Entry" class="headerlink" title="Inconsistent Data Entry"></a>Inconsistent Data Entry</h2><h2 id="Scale-and-Normalize-Data"><a href="#Scale-and-Normalize-Data" class="headerlink" title="Scale and Normalize Data"></a>Scale and Normalize Data</h2><h2 id="Parsing-Dates"><a href="#Parsing-Dates" class="headerlink" title="Parsing Dates"></a>Parsing Dates</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Even though these short term rentals may no longer be booked for vacation or leisure purposes because of coronavirus crackdown recently, we can not deny that Airbnb has seen a meteoric growth since its inception in 2008 with the number of rentals listed on its website growting exponentially every year. I wish this bussiness may flourish again one day, because it definitely makes a cozy place for living or travelling. In the past &lt;/p&gt;
&lt;p&gt;I will be working with Prenzlauer Berg data, one neighborhood of Berlin. Located in the district of Pankow, Prenzlauer Berg is one of the most charismatic neighborhoods there, with countless of bars and cafes. For many reasons, Prenzlauer Berg is a common choice as a base for the visitors of Berlin. There are different varieties of Airbnb properties in this area, so let us driven into it!&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jerilyt.github.io/yantongtt.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Airbnb Listings" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/Airbnb-Listings/"/>
    
      <category term="Exploratory Analysis" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/Exploratory-Analysis/"/>
    
      <category term="Data Cleaning" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/Data-Cleaning/"/>
    
      <category term="Feature Engineering" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/Feature-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>Lending Club</title>
    <link href="https://jerilyt.github.io/yantongtt.github.io/2020/08/04/Lending-Club/"/>
    <id>https://jerilyt.github.io/yantongtt.github.io/2020/08/04/Lending-Club/</id>
    <published>2020-08-04T12:00:41.000Z</published>
    <updated>2020-08-04T12:12:31.435Z</updated>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h1&gt;
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://jerilyt.github.io/yantongtt.github.io/categories/Machine-Learning/"/>
    
    
      <category term="风控" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/%E9%A3%8E%E6%8E%A7/"/>
    
      <category term="机器学习" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>A Markov Chain Model in Manpower Systems</title>
    <link href="https://jerilyt.github.io/yantongtt.github.io/2020/08/04/A-Markov-Chain-Model-in-Manpower-Systems/"/>
    <id>https://jerilyt.github.io/yantongtt.github.io/2020/08/04/A-Markov-Chain-Model-in-Manpower-Systems/</id>
    <published>2020-08-03T16:22:09.000Z</published>
    <updated>2020-08-05T14:53:03.430Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何根据互联网员工年龄"><a href="#如何根据互联网员工年龄" class="headerlink" title="如何根据互联网员工年龄"></a>如何根据互联网员工年龄</h1><h2 id="基于-Markov-过程预测未来公司人员规模"><a href="#基于-Markov-过程预测未来公司人员规模" class="headerlink" title="基于 Markov 过程预测未来公司人员规模"></a>基于 Markov 过程预测未来公司人员规模</h2><p>其实不管是互联网、金融还是其他行业，拟定一个合理员工年龄结构战略对任何一家企业的长期发展都有着极其重要的影响，同时也是人力规划的核心目标。在制定未来一段时期人员补充方案时，企业有必要对不同方案下的员工年龄结构变化趋势进行预测，从而判断不同方案下员工队伍的变化是否能支撑未来发展需要。<br><a id="more"></a></p><h3 id="从Markov-Chain到模型构建"><a href="#从Markov-Chain到模型构建" class="headerlink" title="从Markov Chain到模型构建"></a>从Markov Chain到模型构建</h3><p>这个模型我是基于Markov Chain 的思想来构建的。何为Markov Chain？简单来说就是假设某一时刻状态转移的概率只依赖于它的前一个状态。</p><p>在年龄预测模型中，基本的几个状态为不同的年龄段。Markov Chain 模型主要是分析一个人在某一阶段内由一个年龄段调到另一个年龄段的可能性。从一个年龄段转移到另一个年龄段的状态属于内部流动，意味着该员工仍在该企业就职。但还存在外部流动的状态，离职与退休。属于员工流失。通过运用Markov 过程原理，分析每个年龄段的员工流动到不同状态的流动趋势和概率，构建完整的预测周期运算过程，以便为人力资源在新增人员对企业总量规模、人员年龄结构的规划中提供依据。</p><p>这么说不太直观，举个例子。在项目中，我将内部流动分为7个阶段，分别为20-24，25-28，29-31，32-35，36-40，41-50，51-60。外部流动为离职率和退休率。</p><p>根据Markov Chain原理，转移矩阵中的概率就是某一状态到另一状态的可能性。下表就是项目中转移矩阵的结构。我们就用这个转移矩阵就表示这一年内发生的事情。其中，p_23表示今年处于[25,28]这一年龄阶段的员工在下一年流向[29,31]的概率。为了方便计算，我用频率表示，即今年处于[25,28]这一年龄阶段而在下一年流向[29,31]的员工总数与今年处于[25,28]这一年龄阶段的员工总数比值。为什么大部分概率为0其实很好理解，这些概率为0的状态转移情况是不可能发生的。今年[25,28]年龄段的员工明年可能是26,27,28,29，因此在内部流动中只能有两种状态。而先前说的p_23为今年28岁的员工明年没有离职流向[29,31]年龄的概率。</p><p><strong>Transition Matrix from Preview Year to Next Year</strong></p><div class="table-container"><table><thead><tr><th></th><th>20-24</th><th>25-28</th><th>29-31</th><th>32-35</th><th>36-40</th><th>41-50</th><th>51-60</th><th>Dismission</th><th>Retirement</th><th>Recruitment</th></tr></thead><tbody><tr><td>20-24</td><td>p_11</td><td>p_12</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>p_18</td><td>0</td><td>p_1_10</td></tr><tr><td>25-28</td><td>0</td><td>p_22</td><td>p_23</td><td>0</td><td>0</td><td>0</td><td>0</td><td>p_28</td><td>0</td><td>p_2_10</td></tr><tr><td>29-31</td><td>0</td><td>0</td><td>p_33</td><td>p_34</td><td>0</td><td>0</td><td>0</td><td>p_38</td><td>0</td><td>p_3_10</td></tr><tr><td>32-35</td><td>0</td><td>0</td><td>0</td><td>p_44</td><td>p_45</td><td>0</td><td>0</td><td>p_48</td><td>0</td><td>p_4_10</td></tr><tr><td>36-40</td><td>0</td><td>0</td><td>0</td><td>0</td><td>p_55</td><td>p_56</td><td>0</td><td>p_58</td><td>0</td><td>p_5_10</td></tr><tr><td>41-50</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>p_66</td><td>p_67</td><td>p_68</td><td>0</td><td>p_6_10</td></tr><tr><td>51-60</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>p_77</td><td>p_78</td><td>p_79</td><td>p_7_10</td></tr></tbody></table></div><h3 id="模型假设与定义"><a href="#模型假设与定义" class="headerlink" title="模型假设与定义"></a>模型假设与定义</h3><p><img src="hypotheses.png" alt="images"></p><p>假设，</p><ul><li>$N(t)$ 为第t年年初各年龄段员工总数，$N(t) = [n_1(t), n_2(t), … , n_7(t)]$，其中$n_1(t),n_2(t),…,n_7(t)$ 分别为第t年20-24岁，25-28岁，29-31岁，32-35岁，36-40岁，41-50岁，51-60岁的员工人数。</li><li>$L(t)$ 为在第t整年各年龄段离职员工人数，$L(t) = [l_1(t), l_2(t), … , l_7(t)]$，而$P(t) = [p_1(t), p_2(t), … , p_7(t)]$分别为发生概率，即$L(t)=N(t)\times P(t)$</li><li>$R_t(t)$ 为在第t整年各年龄段退休员工人数，$R_t(t)=[0,0,0,0,0,0,r_7(t)]$ </li><li>$R_c(t)$ 为在第t整年各年龄段新增员工人数</li><li>$P_{ij}(t+1)$ 第t年从状态$i$到下一年转移到状态$j$的概率</li></ul><p><strong>模型假设</strong> : 下一年年初的员工总数=这一年年末剩余+下一年年初新增。而这一年年末剩余就等于这一年年初的总数乘上转移矩阵。这就是markov process在这个算法中的运用。<script type="math/tex">N(t+1)=N(t)*P_{ij}(t+1)+R_c(t+1)</script></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;如何根据互联网员工年龄&quot;&gt;&lt;a href=&quot;#如何根据互联网员工年龄&quot; class=&quot;headerlink&quot; title=&quot;如何根据互联网员工年龄&quot;&gt;&lt;/a&gt;如何根据互联网员工年龄&lt;/h1&gt;&lt;h2 id=&quot;基于-Markov-过程预测未来公司人员规模&quot;&gt;&lt;a href=&quot;#基于-Markov-过程预测未来公司人员规模&quot; class=&quot;headerlink&quot; title=&quot;基于 Markov 过程预测未来公司人员规模&quot;&gt;&lt;/a&gt;基于 Markov 过程预测未来公司人员规模&lt;/h2&gt;&lt;p&gt;其实不管是互联网、金融还是其他行业，拟定一个合理员工年龄结构战略对任何一家企业的长期发展都有着极其重要的影响，同时也是人力规划的核心目标。在制定未来一段时期人员补充方案时，企业有必要对不同方案下的员工年龄结构变化趋势进行预测，从而判断不同方案下员工队伍的变化是否能支撑未来发展需要。&lt;br&gt;
    
    </summary>
    
    
      <category term="Stochastic Model" scheme="https://jerilyt.github.io/yantongtt.github.io/categories/Stochastic-Model/"/>
    
    
      <category term="人力资源" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/%E4%BA%BA%E5%8A%9B%E8%B5%84%E6%BA%90/"/>
    
      <category term="Markov Chain" scheme="https://jerilyt.github.io/yantongtt.github.io/tags/Markov-Chain/"/>
    
  </entry>
  
</feed>
